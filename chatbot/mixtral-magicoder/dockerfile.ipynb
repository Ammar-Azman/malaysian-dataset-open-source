{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f779b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/datasets/malaysia-ai/starcoderdata-sample/resolve/main/dockerfile.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cb5572a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (5.2.0)/charset_normalizer (2.0.7) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b6f5a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [00:01, 74477.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99998"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = []\n",
    "\n",
    "with open('dockerfile.jsonl') as fopen:\n",
    "    for l in tqdm(fopen):\n",
    "        try:\n",
    "            l = json.loads(l)\n",
    "            if len(l['content'].split()) > 5e3:\n",
    "                continue\n",
    "            ls.append(l['content'])\n",
    "        except:\n",
    "            pass\n",
    "len(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e61e8115",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(\n",
    "    '', timeout = 120\n",
    ")\n",
    "\n",
    "\n",
    "def format_prompt(message, history):\n",
    "  prompt = \"<s>\"\n",
    "  for user_prompt, bot_response in history:\n",
    "    prompt += f\"[INST] {user_prompt} [/INST]\"\n",
    "    prompt += f\" {bot_response}</s> \"\n",
    "  prompt += f\"[INST] {message} [/INST]\"\n",
    "  return prompt\n",
    "\n",
    "def format_user(history):\n",
    "    prompt = \"<s>\"\n",
    "    for user_prompt, bot_response in history:\n",
    "        prompt += f\"[INST] {user_prompt} [/INST]\"\n",
    "        prompt += f\" {bot_response}</s> \"\n",
    "    prompt += f\"[INST]\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eda36b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = dict(\n",
    "    temperature=1.0,\n",
    "    max_new_tokens=5120,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.0,\n",
    "    do_sample=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdc207c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘mixtral-dockerfile’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir mixtral-dockerfile\n",
    "# !rm -rf mixtral-dockerfile/*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ef3a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(q, i):\n",
    "    filename = f'mixtral-dockerfile/{i}.json'\n",
    "    if os.path.exists(filename):\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "    # for _ in range(5):\n",
    "        try:\n",
    "            \n",
    "            prompt = f\"\"\"\n",
    "Please gain inspiration from the following random code snippet to create a high-quality Dockerfile problem. Present your output in two distinct sections: [Problem Description] and [Solution].\n",
    "\n",
    "Dockerfile snippet for inspiration:\n",
    "```\n",
    "{q}\n",
    "```\n",
    "\n",
    "Guidelines for each section:\n",
    "\n",
    "1. [Problem Description]: This should be **completely self-contained**, providing all the contextual information one needs to understand and solve the problem. Assume common programming knowledge, but ensure that any specific context, variables, or code snippets pertinent to this problem are explicitly included but do not mention code snippet above.\n",
    "\n",
    "2. [Solution]: Offer a comprehensive, **correct** solution that accurately addresses the [Problem Description] you provided.\n",
    "\"\"\".strip()\n",
    "            formatted_prompt = format_prompt(prompt, [])\n",
    "            stream = client.text_generation(formatted_prompt, **generate_kwargs, stream=False, details=True, return_full_text=False)\n",
    "            output = stream.generated_text\n",
    "            with open(filename, 'w') as fopen:\n",
    "                json.dump(output, fopen)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if 'tokens + `max_new_tokens`' in str(e):\n",
    "                with open(filename, 'w') as fopen:\n",
    "                    json.dump(False, fopen)\n",
    "                break\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f0ac5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumer(queue, name):\n",
    "    while True:\n",
    "        if queue.qsize() == 0:\n",
    "            break\n",
    "        item = queue.get()\n",
    "        answer(*item)\n",
    "    print(f'consumer {name} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "236113b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "queue = Queue()\n",
    "questions = ls\n",
    "urls = [(q, no) for no, q in enumerate(questions)]\n",
    "for u in urls:\n",
    "    queue.put(u)\n",
    "    \n",
    "ori_size = queue.qsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efbd604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer(*urls[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aacdb9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " **[Problem Description]**\n",
      "\n",
      "The task is to create a Dockerfile that will build an image for running Hubble, a network traffic analyzer and visualizer. Hubble requires Python 3.7, pip, git, osquery, and vcredist prerequisites to be installed in the environment. Additionally, it needs several packages installed via pip. The resulting image should have all the necessary dependencies and be ready to run Hubble.\n",
      "\n",
      "The Dockerfile should meet the following requirements:\n",
      "\n",
      "1. Based on a lightweight Windows base image.\n",
      "2. Install Python 3.7, pip, git, osquery, and vcredist prerequisites.\n",
      "3. Clone the Hubble repository and checkout the specified version.\n",
      "4. Build Hubble and its dependencies using pip.\n",
      "5. Set up an entry point to run Hubble when the container starts.\n",
      "\n",
      "**[Solution]**\n",
      "\n",
      "Create a `Dockerfile` with the following content:\n",
      "\n",
      "```Dockerfile\n",
      "# Use a lightweight Windows base image\n",
      "FROM mcr.microsoft.com/windows/servercore:ltsc2019\n",
      "\n",
      "# Set environment variables\n",
      "ENV PYTHONIOENCODING='UTF-8' \\\n",
      "    HUBBLE_CHECKOUT=v4.0.0 \\\n",
      "    HUBBLE_GIT_URL=https://github.com/hubblestack/hubble.git\n",
      "\n",
      "# Create working directory\n",
      "RUN mkdir C:\\temp && cd C:\\temp\n",
      "WORKDIR C:\\temp\n",
      "\n",
      "# Update package index and install required tools\n",
      "RUN Invoke-WebRequest -Uri 'https://chocolatey.org/install.ps1' -OutFile 'choco.ps1'; \\\n",
      "    Set-ExecutionPolicy Bypass -Scope Process -Force; \\\n",
      "    .\\choco.ps1; \\\n",
      "    choco install -y git nssm powershell; \\\n",
      "    rm choco.ps1; \\\n",
      "\n",
      "# Install Python 3.7, pip and vcredist prerequisite\n",
      "RUN Invoke-WebRequest -Uri https://www.python.org/ftp/python/3.7.9/python-3.7.9-amd64.exe -OutFile python.exe; \\\n",
      "    Start-Process python.exe -Wait -ArgumentList '/quiet', 'InstallAllUsers=1', 'PrependPath=1', 'Include_test=0', 'TargetDir=C:\\Python37'; \\\n",
      "    $env:PATH += ';C:\\Python37'; \\\n",
      "    python -m ensurepip; \\\n",
      "    del python.exe;\n",
      "\n",
      "# Install osquery\n",
      "RUN Invoke-WebRequest -Uri https://osquery-packages.s3.amazonaws.com/windows/x64/osquery-3.4.0.zip -OutFile osquery.zip; \\\n",
      "    Expand-Archive osquery.zip -DestinationPath .; \\\n",
      "    Remove-Item osquery.zip; \\\n",
      "    mkdir osquery; \\\n",
      "    Move-Item osquery-* osquery; \\\n",
      "\n",
      "# Clone Hubble and change directory\n",
      "RUN git clone $env:HUBBLE_GIT_URL; \\\n",
      "    cd hubble; \\\n",
      "    git checkout $env:HUBBLE_CHECKOUT;\n",
      "\n",
      "# Build Hubble dependencies\n",
      "RUN .\\pkg\\windows\\build.ps1;\n",
      "\n",
      "# Copy local files to working directory\n",
      "COPY hubble.conf .\\hubble\\\n",
      "\n",
      "# Set entry point script\n",
      "ENTRYPOINT [\"powershell.exe\", \"-ExecutionPolicy\", \"Bypass\", \"-File\", \"C:\\temp\\entrypoint.ps1\"]\n",
      "```\n",
      "\n",
      "The `Dockerfile` creates a Windows-based image with Python 3.7, pip, git, osquery and required dependencies installed. It clones the Hubble repository, checks out the specified version, and builds it with dependencies using pip. The entry point script for running Hubble is not included in the provided code snippet; therefore, you should create an `entrypoint.ps1` script that starts Hubble as needed.\n",
      "\n",
      "For building and running the image, you can use the following commands:\n",
      "\n",
      "```sh\n",
      "# Build the Docker image\n",
      "docker build -t hubble-image .\n",
      "\n",
      "# Run the container and map the required volume\n",
      "docker run -it --rm -v <host folder>:C:\\data hubble-image\n",
      "```\n",
      "\n",
      "Replace `<host folder>` with a folder on your host machine where you want to store Hubble output files.\n"
     ]
    }
   ],
   "source": [
    "with open('mixtral-dockerfile/3.json') as fopen:\n",
    "    print(json.loads(fopen.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c77c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|████████████████▋                                                                                  | 16819/99998 [6:17:02<18:18:07,  1.26it/s]"
     ]
    }
   ],
   "source": [
    "max_worker = 50\n",
    "consumers = [Thread(target=consumer, args=(queue,i)) for i in range(max_worker)]\n",
    "for i in range(len(consumers)):\n",
    "    consumers[i].start()\n",
    "    \n",
    "pbar = tqdm(total=ori_size)\n",
    "last_size = 0\n",
    "while True:\n",
    "    size = queue.qsize()\n",
    "    if size == 0:\n",
    "        break\n",
    "    left = ori_size - size\n",
    "    minus = left - last_size\n",
    "    if minus > 0:\n",
    "        pbar.update(minus)\n",
    "        last_size += minus\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1e0e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
