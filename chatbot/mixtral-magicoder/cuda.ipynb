{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f779b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/datasets/malaysia-ai/starcoderdata-sample/resolve/main/cuda.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cb5572a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (5.2.0)/charset_normalizer (2.0.7) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b6f5a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47973it [00:04, 10594.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46460"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = []\n",
    "\n",
    "with open('cuda.jsonl') as fopen:\n",
    "    for l in tqdm(fopen):\n",
    "        try:\n",
    "            l = json.loads(l)\n",
    "            if len(l['content'].split()) > 5e3:\n",
    "                continue\n",
    "            ls.append(l['content'])\n",
    "        except:\n",
    "            pass\n",
    "len(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e61e8115",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(\n",
    "    '', timeout = 120\n",
    ")\n",
    "\n",
    "\n",
    "def format_prompt(message, history):\n",
    "  prompt = \"<s>\"\n",
    "  for user_prompt, bot_response in history:\n",
    "    prompt += f\"[INST] {user_prompt} [/INST]\"\n",
    "    prompt += f\" {bot_response}</s> \"\n",
    "  prompt += f\"[INST] {message} [/INST]\"\n",
    "  return prompt\n",
    "\n",
    "def format_user(history):\n",
    "    prompt = \"<s>\"\n",
    "    for user_prompt, bot_response in history:\n",
    "        prompt += f\"[INST] {user_prompt} [/INST]\"\n",
    "        prompt += f\" {bot_response}</s> \"\n",
    "    prompt += f\"[INST]\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eda36b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = dict(\n",
    "    temperature=1.0,\n",
    "    max_new_tokens=5120,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.0,\n",
    "    do_sample=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdc207c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘mixtral-cuda’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir mixtral-cuda\n",
    "# !rm -rf mixtral-cuda/*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ef3a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(q, i):\n",
    "    filename = f'mixtral-cuda/{i}.json'\n",
    "    if os.path.exists(filename):\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "    # for _ in range(5):\n",
    "        try:\n",
    "            \n",
    "            prompt = f\"\"\"\n",
    "Please gain inspiration from the following random code snippet to create a high-quality CUDA problem. Present your output in two distinct sections: [Problem Description] and [Solution].\n",
    "\n",
    "CUDA snippet for inspiration:\n",
    "```\n",
    "{q}\n",
    "```\n",
    "\n",
    "Guidelines for each section:\n",
    "\n",
    "1. [Problem Description]: This should be **completely self-contained**, providing all the contextual information one needs to understand and solve the problem. Assume common programming knowledge, but ensure that any specific context, variables, or code snippets pertinent to this problem are explicitly included but do not mention code snippet above.\n",
    "\n",
    "2. [Solution]: Offer a comprehensive, **correct** solution that accurately addresses the [Problem Description] you provided.\n",
    "\"\"\".strip()\n",
    "            formatted_prompt = format_prompt(prompt, [])\n",
    "            stream = client.text_generation(formatted_prompt, **generate_kwargs, stream=False, details=True, return_full_text=False)\n",
    "            output = stream.generated_text\n",
    "            with open(filename, 'w') as fopen:\n",
    "                json.dump(output, fopen)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if 'tokens + `max_new_tokens`' in str(e):\n",
    "                with open(filename, 'w') as fopen:\n",
    "                    json.dump(False, fopen)\n",
    "                break\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f0ac5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumer(queue, name):\n",
    "    while True:\n",
    "        if queue.qsize() == 0:\n",
    "            break\n",
    "        item = queue.get()\n",
    "        answer(*item)\n",
    "    print(f'consumer {name} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "236113b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "queue = Queue()\n",
    "questions = ls\n",
    "urls = [(q, no) for no, q in enumerate(questions)]\n",
    "for u in urls:\n",
    "    queue.put(u)\n",
    "    \n",
    "ori_size = queue.qsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efbd604c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer(*urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aacdb9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Problem Description]\n",
      "\n",
      "Title: CUDA Distance Functions Kernel Optimization\n",
      "\n",
      "In this problem, you are given a set of CUDA functions that calculate different types of distances/differences between two numbers. These functions are defined in the `raft::sparse::distance` namespace and include `Sum`, `SqDiff`, `PDiff`, `Max`, `AtomicAdd`, `AtomicMax`, `Product`, and `AbsDiff`. Each of these functions takes in two template arguments representing the values to be operated on and returns the result of the respective operation.\n",
      "\n",
      "Your task is to optimize the following CUDA kernel, which calculates the pairwise distance between two arrays `d_a` and `d_b` using one of the aforementioned functions (`Sum` in this case):\n",
      "\n",
      "```cuda\n",
      "__global__ void pairwise_distance(value_t *d_output, const value_t *d_a,\n",
      "                                  const value_t *d_b, int64_t size,\n",
      "                                  distance::Sum sum_fn) {\n",
      "  int64_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "  if (idx < size) {\n",
      "    d_output[idx] = sum_fn(d_a[idx], d_b[idx]);\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "The objective is to optimize this kernel for large datasets (arrays of size > 1M elements) while ensuring proper memory access patterns, utilizing shared memory, and possibly coalesced global memory access. You may modify any part of the code, including changing the kernel launch configuration, to optimize the performance.\n",
      "\n",
      "[Solution]\n",
      "\n",
      "To optimize the provided CUDA kernel, first, consider using shared memory to reduce global memory access latency. Shared memory is faster than global memory, and by using it, threads in a block can share data without requiring costly global memory access. In this case, we can use shared memory to load chunks of `d_a` and `d_b` arrays for each block, then perform calculations on these chunks.\n",
      "\n",
      "Here's an optimized version of the kernel using shared memory:\n",
      "\n",
      "```cuda\n",
      "__global__ void optimized_pairwise_distance(value_t *d_output, const value_t *d_a,\n",
      "                                            const value_t *d_b, int64_t size,\n",
      "                                            distance::Sum sum_fn) {\n",
      "  __shared__ value_t s_a[THREADS_PER_BLOCK];\n",
      "  __shared__ value_t s_b[THREADS_PER_BLOCK];\n",
      "\n",
      "  int64_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "  int64_t tid = threadIdx.x;\n",
      "  int64_t grid_size = blockDim.x * gridDim.x;\n",
      "\n",
      "  value_t *output_ptr = d_output + idx;\n",
      "\n",
      "  // Load input data into shared memory\n",
      "  if (tid < THREADS_PER_BLOCK && idx < size) {\n",
      "    s_a[tid] = d_a[idx];\n",
      "    s_b[tid] = d_b[idx];\n",
      "  }\n",
      "\n",
      "  // Synchronize threads to ensure data is loaded\n",
      "  __syncthreads();\n",
      "\n",
      "  // Perform calculations using shared memory\n",
      "  for (int64_t i = tid; i < THREADS_PER_BLOCK; i += grid_size) {\n",
      "    if (i + idx < size) {\n",
      "      output_ptr[0] = sum_fn(s_a[i], s_b[i + idx]);\n",
      "    }\n",
      "  }\n",
      "\n",
      "  // Synchronize threads to ensure calculations are complete\n",
      "  __syncthreads();\n",
      "}\n",
      "```\n",
      "\n",
      "Now, to launch the kernel efficiently, use the following launch configuration:\n",
      "\n",
      "```cuda\n",
      "constexpr int BLOCK_SIZE = 256;\n",
      "constexpr int THREADS_PER_BLOCK = BLOCK_SIZE;\n",
      "constexpr int NUM_BLOCKS = (size + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;\n",
      "\n",
      "optimized_pairwise_distance<<<NUM_BLOCKS, THREADS_PER_BLOCK>>>(d_output, d_a, d_b, size, sum_fn);\n",
      "```\n",
      "\n",
      "Here, we've chosen a block size of 256 threads and adjusted the number of blocks to cover the entire input dataset.\n",
      "\n",
      "In addition to using shared memory, ensure you have proper memory access patterns. By loading data into shared memory, we implicitly reduced the need for coalesced global memory access since each block now works on its own local data.\n",
      "\n",
      "Keep in mind that the choice of distance function (`sum_fn`) may impact performance. Some functions may require more arithmetic operations than others, so you may want to analyze and choose the appropriate function based on the use case.\n"
     ]
    }
   ],
   "source": [
    "with open('mixtral-cuda/0.json') as fopen:\n",
    "    print(json.loads(fopen.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c77c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████████████▋                                                                  | 15364/46460 [2:47:31<13:17:57,  1.54s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 66%|████████████████████████████████████████████████████████████████▉                                 | 30804/46460 [10:22:03<11:24:36,  2.62s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 91%|██████████████████████████████████████████████████████████████████████████████████████████▎        | 42383/46460 [15:47:23<2:10:57,  1.93s/it]"
     ]
    }
   ],
   "source": [
    "max_worker = 100\n",
    "consumers = [Thread(target=consumer, args=(queue,i)) for i in range(max_worker)]\n",
    "for i in range(len(consumers)):\n",
    "    consumers[i].start()\n",
    "    \n",
    "pbar = tqdm(total=ori_size)\n",
    "last_size = 0\n",
    "while True:\n",
    "    size = queue.qsize()\n",
    "    if size == 0:\n",
    "        break\n",
    "    left = ori_size - size\n",
    "    minus = left - last_size\n",
    "    if minus > 0:\n",
    "        pbar.update(minus)\n",
    "        last_size += minus\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1e0e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
