{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33b66619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/datasets/malaysia-ai/filtered-aya-dataset-zsm/resolve/main/filtered-aya_dataset-zsm.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "310ae2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "generate_kwargs = dict(\n",
    "    temperature=1.0,\n",
    "    max_new_tokens=2048,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.0,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "client = InferenceClient(model=\"http://127.0.0.1:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a913e83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('mesolitica/malaysian-mistral-7b-32k-instructions-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3fdb353",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, answers = [], []\n",
    "with open('filtered-aya_dataset-zsm.jsonl') as fopen:\n",
    "    for l in fopen:\n",
    "        l = json.loads(l)\n",
    "        data.append(l['inputs'])\n",
    "        answers.append(l['targets'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1232400b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tuliskan sebuah cerita fiksyen pendek berdasarkan judul \"Bukan Airin\"'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfc6e3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir generated-malaysian-mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e44b3977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(q, i):\n",
    "    filename = f'generated-malaysian-mistral/{i}.json'\n",
    "    if os.path.exists(filename):\n",
    "        return\n",
    "    \n",
    "    formatted_prompt = tokenizer.apply_chat_template([{'role': 'user', 'content': q}], tokenize = False)\n",
    "    while True:\n",
    "        try:\n",
    "            stream = client.text_generation(formatted_prompt, **generate_kwargs, stream=False, details=True, return_full_text=False)\n",
    "            answer = stream.generated_text.strip()\n",
    "            break\n",
    "        except Exception as e:\n",
    "            # print(e)\n",
    "            pass \n",
    "    \n",
    "    d = {\n",
    "        'question': q,\n",
    "        'answer': answer,\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'w') as fopen:\n",
    "        json.dump(d, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c776cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumer(queue, name):\n",
    "    while True:\n",
    "        if queue.qsize() == 0:\n",
    "            break\n",
    "        item = queue.get()\n",
    "        answer(*item)\n",
    "    print(f'consumer {name} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b264a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "queue = Queue()\n",
    "urls = [(q, no) for no, q in enumerate(data)]\n",
    "for u in urls:\n",
    "    queue.put(u)\n",
    "    \n",
    "ori_size = queue.qsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b7aaab2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consumer 19 doneconsumer 17 done\n",
      "consumer 1 done\n",
      "consumer 15 done\n",
      "consumer 3 done\n",
      "consumer 16 done\n",
      "consumer 0 done\n",
      "consumer 10 done\n",
      "consumer 9 done\n",
      "consumer 12 done\n",
      "consumer 11 done\n",
      "consumer 8 done\n",
      "consumer 6 done\n",
      "\n",
      "consumer 14 done\n",
      "consumer 5 done\n",
      "consumer 20 done\n",
      "consumer 21 doneconsumer 22 done\n",
      "\n",
      "consumer 23 done\n",
      "consumer 24 done\n",
      "consumer 25 doneconsumer 26 done\n",
      "\n",
      "consumer 27 done\n",
      "consumer 28 done\n",
      "consumer 29 doneconsumer 30 done\n",
      "\n",
      "consumer 31 done\n",
      "consumer 32 done\n",
      "consumer 33 done\n",
      "consumer 34 done\n",
      "consumer 35 done\n",
      "consumer 36 done\n",
      "consumer 37 done\n",
      "consumer 38 done\n",
      "consumer 39 done\n",
      "consumer 40 done\n",
      "consumer 41 done\n",
      "consumer 42 done\n",
      "consumer 43 done\n",
      "consumer 44 doneconsumer 45 done\n",
      "consumer 46 done\n",
      "\n",
      "consumer 47 done\n",
      "consumer 48 doneconsumer 49 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                      | 0/10073 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "consumer 7 done\n",
      "consumer 2 done\n",
      "consumer 18 done\n",
      "consumer 4 done\n",
      "consumer 13 done\n"
     ]
    }
   ],
   "source": [
    "max_worker = 50\n",
    "consumers = [Thread(target=consumer, args=(queue,i)) for i in range(max_worker)]\n",
    "for i in range(len(consumers)):\n",
    "    consumers[i].start()\n",
    "    \n",
    "pbar = tqdm(total=ori_size)\n",
    "last_size = 0\n",
    "while True:\n",
    "    size = queue.qsize()\n",
    "    if size == 0:\n",
    "        break\n",
    "    left = ori_size - size\n",
    "    minus = left - last_size\n",
    "    if minus > 0:\n",
    "        pbar.update(minus)\n",
    "        last_size += minus\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c0d37b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10073"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "files = glob('generated-malaysian-mistral/*.json')\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "915edbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 10073/10073 [00:00<00:00, 78412.68it/s]\n"
     ]
    }
   ],
   "source": [
    "rejected = []\n",
    "for f in tqdm(files):\n",
    "    try:\n",
    "        with open(f) as fopen:\n",
    "            json.load(fopen)\n",
    "    except:\n",
    "        rejected.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "25e05676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rejected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8555c5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10073"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = []\n",
    "for i in range(len(data)):\n",
    "    with open(f'generated-malaysian-mistral/{i}.json') as fopen:\n",
    "        generated = json.load(fopen)\n",
    "    all_data.append({\n",
    "        'prompt': data[i],\n",
    "        'chosen': answers[i],\n",
    "        'rejected': generated['answer'].strip()\n",
    "    })\n",
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f62ef3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a747c34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 10073\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_list(all_data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "162f5afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e513f3cbfbe645cebdb79d93404078f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8461fb59c1ae4127aa5292a463764b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c034ec3b89405e861d34ad05e2aa92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42acf36ec0e447179bf2311b522ce77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating downloaded metadata with the new split.\n"
     ]
    }
   ],
   "source": [
    "dataset.push_to_hub(repo_id = 'mesolitica/DPO-filtered-aya_dataset-zsm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
