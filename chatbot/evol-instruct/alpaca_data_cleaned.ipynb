{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5cebd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/gururise/AlpacaDataCleaned/main/alpaca_data_cleaned.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7f20345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (5.2.0)/charset_normalizer (2.0.7) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "/home/husein/dev/malaya/malaya/tokenizer.py:214: FutureWarning: Possible nested set at position 3397\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n",
      "/home/husein/dev/malaya/malaya/tokenizer.py:214: FutureWarning: Possible nested set at position 3927\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import instructor\n",
    "import json\n",
    "import os\n",
    "import malaya\n",
    "from pydantic import BaseModel, Field\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "import random\n",
    "\n",
    "minimum_len = 15\n",
    "\n",
    "def simple_cleaning(string):\n",
    "    return re.sub(r'[ ]+', ' ', unidecode(string).replace('\\n', ' ').replace('--', ' ').replace('/', ' ')).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ae9ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_type = 'azure'\n",
    "openai.api_base = \"https://husein-ai2-aiservices.openai.azure.com/\"\n",
    "openai.api_version = \"2023-07-01-preview\"\n",
    "openai.api_key = ''\n",
    "\n",
    "engine = 'gpt-35-turbo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "715722eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prompt):\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "       \n",
    "    ]\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            engine=engine,\n",
    "            messages=messages,\n",
    "            temperature=1.0,\n",
    "            max_tokens=1024,\n",
    "            top_p=1.0\n",
    "        )\n",
    "        return response.choices[0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c9cd32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('alpaca_data_cleaned.json','r') as fopen:\n",
    "    all_objs = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3214a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from depth import createConstraintsPrompt, createDeepenPrompt, createConcretizingPrompt, createReasoningPrompt\n",
    "from breadth import createBreadthPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a0a553f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘output’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir output\n",
    "# !rm output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "502643bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(instruction, index):\n",
    "    filename = f'output/{index}.json'\n",
    "    if os.path.exists(filename):\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        evol_prompts = []\n",
    "        evol_prompts.append(createConstraintsPrompt(instruction))\n",
    "        evol_prompts.append(createDeepenPrompt(instruction))\n",
    "        evol_prompts.append(createConcretizingPrompt(instruction))\n",
    "        evol_prompts.append(createReasoningPrompt(instruction))\n",
    "        evol_prompts.append(createBreadthPrompt(instruction))\n",
    "\n",
    "        selected_evol_prompt = random.choice(evol_prompts)\n",
    "        ins = predict(selected_evol_prompt)\n",
    "        answer = predict(ins + ', jawab dalam bahasa melayu')\n",
    "        d = {\"instruction\":ins,\"output\":answer}\n",
    "        with open(filename, 'w') as fopen:\n",
    "            json.dump(d, fopen)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc278f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = []\n",
    "for cur_obj in all_objs[:len(all_objs) // 2]:\n",
    "    instructions.append(cur_obj['instruction'].strip() + '\\r\\n'+ cur_obj['input'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "855eceee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▍                                | 1224/8627 [00:03<00:26, 282.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'content'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|█████████████▋                        | 3103/8627 [00:09<00:12, 435.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'content'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████                 | 4775/8627 [00:20<00:23, 167.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'content'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|██████████████████████▍               | 5104/8627 [00:21<00:19, 177.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'content'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|████████████████████████████▊          | 6384/8627 [00:38<00:29, 76.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'content'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|██████████████████████████████▍        | 6737/8627 [00:47<00:31, 60.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'content'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|███████████████████████████████▊       | 7034/8627 [00:48<00:19, 79.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'content'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████████████████████████████▊   | 7884/8627 [16:00<2:15:08, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'content'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████████████████████████████▊   | 7896/8627 [18:30<2:22:12, 11.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'content'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|██████████████████████████████████   | 7930/8627 [24:05<1:45:40,  9.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'content'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|███████████████████████████████  | 8121/8627 [1:04:13<26:33:26, 188.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request timed out: HTTPSConnectionPool(host='nous.openai.azure.com', port=443): Read timed out. (read timeout=600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████████████████████████████▎ | 8212/8627 [1:17:51<1:00:06,  8.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'content'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|███████████████████████████████████▋ | 8329/8627 [1:35:55<53:55, 10.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'content'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████████████████████████████████████▎| 8471/8627 [1:55:37<14:45,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'content'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|████████████████████████████████████▍| 8505/8627 [2:00:03<15:15,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'content'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 8627/8627 [2:20:24<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "max_worker = 3\n",
    "for i in tqdm(range(0, len(instructions), max_worker)):\n",
    "    b = instructions[i: i + max_worker]\n",
    "    b = [(ins, i + no) for no, ins in enumerate(b)]\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_worker) as executor:\n",
    "        futures = {executor.submit(generate, f[0], f[1]): f for f in b}\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b3c91e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
