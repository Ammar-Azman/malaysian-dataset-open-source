{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/gururise/AlpacaDataCleaned/main/alpaca_data_cleaned.json\n",
    "filename = 'alpaca_data_cleaned.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('alpaca_data_cleaned.json') as fopen:\n",
    "    data = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51732"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(3))\n",
    "def translate_text(value):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Translate the following text to standard Malaysia: '{value}'\"},\n",
    "            ],\n",
    "        max_tokens=1024,\n",
    "        temperature=0,\n",
    "        )\n",
    "    return response.choices[0][\"message\"][\"content\"].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_item(item):\n",
    "    translated_item = {}\n",
    "    for key, value in item.items():\n",
    "        if value:\n",
    "            translated_value = translate_text(value)\n",
    "            translated_item[key] = translated_value\n",
    "        else:\n",
    "            translated_item[key] = ''\n",
    "    return translated_item\n",
    "\n",
    "# Maximum number of parallel requests\n",
    "MAX_PARALLEL_REQUESTS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|███████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:41<00:00,  3.56it/s]\n",
      "Translating: 100%|███████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:06<00:00,  4.05it/s]\n",
      "Translating: 100%|███████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:36<00:00,  3.62it/s]\n",
      "Translating: 100%|███████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:32<00:00,  3.67it/s]\n",
      "Translating: 100%|███████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:29<00:00,  3.71it/s]\n",
      "Translating: 100%|███████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:42<00:00,  3.53it/s]\n",
      "Translating:  53%|████████████████████████████████████████████▏                                       | 526/1000 [02:22<02:12,  3.57it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Translating: 100%|███████████████████████████████████████████████████████████████████████████████████| 1000/1000 [06:08<00:00,  2.71it/s]\n",
      "Translating: 100%|███████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:05<00:00,  4.07it/s]\n",
      "Translating: 100%|███████████████████████████████████████████████████████████████████████████████████| 1000/1000 [03:49<00:00,  4.36it/s]\n",
      "Translating: 100%|███████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:24<00:00,  3.78it/s]\n",
      "Translating:  60%|██████████████████████████████████████████████████                                  | 596/1000 [02:06<01:54,  3.54it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CHUNK_SIZE = 1000\n",
    "start = 0\n",
    "end = len(data)\n",
    "# Translate the data in chunks of 1000 items\n",
    "for i in range(start, end, CHUNK_SIZE):\n",
    "    start = i\n",
    "    end = i + CHUNK_SIZE\n",
    "    \n",
    "    new_filename = f'{filename}_{start}_to_{end}.json'\n",
    "    if os.path.exists(new_filename):\n",
    "        continue\n",
    "\n",
    "    translated_data = []\n",
    "    data_new = data[start:end]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_PARALLEL_REQUESTS) as executor:\n",
    "        futures = {executor.submit(translate_item, item): item for item in data_new}\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Translating\"):\n",
    "            translated_data.append(future.result())\n",
    "\n",
    "    \n",
    "    with open(new_filename, 'w') as f:\n",
    "        json.dump(translated_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "files = glob('alpaca_data_cleaned.json_*.json')\n",
    "data = []\n",
    "for f in files:\n",
    "    with open(f) as fopen:\n",
    "        data.extend(json.load(fopen))\n",
    "        \n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('translated-alpaca_data_cleaned.json', 'w') as fopen:\n",
    "    json.dump(data, fopen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
