{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8166cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/husein/3.9/lib/python3.9/site-packages/malaya/tokenizer.py:214: FutureWarning: Possible nested set at position 3397\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n",
      "/home/husein/3.9/lib/python3.9/site-packages/malaya/tokenizer.py:214: FutureWarning: Possible nested set at position 3927\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import instructor\n",
    "import json\n",
    "import os\n",
    "import malaya\n",
    "from pydantic import BaseModel, Field\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "import random\n",
    "\n",
    "minimum_len = 15\n",
    "\n",
    "def simple_cleaning(string):\n",
    "    return re.sub(r'[ ]+', ' ', unidecode(string).replace('\\n', ' ').replace('--', ' ').replace('/', ' ')).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7420662",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bd1bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instructor.patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "790c1ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class Conversation(BaseModel):\n",
    "    \"\"\"\n",
    "    generate conversation for user and assistant\n",
    "    \"\"\"\n",
    "    user: str = Field(..., description = 'user question')\n",
    "    assistant: str = Field(..., description = 'respond to user question')\n",
    "        \n",
    "class User(BaseModel):\n",
    "    content: str = Field(..., description = 'user content')\n",
    "        \n",
    "class Assistant(BaseModel):\n",
    "    content: str = Field(..., description = 'assistant content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72129595",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_f = {\n",
    "      \"name\": \"conversation\",\n",
    "      \"description\": \"generate conversation for user and assistant\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"user\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"user question\"\n",
    "          },\n",
    "          \"assistant\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"respond to user question\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"user\", \"assistant\"]\n",
    "      }\n",
    "    }\n",
    "\n",
    "assistant_f = {\n",
    "      \"name\": \"assistant\",\n",
    "      \"description\": \"respond to user question\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"respond\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"respond to user question\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"respond\"]\n",
    "      }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebf4b179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_user(messages):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-16k\",\n",
    "            response_model=User,\n",
    "            messages=messages,\n",
    "            temperature=1.0,\n",
    "            max_tokens=1024,\n",
    "            top_p=0.95,\n",
    "        )\n",
    "        return response.dict()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def predict_assistant(messages):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-16k\",\n",
    "            response_model=Assistant,\n",
    "            messages=messages,\n",
    "            temperature=1.0,\n",
    "            max_tokens=1024,\n",
    "            top_p=0.95,\n",
    "        )\n",
    "        return response.dict()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "def predict(messages):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo-16k\",\n",
    "            messages=messages,\n",
    "            temperature=1.0,\n",
    "            max_tokens=1024,\n",
    "            top_p=1.0\n",
    "        )\n",
    "        return response.choices[0]['message']['content']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd7800c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = 'Above is a conversation between a user and an intelligent assistant. You suppose to simulate conversation between the user and the assistant. Bear in mind your major request is to ask the assistant to generate some material. So you can ask the assistant either to make it more detailed, add more related information, or any other request to improve the generated material. Be creative and diverse in your request. Make the response short and the language casual.'\n",
    "system_malay = 'Above is a conversation between a user and an intelligent assistant. You suppose to simulate conversation between the user and the assistant in malay language. Bear in mind your major request is to ask the assistant to generate some material. So you can ask the assistant either to make it more detailed, add more related information, or any other request to improve the generated material. Be creative and diverse in your request. Make the response short and the language casual.'\n",
    "next_step = 'Above is a conversation between a user and an intelligent assistant. Now suppose you are the user, generate response according to the generated material to continue the conversation.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8955ee04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘ultrachat-wikipedia’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# !rm -rf ultrachat-wikipedia\n",
    "!mkdir ultrachat-wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8b6b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def template(rows):\n",
    "    results = [rows[0]['content'] + '\\n\\n']\n",
    "    for r in rows[1:]:\n",
    "        results.append(f\"{r['role']}: {r['content']}\")\n",
    "    return '\\n'.join(results).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef513724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ultrachat(row, n = 1):\n",
    "    results = [\n",
    "        {'role': 'context', 'content': row['paragraph']},\n",
    "        {'role': 'user', 'content': row['question']},\n",
    "    ]\n",
    "    initial = f\"\"\"\n",
    "    {row['paragraph']}\n",
    "\n",
    "    {row['question']}\n",
    "    \"\"\".strip()\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_malay},\n",
    "        {'role': 'user', 'content': initial},\n",
    "    ]\n",
    "    r = predict(messages)\n",
    "    results.append({\n",
    "        'role': 'assistant', 'content': r,\n",
    "    })\n",
    "    messages.append({\n",
    "        'role': 'assistant', 'content': r,\n",
    "    })\n",
    "    \n",
    "    for _ in range(n):\n",
    "    \n",
    "        messages_temp = messages + [\n",
    "            {'role': 'user', 'content': 'Now suppose you are the user, say something to continue the conversation based on given context. Make the response short and the language casual'}\n",
    "        ]\n",
    "        r2 = predict(messages_temp)\n",
    "        if r2 is None:\n",
    "            break\n",
    "\n",
    "        results.append({\n",
    "            'role': 'user', 'content': r2,\n",
    "        })\n",
    "        messages.append({\n",
    "            'role': 'user', 'content': r2,\n",
    "        })\n",
    "\n",
    "        r = predict(messages)\n",
    "        if r is None:\n",
    "            break\n",
    "        results.append({\n",
    "            'role': 'assistant', 'content': r,\n",
    "        })\n",
    "        messages.append({\n",
    "            'role': 'assistant', 'content': r,\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "480ad6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "files = sorted(glob('wikipedia-question/*.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15e8a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(f):\n",
    "    index = int(os.path.split(f)[1].replace('.json', ''))\n",
    "    filename = f'ultrachat-wikipedia/{index}.json'\n",
    "    if os.path.exists(filename):\n",
    "        return\n",
    "        \n",
    "    with open(f) as fopen:\n",
    "        data = json.load(fopen)\n",
    "    \n",
    "    if len(data['paragraph'].split()) < 100:\n",
    "        return\n",
    "    \n",
    "    r = ultrachat(data, n = random.randint(1, 5))\n",
    "    with open(filename, 'w') as fopen:\n",
    "        json.dump(r, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "302a7fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c5ef9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                    | 12/31686 [09:07<521:39:09, 59.29s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "max_worker = 3\n",
    "for i in tqdm(range(0, len(files), max_worker)):\n",
    "    b = files[i: i + max_worker]\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_worker) as executor:\n",
    "        futures = {executor.submit(generate, f): f for f in b}\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ab67e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9",
   "language": "python",
   "name": "3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
