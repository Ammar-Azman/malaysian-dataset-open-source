{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c3bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/datasets/mesolitica/mixtral-python-data-analytics-instructions/resolve/main/filtered-data-analytics-0.jsonl\n",
    "# !wget https://huggingface.co/datasets/mesolitica/mixtral-python-data-analytics-instructions/resolve/main/filtered-data-analytics-1.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c1d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35cafa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob('filtered-data-analytics-*.jsonl'))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "\n",
    "for f in files:\n",
    "    with open(f) as fopen:\n",
    "        for l in fopen:\n",
    "            l = json.loads(l)\n",
    "            ls.append(l['content'])\n",
    "len(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be7d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(\n",
    "    \"\", timeout = 120\n",
    ")\n",
    "\n",
    "\n",
    "def format_prompt(message, history):\n",
    "  prompt = \"<s>\"\n",
    "  for user_prompt, bot_response in history:\n",
    "    prompt += f\"[INST] {user_prompt} [/INST]\"\n",
    "    prompt += f\" {bot_response}</s> \"\n",
    "  prompt += f\"[INST] {message} [/INST]\"\n",
    "  return prompt\n",
    "\n",
    "def format_user(history):\n",
    "    prompt = \"<s>\"\n",
    "    for user_prompt, bot_response in history:\n",
    "        prompt += f\"[INST] {user_prompt} [/INST]\"\n",
    "        prompt += f\" {bot_response}</s> \"\n",
    "    prompt += f\"[INST]\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca313e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = dict(\n",
    "    temperature=1.0,\n",
    "    max_new_tokens=5120,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.0,\n",
    "    do_sample=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0e9e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir mixtral-data-analytics\n",
    "# !rm -rf mixtral-data-analytics/*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45669cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(q, i):\n",
    "    filename = f'mixtral-data-analytics/{i}.json'\n",
    "    if os.path.exists(filename):\n",
    "        return\n",
    "    \n",
    "    for _ in range(5):\n",
    "        try:\n",
    "            \n",
    "            prompt = f\"\"\"\n",
    "Please gain inspiration from the following random code snippet to create a high-quality programming problem. Present your output in two distinct sections: [Problem Description] and [Solution].\n",
    "\n",
    "Code snippet for inspiration:\n",
    "```\n",
    "{q}\n",
    "```\n",
    "\n",
    "Guidelines for each section:\n",
    "\n",
    "1. [Problem Description]: This should be **completely self-contained**, providing all the contextual information one needs to understand and solve the problem. Assume common programming knowledge, but ensure that any specific context, variables, or code snippets pertinent to this problem are explicitly included.\n",
    "\n",
    "2. [Solution]: Offer a comprehensive, **correct** solution that accurately addresses the [Problem Description] you provided.\n",
    "\"\"\".strip()\n",
    "            formatted_prompt = format_prompt(prompt, [])\n",
    "            stream = client.text_generation(formatted_prompt, **generate_kwargs, stream=False, details=True, return_full_text=False)\n",
    "            output = stream.generated_text\n",
    "            with open(filename, 'w') as fopen:\n",
    "                json.dump(output, fopen)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if 'tokens + `max_new_tokens`' in str(e):\n",
    "                print(e)\n",
    "                with open(filename, 'w') as fopen:\n",
    "                    json.dump(False, fopen)\n",
    "                break\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ffb365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumer(queue, name):\n",
    "    while True:\n",
    "        if queue.qsize() == 0:\n",
    "            break\n",
    "        item = queue.get()\n",
    "        answer(*item)\n",
    "    print(f'consumer {name} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02d52a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "queue = Queue()\n",
    "questions = ls\n",
    "urls = [(q, no) for no, q in enumerate(questions)]\n",
    "for u in urls:\n",
    "    queue.put(u)\n",
    "    \n",
    "ori_size = queue.qsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d75e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_worker = 100\n",
    "consumers = [Thread(target=consumer, args=(queue,i)) for i in range(max_worker)]\n",
    "for i in range(len(consumers)):\n",
    "    consumers[i].start()\n",
    "    \n",
    "pbar = tqdm(total=ori_size)\n",
    "last_size = 0\n",
    "while True:\n",
    "    size = queue.qsize()\n",
    "    if size == 0:\n",
    "        break\n",
    "    left = ori_size - size\n",
    "    minus = left - last_size\n",
    "    if minus > 0:\n",
    "        pbar.update(minus)\n",
    "        last_size += minus\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d88784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(consumers)):\n",
    "    consumers[i].join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c593c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
