{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81f63b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4797dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(\n",
    "    \"https://mixtral.us-west-2.mesolitica.com/\"\n",
    ")\n",
    "\n",
    "\n",
    "def format_prompt(message, history):\n",
    "  prompt = \"<s>\"\n",
    "  for user_prompt, bot_response in history:\n",
    "    prompt += f\"[INST] {user_prompt} [/INST]\"\n",
    "    prompt += f\" {bot_response}</s> \"\n",
    "  prompt += f\"[INST] {message} [/INST]\"\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "153ddb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = dict(\n",
    "    temperature=1.0,\n",
    "    max_new_tokens=5120,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.0,\n",
    "    do_sample=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33df98d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87190"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = []\n",
    "with open('filtered-data-analytics-0.jsonl') as fopen:\n",
    "    for l in fopen:\n",
    "        l = json.loads(l)\n",
    "        ls.append(l)\n",
    "len(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bf5ed4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘filtered-data-analytics-0’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir filtered-data-analytics-0\n",
    "# !rm filtered-data-analytics-0/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc703e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1108/87190 [11:45:05<993:03:18, 41.53s/it] "
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(ls))):\n",
    "    filename = os.path.join('filtered-data-analytics-0', f'{i}.json')\n",
    "    if os.path.exists(filename):\n",
    "        continue\n",
    "    prompt = \"\"\"\n",
    "Please gain inspiration from the following random code snippet to create a high-quality programming problem. Present your output in two distinct sections: [Problem Description] and [Solution].\n",
    "\n",
    "Code snippet for inspiration:\n",
    "```\n",
    "{code}\n",
    "```\n",
    "\n",
    "Guidelines for each section:\n",
    "\n",
    "1. [Problem Description]: This should be **completely self-contained**, providing all the contextual information one needs to understand and solve the problem. Assume common programming knowledge, but ensure that any specific context, variables, or code snippets pertinent to this problem are explicitly included.\n",
    "\n",
    "2. [Solution]: Offer a comprehensive, **correct** solution that accurately addresses the [Problem Description] you provided.\n",
    "\"\"\".strip()\n",
    "\n",
    "    new_text = prompt.format(code = ls[i]['content'].strip())\n",
    "    formatted_prompt = format_prompt(new_text, [])\n",
    "    try:\n",
    "        stream = client.text_generation(formatted_prompt, **generate_kwargs, stream=False, details=True, return_full_text=False)\n",
    "        with open(filename, 'w') as fopen:\n",
    "            fopen.write(f'{json.dumps(stream.generated_text.strip())}\\n')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d27538a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json\n",
    "\n",
    "files = glob('filtered-data-analytics-0/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7136953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**[Problem Description]**\n",
      "\n",
      "A script is provided to gather and organize benchmark results for deep learning model training and inference using different hardware configurations and precision settings. The script looks for logs in the specified path, and organizes the results into CSV files for both training and inference throughput, as well as training batch size. The logs are organized based on a naming convention that maps to different hardware systems and GPU counts.\n",
      "\n",
      "However, the script currently only supports a predefined list of models (`list_test`) and hardware systems (`list_system`). Your task is to modify the script to make it more flexible and configurable, so that users can easily add new models, hardware systems, and other variables to be benchmarked.\n",
      "\n",
      "**[Problem Requirements]**\n",
      "\n",
      "1. Make the `list_test` and `list_system` variables configurable by users.\n",
      "2. Add support for users to specify their own list of models and hardware systems.\n",
      "3. Allow users to easily add new variables to be benchmarked, such as different batch sizes, learning rates, or optimization algorithms.\n",
      "\n",
      "**[Solution]**\n",
      "\n",
      "To make the script more flexible and configurable, we can modify the script in the following ways:\n",
      "\n",
      "1. Create a new configuration file (e.g., `config.yaml`) to store the user-defined variables.\n",
      "2. Modify the script to read the configuration file and use the user-defined variables.\n",
      "\n",
      "Here's an example configuration file (`config.yaml`):\n",
      "```yaml\n",
      "list_test:\n",
      "  - alexnet\n",
      "  - inception3\n",
      "  - inception4\n",
      "  - resnet152\n",
      "  - resnet50\n",
      "  - vgg16\n",
      "\n",
      "list_system:\n",
      "  - i7-6850K-GeForce_GTX_1080_Ti:\n",
      "      gpus: [1]\n",
      "      names: [GTX 1080Ti]\n",
      "  - i7-9750H-GeForce_RTX_2070_with_Max-Q_Design_XLA_TF1_15:\n",
      "      gpus: [1]\n",
      "      names: [RTX 2070 MAX-Q]\n",
      "  # ... add more hardware systems here\n",
      "\n",
      "other_variables:\n",
      "  batch_sizes: [16, 32, 64, 128]\n",
      "  learning_rates: [0.001, 0.01, 0.1]\n",
      "  optimizers: [sgd, adam, adagrad]\n",
      "```\n",
      "We can modify the script to read this configuration file using the `PyYAML` library:\n",
      "```python\n",
      "import yaml\n",
      "\n",
      "with open('config.yaml', 'r') as f:\n",
      "    config = yaml.safe_load(f)\n",
      "\n",
      "list_test = config['list_test']\n",
      "list_system = config['list_system']\n",
      "other_variables = config['other_variables']\n",
      "```\n",
      "We can then modify the `create_row_throughput` and `create_row_batch_size` functions to accept these user-defined variables as input arguments:\n",
      "```python\n",
      "def create_row_throughput(path_logs, mode, data, precision, key, num_gpu, name, df, other_variables, is_train=True):\n",
      "    # ... use `other_variables` to add support for new variables\n",
      "\n",
      "def create_row_batch_size(path_logs, mode, data, precision, key, num_gpu, name, df, other_variables, is_train=True):\n",
      "    # ... use `other_variables` to add support for new variables\n",
      "```\n",
      "Finally, we can modify the `main` function to pass these variables to the `create_row_*` functions:\n",
      "```python\n",
      "def main():\n",
      "    # ... read configuration file and load variables\n",
      "\n",
      "    # Train Throughput\n",
      "    df_throughput = pd.DataFrame(index=list_row, columns=columns)\n",
      "\n",
      "    for key in list_system:\n",
      "        # list_gpus = list_system[key][0]\n",
      "        for (num_gpu, name) in zip(list_system[key]['gpus'], list_system[key]['names']):\n",
      "            create_row_throughput(args.path, args.mode, args.data, args.precision, key, num_gpu, name, df_throughput, other_variables, True)\n",
      "\n",
      "    df_throughput.index.name = 'name_gpu'\n",
      "\n",
      "    df_throughput.to_csv('tf-train-throughput-' + args.precision + '.csv')\n",
      "\n",
      "    # ... continue processing other variables using similar logic\n",
      "```\n",
      "These modifications make the script much more flexible and configurable, allowing users to easily add new models, hardware systems, and other variables to be benchmarked.\n"
     ]
    }
   ],
   "source": [
    "with open(files[4]) as fopen:\n",
    "    print(json.load(fopen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6a2daa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
