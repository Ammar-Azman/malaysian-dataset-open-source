{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/datasets/cnn_dailymail/resolve/11343c3752184397d56efc19a8a7cceb68089318/data/cnn_stories.tgz\n",
    "# !wget https://huggingface.co/datasets/cnn_dailymail/resolve/11343c3752184397d56efc19a8a7cceb68089318/data/dailymail_stories.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/abisee/cnn-dailymail/master/url_lists/all_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar zxf cnn_stories.tgz\n",
    "# !tar zxf dailymail_stories.tgz\n",
    "# !rm cnn_stories.tgz dailymail_stories.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_url_hashes(path):\n",
    "    \"\"\"Get hashes of urls in file.\"\"\"\n",
    "    urls = _read_text_file_path(path)\n",
    "\n",
    "    def url_hash(u):\n",
    "        h = hashlib.sha1()\n",
    "        try:\n",
    "            u = u.encode(\"utf-8\")\n",
    "        except UnicodeDecodeError:\n",
    "            logger.error(\"Cannot hash url: %s\", u)\n",
    "        h.update(u)\n",
    "        return h.hexdigest()\n",
    "\n",
    "    return {url_hash(u) for u in urls}\n",
    "\n",
    "\n",
    "def _get_hash_from_path(p):\n",
    "    \"\"\"Extract hash from path.\"\"\"\n",
    "    return os.path.splitext(os.path.basename(p))[0]\n",
    "\n",
    "\n",
    "DM_SINGLE_CLOSE_QUOTE = \"\\u2019\"  # unicode\n",
    "DM_DOUBLE_CLOSE_QUOTE = \"\\u201d\"\n",
    "# acceptable ways to end a sentence\n",
    "END_TOKENS = [\".\", \"!\", \"?\", \"...\", \"'\", \"`\", '\"', DM_SINGLE_CLOSE_QUOTE, DM_DOUBLE_CLOSE_QUOTE, \")\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_text_file_path(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f]\n",
    "    return lines\n",
    "\n",
    "\n",
    "def _read_text_file(file):\n",
    "    return [line.strip() for line in file]\n",
    "\n",
    "\n",
    "def _get_art_abs(story_file, tfds_version = \"1.0\"):\n",
    "    \"\"\"Get abstract (highlights) and article from a story file path.\"\"\"\n",
    "    # Based on https://github.com/abisee/cnn-dailymail/blob/master/\n",
    "    #     make_datafiles.py\n",
    "\n",
    "    lines = _read_text_file(story_file)\n",
    "\n",
    "    # The github code lowercase the text and we removed it in 3.0.0.\n",
    "\n",
    "    # Put periods on the ends of lines that are missing them\n",
    "    # (this is a problem in the dataset because many image captions don't end in\n",
    "    # periods; consequently they end up in the body of the article as run-on\n",
    "    # sentences)\n",
    "    def fix_missing_period(line):\n",
    "        \"\"\"Adds a period to a line that is missing a period.\"\"\"\n",
    "        if \"@highlight\" in line:\n",
    "            return line\n",
    "        if not line:\n",
    "            return line\n",
    "        if line[-1] in END_TOKENS:\n",
    "            return line\n",
    "        return line + \" .\"\n",
    "\n",
    "    lines = [fix_missing_period(line) for line in lines]\n",
    "\n",
    "    # Separate out article and abstract sentences\n",
    "    article_lines = []\n",
    "    highlights = []\n",
    "    next_is_highlight = False\n",
    "    for line in lines:\n",
    "        if not line:\n",
    "            continue  # empty line\n",
    "        elif line.startswith(\"@highlight\"):\n",
    "            next_is_highlight = True\n",
    "        elif next_is_highlight:\n",
    "            highlights.append(line)\n",
    "        else:\n",
    "            article_lines.append(line)\n",
    "\n",
    "    # Make article into a single string\n",
    "    article = \" \".join(article_lines)\n",
    "\n",
    "    if tfds_version >= \"2.0.0\":\n",
    "        abstract = \"\\n\".join(highlights)\n",
    "    else:\n",
    "        abstract = \" \".join(highlights)\n",
    "\n",
    "    return article, abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = _get_url_hashes('all_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92579, 219506)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnns = glob('cnn/*/*')\n",
    "dailymails = glob('dailymail/*/*')\n",
    "len(cnns), len(dailymails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = cnns + dailymails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('mesolitica/finetune-translation-t5-small-standard-bahasa-cased')\n",
    "model = T5ForConditionalGeneration.from_pretrained('mesolitica/finetune-translation-t5-small-standard-bahasa-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saya suka'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(f'terjemah Inggeris ke Melayu: i like', return_tensors = 'pt')\n",
    "outputs = model.generate(input_ids.cuda(), max_length = 10000)\n",
    "tokenizer.decode(outputs[0],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘translated-train’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "directory = 'translated-train'\n",
    "!mkdir {directory}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 312085/312085 [00:00<00:00, 981251.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "287227"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "selected_hash = []\n",
    "for f in tqdm(files):\n",
    "    path = f\n",
    "    hash_from_path = _get_hash_from_path(path)\n",
    "    if hash_from_path in urls:\n",
    "        selected_hash.append(f)\n",
    "        \n",
    "selected_hash = sorted(selected_hash)\n",
    "len(selected_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.8/site-packages/malaya/tokenizer.py:202: FutureWarning: Possible nested set at position 3361\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n",
      "/home/husein/.local/lib/python3.8/site-packages/malaya/tokenizer.py:202: FutureWarning: Possible nested set at position 3879\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n",
      " 92%|██████████████████████████▋  | 264908/287227 [27:17:23<10:49:18,  1.75s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|████████████████████████████████| 287227/287227 [41:37:54<00:00,  1.92it/s]\n"
     ]
    }
   ],
   "source": [
    "import malaya\n",
    "import json\n",
    "\n",
    "maxlen = 128\n",
    "\n",
    "for f in tqdm(selected_hash):\n",
    "    path = f\n",
    "    hash_from_path = _get_hash_from_path(path)\n",
    "    new_path = os.path.join(directory, hash_from_path)\n",
    "    if os.path.exists(new_path):\n",
    "        continue\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    file = _read_text_file_path(path)\n",
    "    article, highlights = _get_art_abs(file)\n",
    "    splitted = malaya.text.function.split_into_sentences(article, minimum_length = 2)\n",
    "\n",
    "    r, temp = [], []\n",
    "    for s in splitted:\n",
    "        temp.append(s)\n",
    "        if len(' '.join(temp).split()) > maxlen:\n",
    "            r.append(temp)\n",
    "            temp = []\n",
    "\n",
    "    if len(temp):\n",
    "        r.append(temp)\n",
    "\n",
    "    articles = []\n",
    "    for r_ in r:\n",
    "        s = ' '.join(r_)\n",
    "        input_ids = tokenizer.encode(f'terjemah Inggeris ke Melayu: {s}', return_tensors = 'pt').cuda()\n",
    "        outputs = model.generate(input_ids, max_length = 10000)\n",
    "        articles.append(tokenizer.decode(outputs[0],skip_special_tokens=True))\n",
    "\n",
    "    input_ids = tokenizer.encode(f'terjemah Inggeris ke Melayu: {highlights}', return_tensors = 'pt').cuda()\n",
    "    outputs = model.generate(input_ids, max_length = 10000)\n",
    "    t_highlights = tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
    "\n",
    "    trans = {'article': articles, 'highlight': t_highlights}\n",
    "    with open(new_path, 'w') as fopen:\n",
    "        json.dump(trans, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
