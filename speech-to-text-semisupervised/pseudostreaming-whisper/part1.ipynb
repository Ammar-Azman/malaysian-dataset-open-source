{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f90a72d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install git+https://github.com/mesolitica/malaya-speech@8fe9cfea37fc32ac63399d9ae5fff22af697f4be\n",
    "# !pip3 install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6acfa4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfad0140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.bart.modeling_bart import shift_tokens_right\n",
    "from transformers.models.gpt2 import modeling_gpt2\n",
    "import malaya\n",
    "from malaya.text.normalization import cardinal\n",
    "import malaya_speech\n",
    "from malaya_speech.utils.subword import merge_sentencepiece_tokens\n",
    "import re\n",
    "import itertools\n",
    "import unicodedata\n",
    "import json\n",
    "import numpy as np\n",
    "from num2words import num2words\n",
    "from streaming import MDSWriter, LocalDataset\n",
    "\n",
    "tokenizer = malaya.tokenizer.Tokenizer(hypen = False, parliament = False, time = False, time_pukul = False,\n",
    "                                      temperature = False, distance = False, volume = False, duration = False,\n",
    "                                      weight = False, date = False, money = False)\n",
    "\n",
    "def cardinal_en(x):\n",
    "    cp_x = x[:]\n",
    "    try:\n",
    "        if re.match('.*[A-Za-z]+.*', x):\n",
    "            return x\n",
    "        x = re.sub(',', '', x, count=10)\n",
    "\n",
    "        if re.match('.+\\\\..*', x):\n",
    "            x = num2words(float(x))\n",
    "        elif re.match('\\\\..*', x):\n",
    "            x = num2words(float(x))\n",
    "        else:\n",
    "            x = num2words(int(x))\n",
    "        x = re.sub('-', ' ', x, count=10)\n",
    "        return x\n",
    "    except BaseException as e:\n",
    "        logger.debug(traceback.format_exc())\n",
    "        return cp_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fe0fc28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pada lima belas ogos seribu sembilan ratus empat puluh tujuh puluh % pihak berikat menyerang perancis selatan serangan ini dipanggil operation dragoon'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_and_replace(t, en = False):\n",
    "    tokenized = tokenizer.tokenize(t)\n",
    "    for i in range(len(tokenized)):\n",
    "        if en:\n",
    "            c = cardinal_en(tokenized[i])\n",
    "        else:\n",
    "            c = cardinal(tokenized[i])\n",
    "        if c != tokenized[i]:\n",
    "            tokenized[i] = c\n",
    "    return ' '.join(tokenized)\n",
    "\n",
    "tokenize_and_replace('pada 15 ogos 1940 70% pihak berikat menyerang perancis selatan serangan ini dipanggil operation dragoon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "898c1415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pada fifteen ogos one thousand, nine hundred and forty pihak berikat menyerang perancis selatan serangan ini dipanggil operation dragoon'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_replace(\n",
    "    'pada 15 ogos 1940 pihak berikat menyerang perancis selatan serangan ini dipanggil operation dragoon',\n",
    "en = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c57f90b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = [\" \", \"a\", \"e\", \"n\", \"i\", \"t\", \"o\", \"u\", \"s\", \"k\", \"r\", \"l\", \"h\", \"d\", \"m\", \"g\", \"y\", \"b\", \"p\", \"w\", \"c\", \"f\", \"j\", \"v\", \"z\", \"0\", \"1\", \"x\", \"2\", \"q\", \"5\", \"3\", \"4\", \"6\", \"9\", \"8\", \"7\"]\n",
    "sr = 16000\n",
    "\n",
    "def preprocessing_text(string, en = False):\n",
    "    \n",
    "    string = tokenize_and_replace(string, en = en)\n",
    "    string = unicodedata.normalize('NFC', string.lower())\n",
    "    string = ''.join([c if c in vocabs else ' ' for c in string])\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    string = (\n",
    "        ''.join(''.join(s)[:2] for _, s in itertools.groupby(string))\n",
    "    )\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef148c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = malaya_speech.force_alignment.transducer.huggingface(model = 'mesolitica/conformer-medium-mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd2bcf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d52be87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2221856"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = LocalDataset('pseudolabel')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6df76635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '{\"predict_ms\": \"<|startoftranscript|><|ms|><|transcribe|> anda tahu keuntungan boleh lebih tinggi daripada keuntungan kewangan rumah maka saya tidak akan mencari dalam akaun saya akan mencari ke dalam ethereum atau beberapa crypto punks bergantung pada faktor risiko anda kerana rumah kajang dihantar tidak mengganggu dsr saya sejauh ini jadi sekarang apa posisi saya untuk mendapatkan kewangan ketiga jadi mungkin setelah melihat sekeliling saya menemui seorang penjual yang dapat menutupi perhubungan tetapi bank hanya menerima 70% dari itu saya boleh membayar perbezaan dengan menggunakan wang ini kerana sekali lagi ia menyusahkan saya dan aset tetapi jika anda tidak selesa dengan mencari<|endoftext|>\", \"predict_en\": \"<|startoftranscript|><|en|><|transcribe|> you know the returns can be higher than the savings of the housing loan interest then i will not put in the account i\\'ll put into ethereum or some crypto punks depending on your risk factor then because of the kajang house being let out it does not affect my dsr as much so now what\\'s my position on getting the third loan so maybe after looking around i found one where the renter can somewhat cover the installment but the bank only approved 70 percent of it i\\'m okay to pay the difference using this cash because again it somewhat secures me an asset but if you are not<|endoftext|>\", \"score_ms\": 11.25, \"score_en\": 8.875, \"audio_filename\": \"output-audio/1-0-0.mp3\", \"filename\": \"output/1-0.json\", \"i\": 0, \"repeat_ms\": false, \"repeat_en\": false}\\n'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c988e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir force-alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1196fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(results):\n",
    "    diag = np.diag(results['alignment']).tolist()\n",
    "    subwords_alignment = results['subwords_alignment']\n",
    "\n",
    "    for i in range(len(subwords_alignment)):\n",
    "        subwords_alignment[i]['start'] = float(subwords_alignment[i]['start'])\n",
    "        subwords_alignment[i]['end'] = float(subwords_alignment[i]['end'])\n",
    "        subwords_alignment[i]['score'] = float(subwords_alignment[i]['score'])\n",
    "        \n",
    "    return diag, subwords_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14b3574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 223/1110928 [06:39<575:20:58,  1.86s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range((len(dataset) // 2) * 0, (len(dataset) // 2) * 1, 1)):\n",
    "    filename = os.path.join('force-alignment', f'{i}.json')\n",
    "    if os.path.exists(filename):\n",
    "        continue\n",
    "    \n",
    "    l = json.loads(dataset[i]['text'])\n",
    "    if not os.path.exists(l['audio_filename']):\n",
    "        continue\n",
    "    \n",
    "    y, _ = malaya_speech.load(l['audio_filename'])\n",
    "    t_ms = l['predict_ms'][41:-13].strip()\n",
    "    t_en = l['predict_en'][41:-13].strip()\n",
    "    try:\n",
    "        p_ms = preprocessing_text(t_ms)\n",
    "    except:\n",
    "        p_ms = None\n",
    "    try:\n",
    "        p_en = preprocessing_text(t_en, en = True)\n",
    "    except:\n",
    "        p_en = None\n",
    "    \n",
    "    try:\n",
    "        results_ms = model.predict(y, p_ms)\n",
    "        diag_ms, subwords_alignment_ms = convert(results_ms)\n",
    "    except:\n",
    "        diag_ms = None\n",
    "        subwords_alignment_ms = None\n",
    "        \n",
    "    try:\n",
    "        results_en = model.predict(y, p_en)\n",
    "        diag_en, subwords_alignment_en = convert(results_en)\n",
    "    except:\n",
    "        diag_en = None\n",
    "        subwords_alignment_en = None\n",
    "\n",
    "    d = {\n",
    "        'p_ms': p_ms,\n",
    "        'p_en': p_en,\n",
    "        'diag_ms': diag_ms,\n",
    "        'subwords_alignment_ms': subwords_alignment_ms,\n",
    "        'diag_en': diag_en,\n",
    "        'subwords_alignment_en': subwords_alignment_en,\n",
    "    }\n",
    "\n",
    "    with open(filename, 'w') as fopen:\n",
    "        json.dump(d, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f2e05892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "files = glob('force-alignment/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6b5fcc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(files[0]) as fopen:\n",
    "    data = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b0e13b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['p_ms', 'p_en', 'diag_ms', 'subwords_alignment_ms', 'diag_en', 'subwords_alignment_en'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0aeb9baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split, temp = [], []\n",
    "diag = data['diag_ms']\n",
    "for no, r in enumerate(data['subwords_alignment_ms']):\n",
    "    if r['score'] >= 0.05 or diag[no] > 0.1:\n",
    "        temp.append(r)\n",
    "    \n",
    "    else:\n",
    "        if len(temp):\n",
    "            split.append(temp)\n",
    "            temp = []\n",
    "            \n",
    "if len(temp):\n",
    "    split.append(temp)\n",
    "    \n",
    "selected = []\n",
    "for s in split:\n",
    "    start = s[0]['start']\n",
    "    end = s[-1]['start']\n",
    "    if end - start >= 0.5:\n",
    "        seq = [s_['text'] for s_ in s]\n",
    "        merged = model.tokenizer.sp_model.Decode(model.tokenizer.sp_model.PieceToId(seq))\n",
    "        selected.append((merged, start, end))\n",
    "        \n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1d2a426b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split, temp = [], []\n",
    "diag = data['diag_en']\n",
    "for no, r in enumerate(data['subwords_alignment_en']):\n",
    "    if r['score'] >= 0.05 or diag[no] > 0.1:\n",
    "        temp.append(r)\n",
    "    \n",
    "    else:\n",
    "        if len(temp):\n",
    "            split.append(temp)\n",
    "            temp = []\n",
    "            \n",
    "if len(temp):\n",
    "    split.append(temp)\n",
    "    \n",
    "selected = []\n",
    "for s in split:\n",
    "    start = s[0]['start']\n",
    "    a = [s[0]]\n",
    "    for s_ in s[1:]:\n",
    "        a.append(s_)\n",
    "        end = s_['end'] + 0.1\n",
    "        if end - start >= 0.5:\n",
    "            seq = [s__['text'] for s__ in a]\n",
    "            merged = model.tokenizer.sp_model.Decode(model.tokenizer.sp_model.PieceToId(seq))\n",
    "            selected.append((merged, start, end))\n",
    "        \n",
    "len(selected)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
