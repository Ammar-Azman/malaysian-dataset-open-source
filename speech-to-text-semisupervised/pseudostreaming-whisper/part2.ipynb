{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cdad659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install git+https://github.com/mesolitica/malaya-speech@8fe9cfea37fc32ac63399d9ae5fff22af697f4be\n",
    "# !pip3 install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65dd9660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14b6309d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot import beam_search_ops from Tensorflow 1, ['malaya.jawi_rumi.deep_model', 'malaya.phoneme.deep_model', 'malaya.rumi_jawi.deep_model', 'malaya.stem.deep_model'] for stemmer will not available to use, make sure Tensorflow 1 version >= 1.15\n",
      "`pyaudio` is not available, `malaya_speech.streaming.pyaudio` is not able to use.\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.bart.modeling_bart import shift_tokens_right\n",
    "from transformers.models.gpt2 import modeling_gpt2\n",
    "import malaya\n",
    "from malaya.text.normalization import cardinal\n",
    "import malaya_speech\n",
    "from malaya_speech.utils.subword import merge_sentencepiece_tokens\n",
    "import re\n",
    "import itertools\n",
    "import unicodedata\n",
    "import json\n",
    "import numpy as np\n",
    "from num2words import num2words\n",
    "from streaming import MDSWriter, LocalDataset\n",
    "\n",
    "tokenizer = malaya.tokenizer.Tokenizer(hypen = False, parliament = False, time = False, time_pukul = False,\n",
    "                                      temperature = False, distance = False, volume = False, duration = False,\n",
    "                                      weight = False, date = False, money = False)\n",
    "\n",
    "def cardinal_en(x):\n",
    "    cp_x = x[:]\n",
    "    try:\n",
    "        if re.match('.*[A-Za-z]+.*', x):\n",
    "            return x\n",
    "        x = re.sub(',', '', x, count=10)\n",
    "\n",
    "        if re.match('.+\\\\..*', x):\n",
    "            x = num2words(float(x))\n",
    "        elif re.match('\\\\..*', x):\n",
    "            x = num2words(float(x))\n",
    "        else:\n",
    "            x = num2words(int(x))\n",
    "        x = re.sub('-', ' ', x, count=10)\n",
    "        return x\n",
    "    except BaseException as e:\n",
    "        logger.debug(traceback.format_exc())\n",
    "        return cp_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6f929c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pada lima belas ogos seribu sembilan ratus empat puluh 70% pihak berikat menyerang perancis selatan serangan ini dipanggil operation dragoon'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_and_replace(t, en = False):\n",
    "    tokenized = tokenizer.tokenize(t)\n",
    "    for i in range(len(tokenized)):\n",
    "        if en:\n",
    "            c = cardinal_en(tokenized[i])\n",
    "        else:\n",
    "            c = cardinal(tokenized[i])\n",
    "        if c != tokenized[i]:\n",
    "            tokenized[i] = c\n",
    "    return ' '.join(tokenized)\n",
    "\n",
    "tokenize_and_replace('pada 15 ogos 1940 70% pihak berikat menyerang perancis selatan serangan ini dipanggil operation dragoon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0465d8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pada fifteen ogos one thousand, nine hundred and forty pihak berikat menyerang perancis selatan serangan ini dipanggil operation dragoon'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_replace(\n",
    "    'pada 15 ogos 1940 pihak berikat menyerang perancis selatan serangan ini dipanggil operation dragoon',\n",
    "en = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ff3631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = [\" \", \"a\", \"e\", \"n\", \"i\", \"t\", \"o\", \"u\", \"s\", \"k\", \"r\", \"l\", \"h\", \"d\", \"m\", \"g\", \"y\", \"b\", \"p\", \"w\", \"c\", \"f\", \"j\", \"v\", \"z\", \"0\", \"1\", \"x\", \"2\", \"q\", \"5\", \"3\", \"4\", \"6\", \"9\", \"8\", \"7\"]\n",
    "sr = 16000\n",
    "\n",
    "def preprocessing_text(string, en = False):\n",
    "    \n",
    "    string = tokenize_and_replace(string, en = en)\n",
    "    string = unicodedata.normalize('NFC', string.lower())\n",
    "    string = ''.join([c if c in vocabs else ' ' for c in string])\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    string = (\n",
    "        ''.join(''.join(s)[:2] for _, s in itertools.groupby(string))\n",
    "    )\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "698a3d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = malaya_speech.force_alignment.transducer.huggingface(model = 'mesolitica/conformer-medium-mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87285bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6cbdbee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2221856"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = LocalDataset('pseudolabel')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10c6ca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘force-alignment’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir force-alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca1bca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(results):\n",
    "    diag = np.diag(results['alignment']).tolist()\n",
    "    subwords_alignment = results['subwords_alignment']\n",
    "\n",
    "    for i in range(len(subwords_alignment)):\n",
    "        subwords_alignment[i]['start'] = float(subwords_alignment[i]['start'])\n",
    "        subwords_alignment[i]['end'] = float(subwords_alignment[i]['end'])\n",
    "        subwords_alignment[i]['score'] = float(subwords_alignment[i]['score'])\n",
    "        \n",
    "    return diag, subwords_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc7048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 208/1110928 [06:22<589:03:45,  1.91s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range((len(dataset) // 2) * 1, (len(dataset) // 2) * 2, 1)):\n",
    "    filename = os.path.join('force-alignment', f'{i}.json')\n",
    "    if os.path.exists(filename):\n",
    "        continue\n",
    "    \n",
    "    l = json.loads(dataset[i]['text'])\n",
    "    if not os.path.exists(l['audio_filename']):\n",
    "        continue\n",
    "    \n",
    "    y, _ = malaya_speech.load(l['audio_filename'])\n",
    "    t_ms = l['predict_ms'][41:-13].strip()\n",
    "    t_en = l['predict_en'][41:-13].strip()\n",
    "    try:\n",
    "        p_ms = preprocessing_text(t_ms)\n",
    "    except:\n",
    "        p_ms = None\n",
    "    try:\n",
    "        p_en = preprocessing_text(t_en, en = True)\n",
    "    except:\n",
    "        p_en = None\n",
    "    \n",
    "    try:\n",
    "        results_ms = model.predict(y, p_ms)\n",
    "        diag_ms, subwords_alignment_ms = convert(results_ms)\n",
    "    except:\n",
    "        diag_ms = None\n",
    "        subwords_alignment_ms = None\n",
    "        \n",
    "    try:\n",
    "        results_en = model.predict(y, p_en)\n",
    "        diag_en, subwords_alignment_en = convert(results_en)\n",
    "    except:\n",
    "        diag_en = None\n",
    "        subwords_alignment_en = None\n",
    "\n",
    "    d = {\n",
    "        'p_ms': p_ms,\n",
    "        'p_en': p_en,\n",
    "        'diag_ms': diag_ms,\n",
    "        'subwords_alignment_ms': subwords_alignment_ms,\n",
    "        'diag_en': diag_en,\n",
    "        'subwords_alignment_en': subwords_alignment_en,\n",
    "    }\n",
    "\n",
    "    with open(filename, 'w') as fopen:\n",
    "        json.dump(d, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f5dd9826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "files = glob('force-alignment/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b9c18e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(files[0]) as fopen:\n",
    "    data = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7bcc7cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['p_ms', 'p_en', 'diag_ms', 'subwords_alignment_ms', 'diag_en', 'subwords_alignment_en'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9a1d869a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split, temp = [], []\n",
    "diag = data['diag_ms']\n",
    "for no, r in enumerate(data['subwords_alignment_ms']):\n",
    "    if r['score'] >= 0.05 or diag[no] > 0.1:\n",
    "        temp.append(r)\n",
    "    \n",
    "    else:\n",
    "        if len(temp):\n",
    "            split.append(temp)\n",
    "            temp = []\n",
    "            \n",
    "if len(temp):\n",
    "    split.append(temp)\n",
    "    \n",
    "selected = []\n",
    "for s in split:\n",
    "    start = s[0]['start']\n",
    "    end = s[-1]['start']\n",
    "    if end - start >= 0.5:\n",
    "        seq = [s_['text'] for s_ in s]\n",
    "        merged = model.tokenizer.sp_model.Decode(model.tokenizer.sp_model.PieceToId(seq))\n",
    "        selected.append((merged, start, end))\n",
    "        \n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cf0e8ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split, temp = [], []\n",
    "diag = data['diag_en']\n",
    "for no, r in enumerate(data['subwords_alignment_en']):\n",
    "    if r['score'] >= 0.05 or diag[no] > 0.1:\n",
    "        temp.append(r)\n",
    "    \n",
    "    else:\n",
    "        if len(temp):\n",
    "            split.append(temp)\n",
    "            temp = []\n",
    "            \n",
    "if len(temp):\n",
    "    split.append(temp)\n",
    "    \n",
    "selected = []\n",
    "for s in split:\n",
    "    start = s[0]['start']\n",
    "    a = [s[0]]\n",
    "    for s_ in s[1:]:\n",
    "        a.append(s_)\n",
    "        end = s_['end'] + 0.1\n",
    "        if end - start >= 0.5:\n",
    "            seq = [s__['text'] for s__ in a]\n",
    "            merged = model.tokenizer.sp_model.Decode(model.tokenizer.sp_model.PieceToId(seq))\n",
    "            selected.append((merged, start, end))\n",
    "        \n",
    "len(selected)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
