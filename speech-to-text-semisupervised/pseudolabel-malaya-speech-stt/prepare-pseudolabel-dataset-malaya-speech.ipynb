{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa088073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoTokenizer, WhisperConfig\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "config = WhisperConfig.from_pretrained('openai/whisper-large-v3')\n",
    "maxlen = config.max_length - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c431b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('openai/whisper-large-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a39b6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "sr = 16000\n",
    "audio = Audio(sampling_rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1db5783c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38052"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = sorted(glob('output-malaya/*.json'), key = lambda x: int(x.split('-')[-1].replace('.json', '')))\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a02855e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output-malaya/2-0.json'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e26c008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(files[0]) as fopen:\n",
    "    d = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfd6657f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|startoftranscript|><|ms|><|transcribe|><|0.00|> kerajaan persekutuan<|1.46|><|1.46|> dan banyak masalah hubungan<|3.96|><|3.96|> antara kerajaan negeri dan<|5.70|><|endoftext|>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array(d[0]['predict_ms'])\n",
    "a = a[a != 50257].tolist() + [50257]\n",
    "t = tokenizer.decode(a, decode_with_timestamps = True).strip()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5e9a473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.00', ' kerajaan persekutuan', '1.46'),\n",
       " ('1.46', ' dan banyak masalah hubungan', '3.96')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern_pair = r'<\\|(\\d+\\.\\d+)\\|>(.*?)<\\|(\\d+\\.\\d+)\\|>'\n",
    "matches = re.findall(pattern_pair, '<|0.00|> kerajaan persekutuan<|1.46|><|1.46|> dan banyak masalah hubungan<|3.96|><|3.96|> antara kerajaan negeri dan')\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d39c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def _pad_sequence(\n",
    "    sequence,\n",
    "    n,\n",
    "    pad_left=False,\n",
    "    pad_right=False,\n",
    "    left_pad_symbol=None,\n",
    "    right_pad_symbol=None,\n",
    "):\n",
    "    sequence = iter(sequence)\n",
    "    if pad_left:\n",
    "        sequence = itertools.chain((left_pad_symbol,) * (n - 1), sequence)\n",
    "    if pad_right:\n",
    "        sequence = itertools.chain(sequence, (right_pad_symbol,) * (n - 1))\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def ngrams(\n",
    "    sequence,\n",
    "    n: int,\n",
    "    pad_left=False,\n",
    "    pad_right=False,\n",
    "    left_pad_symbol=None,\n",
    "    right_pad_symbol=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    generate ngrams.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sequence : List[str]\n",
    "        list of tokenize words.\n",
    "    n : int\n",
    "        ngram size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result: List[Tuple[str, str]]\n",
    "    \"\"\"\n",
    "    sequence = _pad_sequence(\n",
    "        sequence, n, pad_left, pad_right, left_pad_symbol, right_pad_symbol\n",
    "    )\n",
    "\n",
    "    history = []\n",
    "    while n > 1:\n",
    "        try:\n",
    "            next_item = next(sequence)\n",
    "        except StopIteration:\n",
    "            return\n",
    "        history.append(next_item)\n",
    "        n -= 1\n",
    "    for item in sequence:\n",
    "        history.append(item)\n",
    "        yield tuple(history)\n",
    "        del history[0]\n",
    "\n",
    "def remove_duplicate(string, n = 3):\n",
    "    n = list(ngrams(string.split(), 3))\n",
    "    already = set()\n",
    "    dedup = []\n",
    "    for n_ in n:\n",
    "        n_ = ' '.join(n_)\n",
    "        if n_ not in already:\n",
    "            dedup.append(n_)\n",
    "            already.add(n_)\n",
    "    return ' '.join(dedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c2839f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def round_to_nearest_0_02(number):\n",
    "    return round(number * 50) / 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43e960ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = [\n",
    "    'terima kasih kerana menonton',\n",
    "    'terima kasih',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9551e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mp\n",
    "import copy\n",
    "\n",
    "minimum_score = 5\n",
    "\n",
    "def loop(files):\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained('openai/whisper-large-v3')\n",
    "    \n",
    "    files, _ = files\n",
    "    results = []\n",
    "    for f in tqdm(files):\n",
    "        try:\n",
    "            with open(f) as fopen:\n",
    "                data = json.load(fopen)\n",
    "        except:\n",
    "            continue\n",
    "        f_split = os.path.split(f)[-1].replace('.json', '')\n",
    "        for i in range(len(data)):\n",
    "            \n",
    "            audio_filename = os.path.join('output-audio-malaya', f'{f_split}-{i}.mp3')\n",
    "            if not os.path.exists(audio_filename):\n",
    "                continue\n",
    "                \n",
    "            y = audio.decode_example(audio.encode_example(audio_filename))['array']\n",
    "            len_y = len(y) / sr\n",
    "            if len_y > 30:\n",
    "                continue\n",
    "            rounded_num = f'<|{round_to_nearest_0_02(len_y):.2f}|>'\n",
    "            \n",
    "            if data[i]['score_ms'] > minimum_score:\n",
    "                a = np.array(data[i]['predict_ms'])\n",
    "                a = a[a != 50257].tolist() + [50257]\n",
    "                t = tokenizer.decode(a, skip_special_tokens = True, decode_with_timestamps = True).strip()\n",
    "                if t.split('|>')[-1] != '':\n",
    "                    t += rounded_num\n",
    "                \n",
    "                matches = re.findall(pattern_pair, t)\n",
    "                rs = []\n",
    "                for match in matches:\n",
    "                    l = float(match[0])\n",
    "                    r = float(match[2])\n",
    "                    t_ = match[1]\n",
    "                    rt_ = re.sub('[^a-z ]+', '', t_.lower()).strip()\n",
    "                    if (r - l > 3) and any([s == rt_ for s in selected]):\n",
    "                        # print(audio_filename, t_)\n",
    "                        t_ = ''\n",
    "                    else:\n",
    "                        try:\n",
    "                            dense = CountVectorizer(ngram_range = (3,3)).fit_transform([t_]).todense()\n",
    "                            repeat = (dense > 3).sum() > 1\n",
    "                            if repeat:\n",
    "                                t_ = remove_duplicate(t_)\n",
    "                        except:\n",
    "                            if len(t_) > 100:\n",
    "                                t_ = remove_duplicate(t_)\n",
    "                    rs.append(f'<|{match[0]}|>{t_}<|{match[2]}|>')\n",
    "                rs = ''.join(rs)\n",
    "                t = f'<|startoftranscript|><|ms|><|transcribe|>{rs}<|endoftext|>'\n",
    "                d = {\n",
    "                    'new_text': t,\n",
    "                    'audio_filename': audio_filename,\n",
    "                }\n",
    "                results.append(d)\n",
    "                    \n",
    "            \n",
    "            if data[i]['score_en'] > minimum_score:\n",
    "                a = np.array(data[i]['predict_en'])\n",
    "                a = a[a != 50257].tolist() + [50257]\n",
    "                t = tokenizer.decode(a, skip_special_tokens = True, decode_with_timestamps = True).strip()\n",
    "                if t.split('|>')[-1] != '':\n",
    "                    t += rounded_num\n",
    "                \n",
    "                matches = re.findall(pattern_pair, t)\n",
    "                rs = []\n",
    "                for match in matches:\n",
    "                    l = float(match[0])\n",
    "                    r = float(match[2])\n",
    "                    t_ = match[1]\n",
    "                    rt_ = re.sub('[^a-z ]+', '', t_.lower()).strip()\n",
    "                    if (r - l > 3) and any([s == rt_ for s in selected]):\n",
    "                        # print(audio_filename, t_)\n",
    "                        t_ = ''\n",
    "                    else:\n",
    "                        try:\n",
    "                            dense = CountVectorizer(ngram_range = (3,3)).fit_transform([t_]).todense()\n",
    "                            repeat = (dense > 3).sum() > 1\n",
    "                            if repeat:\n",
    "                                t_ = remove_duplicate(t_)\n",
    "                        except:\n",
    "                            if len(t_) > 100:\n",
    "                                t_ = remove_duplicate(t_)\n",
    "                    rs.append(f'<|{match[0]}|>{t_}<|{match[2]}|>')\n",
    "                rs = ''.join(rs)\n",
    "                t = f'<|startoftranscript|><|en|><|transcribe|>{rs}<|endoftext|>'\n",
    "                d = {\n",
    "                    'new_text': t,\n",
    "                    'audio_filename': audio_filename,\n",
    "                }\n",
    "                results.append(d)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7abad3a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# results = loop((files[:10], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f8c38d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s].41s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s].15s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 1/380 [00:02<17:36,  2.79s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s].12s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 1/380 [00:04<30:19,  4.80s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  1%|          | 2/380 [00:05<15:22,  2.44s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 1/380 [00:04<26:28,  4.19s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  1%|          | 2/380 [00:05<15:09,  2.41s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 1/380 [00:02<14:03,  2.23s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s].20s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 1/380 [00:04<30:13,  4.79s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 1/380 [00:05<35:26,  5.61s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  1%|          | 2/380 [00:05<17:54,  2.84s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s].51s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      " 30%|███       | 114/380 [27:10<55:20, 12.48s/it]t]"
     ]
    }
   ],
   "source": [
    "results = mp.multiprocessing(files, loop, cores = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f33581b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1089630"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0bb47ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'new_text': \"<|startoftranscript|><|en|><|transcribe|><|0.00|> Allah Ta'ala<|1.44|><|1.44|> Jadikan kita<|2.34|><|2.34|> Kita bahasa Melayu<|5.20|><|5.20|> Sama ada bahasa Cina<|6.10|><|endoftext|>\",\n",
       " 'audio_filename': 'output-audio-malaya/0-9735-38.mp3'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbbd91b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1089630/1089630 [00:03<00:00, 280322.10it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('prepared-pseudolabel-malaya.jsonl', 'w') as fopen:\n",
    "    for r in tqdm(results):\n",
    "        fopen.write(f'{json.dumps(r)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa611004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "914c3f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0874633c98d4d4f885e9c8aa29682fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "prepared-pseudolabel-malaya.jsonl:   0%|          | 0.00/309M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/datasets/mesolitica/pseudolabel-malaya-speech-stt-train-whisper-large-v3-timestamp/blob/main/prepared-pseudolabel-malaya.jsonl'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.upload_file(\n",
    "    path_or_fileobj='prepared-pseudolabel-malaya.jsonl',\n",
    "    path_in_repo='prepared-pseudolabel-malaya.jsonl',\n",
    "    repo_id='mesolitica/pseudolabel-malaya-speech-stt-train-whisper-large-v3-timestamp',\n",
    "    repo_type='dataset',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbc83727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('openai/whisper-medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1c4bab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftranscript|>',\n",
       " '<|en|>',\n",
       " '<|transcribe|>',\n",
       " '<|0.00|>',\n",
       " 'ĠAllah',\n",
       " 'ĠTa',\n",
       " \"'\",\n",
       " 'ala',\n",
       " '<|1.44|>',\n",
       " '<|1.44|>',\n",
       " 'ĠJ',\n",
       " 'ad',\n",
       " 'ikan',\n",
       " 'Ġkita',\n",
       " '<|2.34|>',\n",
       " '<|2.34|>',\n",
       " 'ĠKita',\n",
       " 'Ġbah',\n",
       " 'asa',\n",
       " 'ĠMel',\n",
       " 'ay',\n",
       " 'u',\n",
       " '<|5.20|>',\n",
       " '<|5.20|>',\n",
       " 'ĠS',\n",
       " 'ama',\n",
       " 'Ġada',\n",
       " 'Ġbah',\n",
       " 'asa',\n",
       " 'ĠC',\n",
       " 'ina',\n",
       " '<|6.10|>',\n",
       " '<|endoftext|>']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(results[-4]['new_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b40c48d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
