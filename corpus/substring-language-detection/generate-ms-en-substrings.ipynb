{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227bbcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0a65e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/datasets/mesolitica/ms-en/resolve/main/ms-en-left.train\n",
    "# !wget https://huggingface.co/datasets/mesolitica/ms-en/resolve/main/ms-en-right.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5fa73ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ms-en-left.train') as fopen:\n",
    "    left = fopen.read().split('\\n')\n",
    "    \n",
    "with open('ms-en-right.train') as fopen:\n",
    "    right = fopen.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87f9717a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.5.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "/home/husein/.local/lib/python3.8/site-packages/tensorflow_addons/utils/resource_loader.py:72: UserWarning: You are currently using TensorFlow 2.6.0 and trying to load a custom op (custom_ops/seq2seq/_beam_search_ops.so).\n",
      "TensorFlow Addons has compiled its custom ops against TensorFlow 2.4.0, and there are no compatibility guarantees between the two versions. \n",
      "This means that you might get segfaults when loading the custom op, or other kind of low-level errors.\n",
      " If you do, do not file an issue on Github. This is a known limitation.\n",
      "\n",
      "It might help you to fallback to pure Python ops with TF_ADDONS_PY_OPS . To do that, see https://github.com/tensorflow/addons#gpucpu-custom-ops \n",
      "\n",
      "You can also change the TensorFlow version installed on your system. You would need a TensorFlow version equal to or above 2.4.0 and strictly below 2.5.0.\n",
      " Note that nightly versions of TensorFlow, as well as non-pip TensorFlow like `conda install tensorflow` or compiled from source are not supported.\n",
      "\n",
      "The last solution is to find the TensorFlow Addons version that has custom ops compatible with the TensorFlow installed on your system. To do that, refer to the readme: https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "/home/husein/.local/lib/python3.8/site-packages/malaya_boilerplate/frozen_graph.py:35: UserWarning: Cannot import beam_search_ops from Tensorflow Addons, ['malaya.jawi_rumi.deep_model', 'malaya.phoneme.deep_model', 'malaya.rumi_jawi.deep_model', 'malaya.stem.deep_model'] will not available to use, make sure Tensorflow Addons version >= 0.12.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import malaya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07a2446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from malaya.text.rules import rules_normalizer, rules_compound_normalizer\n",
    "from malaya.text.normalization import _is_number_regex\n",
    "from malaya.text.function import replace_punct\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cd8d012",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATION = '!\"#$%&\\'()*+,./:;<=>?@[\\]^_`{|}~'\n",
    "\n",
    "def case_of(text):\n",
    "    return (\n",
    "        str.upper\n",
    "        if text.isupper()\n",
    "        else str.lower\n",
    "        if text.islower()\n",
    "        else str.title\n",
    "        if text.istitle()\n",
    "        else str\n",
    "    )\n",
    "\n",
    "def strip_punct(word):\n",
    "    left = []\n",
    "    right = []\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        if word[i] in PUNCTUATION:\n",
    "            left.append(word[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            break\n",
    "    i = len(word) - 1\n",
    "    while i > 0:\n",
    "        if word[i] in PUNCTUATION:\n",
    "            right.append(word[i])\n",
    "            i -= 1\n",
    "        else:\n",
    "            break\n",
    "    left = ''.join(left)\n",
    "    right = ''.join(right[::-1])\n",
    "    if len(right):\n",
    "        word_ = word[:-len(right)]\n",
    "    else:\n",
    "        word_ = word\n",
    "    word_ = word_[len(left):]\n",
    "    return left, right, word_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a691ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words_punct(left_word, right_word):\n",
    "    left_left, left_right, left_word = strip_punct(left_word)\n",
    "    right_left, right_right, right_word = strip_punct(right_word)\n",
    "    return f'{left_left}{right_word}{left_right}'\n",
    "\n",
    "def random_replace_alignment(left, right, alignment, min_replace = 2, max_replace = 10):\n",
    "    splitted_left = left.split()\n",
    "    splitted_right = right.split()\n",
    "    \n",
    "    selected_alignment = []\n",
    "    for s in alignment:\n",
    "        l = s[0]\n",
    "        r = s[1]\n",
    "        try:\n",
    "            if _is_number_regex(splitted_left[l].replace(',', '').replace('.', '')) or _is_number_regex(splitted_right[r].replace(',', '').replace('.', '')):\n",
    "                continue\n",
    "            elif splitted_left[l].isupper() or splitted_right[r].isupper():\n",
    "                continue\n",
    "            elif splitted_left[l].lower() == splitted_right[r].lower():\n",
    "                continue\n",
    "            elif splitted_right[r].lower() in ['the', 'a', 'an', 'it', 'is', 'are']:\n",
    "                continue\n",
    "            elif l == r:\n",
    "                continue\n",
    "            else:\n",
    "                selected_alignment.append((l, r))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    try:\n",
    "        count_replace = random.randint(min_replace, min(max_replace, len(selected_alignment)))\n",
    "        selected = random.sample(selected_alignment, count_replace)\n",
    "        for s in selected:\n",
    "            splitted_left[s[0]] = replace_words_punct(splitted_left[s[0]], splitted_right[s[1]])\n",
    "\n",
    "        return ' '.join(splitted_left), selected\n",
    "    \n",
    "    except:\n",
    "        return ' '.join(splitted_left), []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9045bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "eflomal = malaya.alignment.ms_en.eflomal(preprocessing_func=replace_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acab5eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "fast_text = malaya.language_detection.fasttext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44acf599",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = malaya.language_detection.substring_rules(model = fast_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c486c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sastrawi = malaya.stem.sastrawi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03486b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EN']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(['lifestyle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "578ddb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = malaya.tokenizer.Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "552658ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ' '.join(tokenizer.tokenize(left[0]))\n",
    "r = ' '.join(tokenizer.tokenize(right[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "61601150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.8 ms, sys: 36.1 ms, total: 48.9 ms\n",
      "Wall time: 174 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "alignment = eflomal.align([l], [r])['forward'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d70bde5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Terminal 1 KKIA equipped with 64 kaunter daftar masuk , 12 aero bridge selain mampu menampung 3,200 penumpang dalam satu masa .',\n",
       " [(3, 4), (4, 5)])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_ = random_replace_alignment(l, r, alignment)\n",
    "r_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce0ee85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MS',\n",
       " 'NOT_LANG',\n",
       " 'CAPITAL',\n",
       " 'EN',\n",
       " 'EN',\n",
       " 'NOT_LANG',\n",
       " 'MS',\n",
       " 'MS',\n",
       " 'MS',\n",
       " 'NOT_LANG',\n",
       " 'NOT_LANG',\n",
       " 'OTHERS',\n",
       " 'EN',\n",
       " 'MS',\n",
       " 'MS',\n",
       " 'MS',\n",
       " 'NOT_LANG',\n",
       " 'MS',\n",
       " 'MS',\n",
       " 'MS',\n",
       " 'MS',\n",
       " 'NOT_LANG']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted = l.split()\n",
    "predicted = model.predict(splitted)\n",
    "for no, w in enumerate(splitted):\n",
    "    if predicted[no] != 'MS':\n",
    "        w_stem = sastrawi.stem(w)\n",
    "        if malaya.text.function.is_malay(w_stem) or fast_text.predict([w_stem])[0] in ['malay', 'ind']:\n",
    "            predicted[no] = 'MS'\n",
    "            \n",
    "for i in r_[1]:\n",
    "    predicted[i[0]] = 'EN'\n",
    "    \n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eebd9603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Terminal', 'MS'),\n",
       " ('1', 'NOT_LANG'),\n",
       " ('KKIA', 'CAPITAL'),\n",
       " ('equipped', 'EN'),\n",
       " ('with', 'EN'),\n",
       " ('64', 'NOT_LANG'),\n",
       " ('kaunter', 'MS'),\n",
       " ('daftar', 'MS'),\n",
       " ('masuk', 'MS'),\n",
       " (',', 'NOT_LANG'),\n",
       " ('12', 'NOT_LANG'),\n",
       " ('aero', 'OTHERS'),\n",
       " ('bridge', 'EN'),\n",
       " ('selain', 'MS'),\n",
       " ('mampu', 'MS'),\n",
       " ('menampung', 'MS'),\n",
       " ('3,200', 'NOT_LANG'),\n",
       " ('penumpang', 'MS'),\n",
       " ('dalam', 'MS'),\n",
       " ('satu', 'MS'),\n",
       " ('masa', 'MS'),\n",
       " ('.', 'NOT_LANG')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(r_[0].split(), predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e964a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def loop(rows):\n",
    "    rows, _ = rows\n",
    "    strings, labels = [], []\n",
    "    for i in tqdm(range(len(rows))):\n",
    "        try:\n",
    "            left_, right_ = rows[i][0], rows[i][1]\n",
    "            tokenized_l = tokenizer.tokenize(left_)\n",
    "            tokenized_r = tokenizer.tokenize(right_)\n",
    "            l = ' '.join(tokenized_l)\n",
    "            r = ' '.join(tokenized_r)\n",
    "            \n",
    "            if len(tokenized_l) > 60 or len(tokenized_r) > 60:\n",
    "                continue\n",
    "                \n",
    "            alignment = eflomal.align([l], [r])['forward'][0]\n",
    "            r_ = random_replace_alignment(l, r, alignment)\n",
    "            \n",
    "            splitted = l.split()\n",
    "            predicted = model.predict(splitted)\n",
    "            for no, w in enumerate(splitted):\n",
    "                if predicted[no] != 'MS':\n",
    "                    w_stem = sastrawi.stem(w)\n",
    "                    if malaya.text.function.is_malay(w_stem) or fast_text.predict([w_stem])[0] in ['malay', 'ind']:\n",
    "                        predicted[no] = 'MS'\n",
    "\n",
    "            for i in r_[1]:\n",
    "                predicted[i[0]] = 'EN'\n",
    "    \n",
    "            strings.append(r_[0])\n",
    "            labels.append(predicted)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    return [[strings, labels]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "05d2a47e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  5.35it/s]\n"
     ]
    }
   ],
   "source": [
    "r = loop((list(zip(left[:10], right[:10])),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b286adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21431ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▊                                                                                     | 2170/50000 [10:46<3:58:23,  3.34it/s]"
     ]
    }
   ],
   "source": [
    "r = mp.multiprocessing(list(zip(left[:200000], right[:200000])), loop, cores = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b12585",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings, labels = [], []\n",
    "for i in range(len(r)):\n",
    "    print(i, len(r[i][0]))\n",
    "    strings.extend(r[i][0])\n",
    "    labels.extend(r[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0718461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings[-1], labels[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db974a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ms-en-substrings.json', 'w') as fopen:\n",
    "    json.dump({'strings': strings, 'labels': labels}, fopen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
