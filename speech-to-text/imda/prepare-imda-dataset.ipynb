{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b64da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from transformers import AutoTokenizer, WhisperConfig\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "def parse_lang(s):\n",
    "    ori_s = s\n",
    "    try:\n",
    "        s = s.split('>')[1].split('<')[0].split(':')[0]\n",
    "    except:\n",
    "        pass\n",
    "    if not len(s):\n",
    "        s = ori_s\n",
    "    return s\n",
    "\n",
    "def cleaning(s):\n",
    "    return re.sub(r'[ ]+', ' ', s).strip()\n",
    "\n",
    "replaces = [\n",
    "    '</P1>',\n",
    "    '</P2>',\n",
    "    '</mandarin>',\n",
    "    '</tamil>',\n",
    "    '<>',\n",
    "    '<B>',\n",
    "    '<EX2>',\n",
    "    '<EX2><tamil>சனியன்:saniyan</tamil></EX2>',\n",
    "    '<EX2><tamil>சப்பிட்டு:sapittu</tamil></EX2>',\n",
    "    '<EX2><tamil>சூத்து:sutthu</tamil></EX2>',\n",
    "    '<EX2><tamil>சூத்து:suuthu</tamil></EX2>',\n",
    "    '<EX2><tamil>சூத்தை:suuthai</tamil></EX2>',\n",
    "    '<FIL/>',\n",
    "    '<INK>',\n",
    "    '<NEN>',\n",
    "    '<NON/>',\n",
    "    '<NPS/>',\n",
    "    '<P1>',\n",
    "    '<P2>',\n",
    "    '<P2><P2>#Aisyah#</P2></P2>',\n",
    "    '<S>',\n",
    "    '<SPK/>',\n",
    "    '<UKN>',\n",
    "    '<UNK/>',\n",
    "    '<UNK>',\n",
    "    '<UNk>',\n",
    "    '<YNK>',\n",
    "    '<Z>',\n",
    "    '<Z></tamil>',\n",
    "    '<c/>',\n",
    "    '<kopi>',\n",
    "    '<kopis>',\n",
    "    '<mandarin>',\n",
    "    '<mandarin>:</mandarin>',\n",
    "    '<opp>',\n",
    "    '<ppl>',\n",
    "    '<s/>',\n",
    "    '<s>',\n",
    "    '<tami>',\n",
    "    '<tamil>',\n",
    "    '<tamil>:இதுனாலேதான்ithunaalaethaan</tamil>',\n",
    "    '<tamil>:வெச்சுட்டு:vechuttu</tamil>',\n",
    "    '<tamil></tamil>',\n",
    "    '<unk/>',\n",
    "    '<unk>',\n",
    "    '<way>',\n",
    "    '<we>'\n",
    "    '<malay>',\n",
    "    '</malay>',\n",
    "    '_',\n",
    "    '<UNK>',\n",
    "    '(ppl)',\n",
    "    '(ppo)',\n",
    "    '(mm)',\n",
    "    '(um)',\n",
    "    '(ppb)',\n",
    "    '(mmhmm)',\n",
    "    '<mandarin>',\n",
    "    '</mandarin>',\n",
    "    '(!ee!)',\n",
    "    '((m)',\n",
    "    '((mmhmm)m)',\n",
    "    '((mmhmm)mm)',\n",
    "    '()',\n",
    "    '(E)',\n",
    "    '(Er)',\n",
    "    '(Erm)',\n",
    "    '(Err)',\n",
    "    '(Hm)',\n",
    "    '(M)',\n",
    "    '(MM)',\n",
    "    '(Oh)',\n",
    "    '(PPO)',\n",
    "    '(UH)',\n",
    "    '(UM)',\n",
    "    '(UNK)',\n",
    "    '(Uh)',\n",
    "    '(a)',\n",
    "    '(ah)',\n",
    "    '(aha)',\n",
    "    '(ahh)',\n",
    "    '(aiya)',\n",
    "    '(aiyo)',\n",
    "    '(compassionate)',\n",
    "    '(d)',\n",
    "    '(duh)',\n",
    "    '(ee)',\n",
    "    '(eer)',\n",
    "    '(eh)',\n",
    "    '(ehe)',\n",
    "    '(emm)',\n",
    "    '(er)',\n",
    "    '(erm)',\n",
    "    '(erp)',\n",
    "    '(err)',\n",
    "    '(errr)',\n",
    "    '(euh)',\n",
    "    '(ew)',\n",
    "    '(h)',\n",
    "    '(ha)',\n",
    "    '(haa)',\n",
    "    '(hah)',\n",
    "    '(har)',\n",
    "    '(hm)',\n",
    "    '(hmm)',\n",
    "    '(hmm)(hmm)',\n",
    "    '(ho)',\n",
    "    '(hoo)',\n",
    "    '(hor)',\n",
    "    '(hu)',\n",
    "    '(huh)',\n",
    "    '(hur)',\n",
    "    '(ih)',\n",
    "    '(la)',\n",
    "    '(lah)',\n",
    "    '(leh)',\n",
    "    '(lor)',\n",
    "    '(m)',\n",
    "    '(mM)',\n",
    "    '(mah)',\n",
    "    '(mhm)',\n",
    "    '(mm)',\n",
    "    '(mmhmm)',\n",
    "    '(mmhmm)(mmhmm)',\n",
    "    '(mmhmmm)',\n",
    "    '(mmhmn)',\n",
    "    '(mmhnn)',\n",
    "    '(mmm)',\n",
    "    '(mmmhmmm)',\n",
    "    '(nah)',\n",
    "    '(no)',\n",
    "    '(oh)',\n",
    "    '(ohoh)',\n",
    "    '(oi)',\n",
    "    '(one)',\n",
    "    '(oo)',\n",
    "    '(oob)',\n",
    "    '(ooh)',\n",
    "    '(or)',\n",
    "    '(orh)',\n",
    "    '(ow)',\n",
    "    '(pare)',\n",
    "    '(pb)',\n",
    "    '(pbb)',\n",
    "    '(pbo)',\n",
    "    '(pbpb)',\n",
    "    '(pl)',\n",
    "    '(pop)',\n",
    "    '(popb)',\n",
    "    '(pp0)',\n",
    "    '(pp;)',\n",
    "    '(ppO)',\n",
    "    '(ppb)',\n",
    "    '(ppc)',\n",
    "    '(ppl)',\n",
    "    '(ppll)',\n",
    "    '(ppo)',\n",
    "    '(pppo)',\n",
    "    '(u)',\n",
    "    '(ugh)',\n",
    "    '(uh)',\n",
    "    '(uh-huh)',\n",
    "    '(uh-oh)',\n",
    "    '(uhh)',\n",
    "    '(uhhh)',\n",
    "    '(uhm)',\n",
    "    '(uhmmm)',\n",
    "    '(uhn)',\n",
    "    '(uhuh)',\n",
    "    '(um)',\n",
    "    '(umm)',\n",
    "    '(urm)',\n",
    "    '(us)',\n",
    "    '(uyh)',\n",
    "    '(wah)',\n",
    "    '(woah)',\n",
    "    '(woo)',\n",
    "    '(wow)',\n",
    "    '(ya)',\n",
    "    '(yeah)',\n",
    "    '(yup)',\n",
    "    '</P1>',\n",
    "    '</P2>',\n",
    "    '</mandarin>',\n",
    "    '</tamil>',\n",
    "    '<>',\n",
    "    '<B>',\n",
    "    '<EX2>',\n",
    "    '<EX2><tamil>சனியன்:saniyan</tamil></EX2>',\n",
    "    '<EX2><tamil>சப்பிட்டு:sapittu</tamil></EX2>',\n",
    "    '<EX2><tamil>சூத்து:sutthu</tamil></EX2>',\n",
    "    '<EX2><tamil>சூத்து:suuthu</tamil></EX2>',\n",
    "    '<EX2><tamil>சூத்தை:suuthai</tamil></EX2>',\n",
    "    '<FIL/>',\n",
    "    '<INK>',\n",
    "    '<NEN>',\n",
    "    '<NON/>',\n",
    "    '<NPS/>',\n",
    "    '<P1>',\n",
    "    '<P2>',\n",
    "    '<P2><P2>#Aisyah#</P2></P2>',\n",
    "    '<S>',\n",
    "    '<SPK/>',\n",
    "    '<UKN>',\n",
    "    '<UNK/>',\n",
    "    '<UNK>',\n",
    "    '<UNk>',\n",
    "    '<YNK>',\n",
    "    '<Z>',\n",
    "    '<Z></tamil>',\n",
    "    '<c/>',\n",
    "    '<kopi>',\n",
    "    '<kopis>',\n",
    "    '<mandarin>',\n",
    "    '<mandarin>:</mandarin>',\n",
    "    '<opp>',\n",
    "    '<ppl>',\n",
    "    '<s/>',\n",
    "    '<s>',\n",
    "    '<tami>',\n",
    "    '<tamil>',\n",
    "    '<tamil>:இதுனாலேதான்ithunaalaethaan</tamil>',\n",
    "    '<tamil>:வெச்சுட்டு:vechuttu</tamil>',\n",
    "    '<tamil></tamil>',\n",
    "    '<unk/>',\n",
    "    '<unk>',\n",
    "    '<way>',\n",
    "    '<we>',\n",
    "    '<c>',\n",
    "    '(Um)',\n",
    "    '(Z)',\n",
    "    \"(a'ah)\",\n",
    "    '(aa)',\n",
    "    '(am)',\n",
    "    '(em)',\n",
    "    '(euhr)',\n",
    "    '(hmmhmm)',\n",
    "    '(mhmm)',\n",
    "    '(mmhm)',\n",
    "    '(pcc)',\n",
    "    '(poo)',\n",
    "    '(rr)',\n",
    "    '(rre)',\n",
    "    '(ua)',\n",
    "    '(uerr)',\n",
    "    '(uhhuh)',\n",
    "    '(um\\\\h)',\n",
    "    '(umh)',\n",
    "    '(un)',\n",
    "    '(urr)',\n",
    "    '(uu)',\n",
    "    '(woh)',\n",
    "    '(yh)',\n",
    "    '(B>', \n",
    "    '(uh]', \n",
    "    '</mandarin)', \n",
    "    '<mmhmm)', \n",
    "    '<ppb)'\n",
    "]\n",
    "\n",
    "replaces = set(replaces)\n",
    "\n",
    "replaces_with = {\n",
    "    '[lah]': 'lah',\n",
    "    '[ah]': 'ah',\n",
    "    '[sia]': 'sia',\n",
    "    '[eh]': 'eh',\n",
    "    '(uh)': 'uh',\n",
    "    '[what]': 'what',\n",
    "    '[oh]': 'oh',\n",
    "    '(err)': 'err',\n",
    "    '[lor]': 'lor',\n",
    "    '[ha]': 'ha',\n",
    "    '[meh]': 'meh',\n",
    "    '[one]': 'one',\n",
    "    \"[a'ah]\": \"a'ah\",\n",
    "    '[hor]': 'hor',\n",
    "    '[leh]': 'leh',\n",
    "    '[mah]': 'mah',\n",
    "    '[nah]': 'nah',\n",
    "    '[tau]': 'tau',\n",
    "    '[uh]': 'uh',\n",
    "    '[wah]': 'wah',\n",
    "    '[;ah]': 'lah',\n",
    "     '[;lah]' : 'lah',\n",
    "     '[AH]': 'lah',\n",
    "     '[Ah]': 'lah',\n",
    "     '[Iah]': 'lah',\n",
    "     '[Oh]': 'oh',\n",
    "     '[Sia]': 'sia',\n",
    "     '[Wah]': 'wah',\n",
    "     '[[h]': '',\n",
    "     \"[a'ah]\": \"a'ah\",\n",
    "     \"[a'ha]\": \"a'ha\",\n",
    "     '[a-nah]': 'nah',\n",
    "     '[a]': '',\n",
    "     '[aah]': 'ah',\n",
    "     '[ag]': 'ah',\n",
    "     '[agh]': 'ah',\n",
    "     '[ah]': 'ah',\n",
    "     '[aha]': 'aha',\n",
    "     '[aiya]': 'aiya',\n",
    "     '[aiyo]': 'aiyo',\n",
    "     '[aj]': 'ah',\n",
    "     '[ajh]': 'ah',\n",
    "     '[alah]': 'alah',\n",
    "     '[anah]': 'alah',\n",
    "     '[anor]': '',\n",
    "     '[arh]': 'arh',\n",
    "     '[aw]': 'aw',\n",
    "     '[aww]': 'aw',\n",
    "     '[ay]': 'ay',\n",
    "     '[bah]': 'bah',\n",
    "     '[chey]': 'chey',\n",
    "     '[da]': 'dah',\n",
    "     '[dah]': 'dah',\n",
    "     '[deh]': 'deh',\n",
    "     '[dey]': 'dey',\n",
    "     '[do]': 'do',\n",
    "     '[duh]': 'duh',\n",
    "     '[e]': '',\n",
    "     '[ee]': 'ee',\n",
    "     '[eh]': 'eh',\n",
    "     '[ehe]': 'ehe',\n",
    "     '[ehh]': 'eh',\n",
    "     '[er]': 'er',\n",
    "     '[err]': 'er',\n",
    "     '[eww]': 'eww',\n",
    "     '[h]': '',\n",
    "     '[ha]': 'ha',\n",
    "     '[hah]': 'hah',\n",
    "     '[haha]': 'haha',\n",
    "     '[har]': 'har',\n",
    "     '[heh]': 'heh',\n",
    "     '[hey]': 'hey',\n",
    "     '[hm]': 'hmm',\n",
    "     '[hmm]': 'hmm',\n",
    "     '[ho]': 'hor',\n",
    "     '[hor]': 'hor',\n",
    "     '[horh]': 'hor',\n",
    "     '[hpr]': 'hor',\n",
    "     '[huh]': 'huh',\n",
    "     '[hur]': 'huh',\n",
    "     '[hurhur]': 'huh',\n",
    "     '[je]': 'jer',\n",
    "     '[jer]': 'jer',\n",
    "     '[kan]': 'kan',\n",
    "     '[l]': 'lah',\n",
    "     '[la]': 'lah',\n",
    "     '[lah]': 'lah',\n",
    "     '[lahh]': 'lah',\n",
    "     '[le]': 'leh',\n",
    "     '[leh]': 'leh',\n",
    "     '[liao]': 'liao',\n",
    "     '[like]': 'like',\n",
    "     '[loh]': 'loh',\n",
    "     '[lor]': 'lor',\n",
    "     '[lorh]': 'lor',\n",
    "     '[ma]': 'ma',\n",
    "     '[mah]': 'mah',\n",
    "     '[man]': 'man',\n",
    "     '[meh]': 'meh',\n",
    "     '[mhm]': 'mhm',\n",
    "     '[mm]': 'mhm',\n",
    "     '[mmhmm]': 'mhm',\n",
    "     '[neh]': 'neh',\n",
    "     '[ng]': 'ng',\n",
    "     '[nia]': 'nia',\n",
    "     '[oh]': 'oh',\n",
    "     '[oho]': 'oh',\n",
    "     '[oi]': 'oi',\n",
    "     '[ok]': 'okay',\n",
    "     '[okay]': 'okay',\n",
    "     \"[one's]\": 'one',\n",
    "     '[one]': 'one',\n",
    "     '[ones]': 'one',\n",
    "     '[oo]': 'oo',\n",
    "     '[ooh]': 'ooh',\n",
    "     '[or\\\\h]': 'orh',\n",
    "     '[orh]': 'orh',\n",
    "     '[orh][orh][orh]': 'orh',\n",
    "     '[oth]': 'orh',\n",
    "     '[pe]': '',\n",
    "     '[poh]': 'poh',\n",
    "     '[ppl]': '',\n",
    "     '[prh]': '',\n",
    "     '[pun]': 'pun',\n",
    "     '[right]': 'right',\n",
    "     '[roh]': 'roh',\n",
    "     '[roti]': 'roti',\n",
    "     '[run]': 'run',\n",
    "     '[seh]': 'seh',\n",
    "     '[shiok]': 'shiok',\n",
    "     '[sia]': 'sia',\n",
    "     '[siah]': 'sia',\n",
    "     '[sial]': 'sial',\n",
    "     '[sialah]': 'sialah',\n",
    "     '[sian]': 'sian',\n",
    "     '[sio]': 'siol',\n",
    "     '[siol]': 'siol',\n",
    "     '[tau]': 'tau',\n",
    "     '[that]': 'that',\n",
    "     '[ugh]': 'ugh',\n",
    "     '[uh)]': 'ugh',\n",
    "     '[uh-oh]': 'uh-oh',\n",
    "     '[uhn]': 'uh',\n",
    "     '[um]': 'um',\n",
    "     '[what]': 'what',\n",
    "     '[woah]': 'woah',\n",
    "     '[wor]': 'wor',\n",
    "     '[wow]': 'wow',\n",
    "     '[ya]': 'ya',\n",
    "     '[出国]': '出国',\n",
    "     '[ah[]': 'ah',\n",
    "     '[ai]': 'ai',\n",
    "     '[alh]': 'lah',\n",
    "     '[kah]': '',\n",
    "     '[laj]': 'lah',\n",
    "     '[og]': '',\n",
    "     '[or]': '',\n",
    "     '[ow]': '',\n",
    "     '[uhm]': 'uhm',\n",
    "     '[uhum]': 'uhm',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf9aa819",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = WhisperConfig.from_pretrained('openai/whisper-large-v3')\n",
    "maxlen = config.max_length - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a005df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['imda-part1.jsonl',\n",
       " 'imda-part2.jsonl',\n",
       " 'imda-part6-1.jsonl',\n",
       " 'imda-part6-2.jsonl',\n",
       " 'imda-part6-3.jsonl',\n",
       " 'imda-same-part3.jsonl',\n",
       " 'imda-same-part4.jsonl',\n",
       " 'imda-same-part5.jsonl',\n",
       " 'imda-separate-part3.jsonl',\n",
       " 'imda-separate-part4.jsonl',\n",
       " 'imda-separate-part5.jsonl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_small = AutoTokenizer.from_pretrained('openai/whisper-small')\n",
    "tokenizer = AutoTokenizer.from_pretrained('openai/whisper-large-v3')\n",
    "\n",
    "files = sorted(glob('imda*.jsonl'))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0dc5b30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "755724it [00:41, 17996.46it/s]\n",
      "826469it [00:39, 20853.30it/s]\n",
      "117890it [00:09, 12237.31it/s]\n",
      "108538it [00:08, 12352.59it/s]\n",
      "107820it [00:08, 12181.03it/s]\n",
      "185448it [00:13, 13344.17it/s]\n",
      "172166it [00:13, 13179.57it/s]\n",
      "51982it [00:05, 10344.15it/s]\n",
      "169876it [00:12, 13783.33it/s]\n",
      "163431it [00:12, 13320.73it/s]\n",
      "215412it [00:18, 11351.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words = []\n",
    "for f in files:\n",
    "    with open(f) as fopen:\n",
    "        for l in tqdm(fopen):\n",
    "            l = json.loads(l)\n",
    "            t = l['text'].strip()\n",
    "            \n",
    "            splitted = t.split()\n",
    "            filtered = []\n",
    "            for s in splitted:\n",
    "                filtered.append(parse_lang(s))\n",
    "            t = ' '.join(filtered)\n",
    "            \n",
    "            for r in replaces:\n",
    "                t = t.replace(r, '')\n",
    "            for r, v in replaces_with.items():\n",
    "                t = t.replace(r, v)\n",
    "            t = cleaning(t)\n",
    "            \n",
    "            splitted = t.split()\n",
    "            for s in splitted:\n",
    "                after = parse_lang(s)\n",
    "                if after[0] in {'(', '[', '<'} and after[-1] in {')', ']', '>'}:\n",
    "                    filtered_words.append(after)\n",
    "set(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d1b2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(files):\n",
    "    files, _ = files\n",
    "    results = []\n",
    "    for f in files:\n",
    "        with open(f) as fopen:\n",
    "            for l in tqdm(fopen):\n",
    "                l = json.loads(l)\n",
    "                audio_filename = l.get('new_filename')\n",
    "                if audio_filename is None:\n",
    "                    audio_filename = l['filename']\n",
    "                if not os.path.exists(audio_filename):\n",
    "                    continue\n",
    "\n",
    "                t = l['text'].strip()\n",
    "                if len(t) < 2:\n",
    "                    continue\n",
    "\n",
    "                splitted = t.split()\n",
    "                filtered = []\n",
    "                for s in splitted:\n",
    "                    filtered.append(parse_lang(s))\n",
    "                t = ' '.join(filtered)\n",
    "\n",
    "                for r in replaces:\n",
    "                    t = t.replace(r, '')\n",
    "                for r, v in replaces_with.items():\n",
    "                    t = t.replace(r, v)\n",
    "                t = cleaning(t)\n",
    "                \n",
    "                if len(t) < 2:\n",
    "                    continue\n",
    "                \n",
    "                if t[0] in {',', '.'}:\n",
    "                    t = t[1:]\n",
    "\n",
    "                if len(t) < 2:\n",
    "                    continue\n",
    "\n",
    "                new_text = f'<|startoftranscript|><|en|><|transcribe|> {t}<|endoftext|>'\n",
    "                input_ids = tokenizer(new_text, add_special_tokens = False).input_ids\n",
    "                if len(input_ids) > maxlen:\n",
    "                    continue\n",
    "\n",
    "                l['new_text'] = new_text\n",
    "                l['audio_filename'] = audio_filename\n",
    "                results.append(l)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb848c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "084a24e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "51982it [00:12, 4009.13it/s]]\n",
      "107820it [00:20, 5213.36it/s]\n",
      "108538it [00:21, 5168.29it/s]\n",
      "117890it [00:23, 5043.85it/s]\n",
      "169876it [00:30, 5507.83it/s]\n",
      "163431it [00:33, 4862.09it/s]\n",
      "185448it [00:33, 5476.66it/s]\n",
      "172166it [00:35, 4792.99it/s]\n",
      "215412it [00:46, 4601.68it/s]\n",
      "826469it [01:41, 8133.04it/s]\n",
      "755724it [01:46, 7116.38it/s]\n"
     ]
    }
   ],
   "source": [
    "results = mp.multiprocessing(files, loop, cores = len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "44251ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2840417"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0890f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prepared-imda.jsonl', 'w') as fopen:\n",
    "    for r in results:\n",
    "        fopen.write(f'{json.dumps(r)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f7239e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from streaming import MDSWriter, LocalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80065dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {\n",
    "    'new_text': 'str',\n",
    "    'audio_filename': 'str'\n",
    "}\n",
    "\n",
    "hashes = 'sha1', 'xxh64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3d86f3d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': 'part5-separate-audio-mp3/app_3773_5546_phnd_pos-59.mp3',\n",
       " 'text': 'ya [lor] ya [lor] not bad not bad, can can you one week (uh) you one week go how many times maybe can find you, two three times [ah] [oh] okay okay okay',\n",
       " 'new_text': '<|startoftranscript|><|en|><|transcribe|> ya lor ya lor not bad not bad, can can you one week you one week go how many times maybe can find you, two three times ah oh okay okay okay<|endoftext|>',\n",
       " 'audio_filename': 'part5-separate-audio-mp3/app_3773_5546_phnd_pos-59.mp3'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d0c03e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf mosaic-imda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1580bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2840417it [00:22, 128350.45it/s]\n"
     ]
    }
   ],
   "source": [
    "with MDSWriter(out='mosaic-imda', columns=columns, compression=None, hashes=hashes) as out:\n",
    "    with open('prepared-imda.jsonl') as fopen:\n",
    "        for l in tqdm(fopen):\n",
    "            l = json.loads(l)\n",
    "            sample = {\n",
    "                'new_text': l['new_text'],\n",
    "                'audio_filename': l['audio_filename'],\n",
    "            }\n",
    "            out.write(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a4981bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LocalDataset('mosaic-imda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "104d421b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio_filename': 'part1-mp3/000010001.mp3',\n",
       " 'new_text': '<|startoftranscript|><|en|><|transcribe|> There were barrels of wine in the huge cellar.<|endoftext|>'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0169a932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mosaic-imda-stt'...\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 3, done.\u001b[K\r\n",
      "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 3\u001b[K\r\n",
      "Unpacking objects: 100% (3/3), 517 bytes | 517.00 KiB/s, done.\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/datasets/malaysia-ai/mosaic-imda-stt\n",
    "!cp mosaic-imda/* mosaic-imda-stt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9af0d0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537M\tmosaic-imda-stt\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!du -hs mosaic-imda-stt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
