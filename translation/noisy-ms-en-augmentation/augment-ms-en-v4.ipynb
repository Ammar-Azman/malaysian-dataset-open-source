{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b8fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/train-ms-en.tar.gz\n",
    "# !tar -zxf train-ms-en.tar.gz\n",
    "# !rm train-ms-en.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4229a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pyenchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b79e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c6f423b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.check(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "115c2ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/huseinzol05/malay-dataset/master/normalization/en-lexicon/en-social-media-lexicon.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8290d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the .py files at https://github.com/huseinzol05/malay-dataset/tree/master/dictionary/dialect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69ff672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kedah\n",
    "import kelantan_v2\n",
    "import kelantan\n",
    "import melaka\n",
    "import negeri_sembilan\n",
    "import pahang\n",
    "import perak\n",
    "import sabah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51eb22f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe86564f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 890M\r\n",
      "drwxr-xr-x  2 ubuntu ubuntu 4.0K Jun  26  2020 .\r\n",
      "drwxr-xr-x 41 ubuntu ubuntu 4.0K Jul  10 00:46 ..\r\n",
      "-rw-r--r--  1 ubuntu ubuntu 471M Jun  27  2020 left.txt\r\n",
      "-rw-r--r--  1 ubuntu ubuntu 420M Jun  27  2020 right.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lha train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0741e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train/left.txt') as fopen:\n",
    "    left = fopen.read().split('\\n')\n",
    "    \n",
    "with open('train/right.txt') as fopen:\n",
    "    right = fopen.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "042d9bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('He said it was ridiculous for Apandi to state that investigations conducted by law enforcement agencies here had revealed that there was no evidence to show 1MDB money had been misappropriated.',\n",
       " 'He said it was ridiculous for Apandi to state that investigations conducted by law enforcement agencies here had revealed that there was no evidence to show 1MDB money had been misappropriated.')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left[7106], right[7106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "762dce3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3712555, 3712555)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(left), len(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9272db3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.5.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow_addons/utils/resource_loader.py:72: UserWarning: You are currently using TensorFlow 2.6.0 and trying to load a custom op (custom_ops/seq2seq/_beam_search_ops.so).\n",
      "TensorFlow Addons has compiled its custom ops against TensorFlow 2.4.0, and there are no compatibility guarantees between the two versions. \n",
      "This means that you might get segfaults when loading the custom op, or other kind of low-level errors.\n",
      " If you do, do not file an issue on Github. This is a known limitation.\n",
      "\n",
      "It might help you to fallback to pure Python ops with TF_ADDONS_PY_OPS . To do that, see https://github.com/tensorflow/addons#gpucpu-custom-ops \n",
      "\n",
      "You can also change the TensorFlow version installed on your system. You would need a TensorFlow version equal to or above 2.4.0 and strictly below 2.5.0.\n",
      " Note that nightly versions of TensorFlow, as well as non-pip TensorFlow like `conda install tensorflow` or compiled from source are not supported.\n",
      "\n",
      "The last solution is to find the TensorFlow Addons version that has custom ops compatible with the TensorFlow installed on your system. To do that, refer to the readme: https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/malaya_boilerplate/frozen_graph.py:35: UserWarning: Cannot import beam_search_ops from Tensorflow Addons, ['malaya.jawi_rumi.deep_model', 'malaya.phoneme.deep_model', 'malaya.rumi_jawi.deep_model', 'malaya.stem.deep_model'] will not available to use, make sure Tensorflow Addons version >= 0.12.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import malaya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1aa318d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from malaya.text.rules import rules_normalizer, rules_compound_normalizer\n",
    "from malaya.text.normalization import _is_number_regex\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ff9548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en-social-media-lexicon.json') as fopen:\n",
    "    en_lexicon = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c907e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_rules_compound_normalizer = defaultdict(list)\n",
    "for k, v in rules_compound_normalizer.items():\n",
    "    rev_rules_compound_normalizer[v].append(k)\n",
    "    \n",
    "rev_rules_normalizer = defaultdict(list)\n",
    "for k, v in rules_normalizer.items():\n",
    "    rev_rules_normalizer[v].append(k)\n",
    "    \n",
    "rules_compound_normalizer_regex = (\n",
    "    '(?:' + '|'.join(list(rev_rules_compound_normalizer.keys())) + ')'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85e7dd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'sudah tentu': ['of course', 'of cos'],\n",
       "             'selagi': ['as long', 'as long as'],\n",
       "             'sekurang - kurangnya': ['at least'],\n",
       "             'tidak mahu': ['tak nak'],\n",
       "             'tentang itu': ['tang tu'],\n",
       "             'tidak ada': ['tak ada'],\n",
       "             'yang ada': ['hok ado'],\n",
       "             'tidak mengapa': ['tak pe'],\n",
       "             'pertama kalinya': ['at first'],\n",
       "             'sahaja lah': ['je lah', 'ja lah'],\n",
       "             'serius walaupun': ['seriously though', 'seriously thou'],\n",
       "             'seperti mana': ['lagu mana'],\n",
       "             'apa benda la': ['pebenda la'],\n",
       "             'asam pedas': ['as ped'],\n",
       "             'mari sini': ['meh sini'],\n",
       "             'apa cerita': ['apa cer'],\n",
       "             'air longkang': ['air chor'],\n",
       "             'mesin layan diri': ['mesin gedegang'],\n",
       "             'mahu pergi': ['nk gi'],\n",
       "             'sahaja': ['jer dok'],\n",
       "             'ke': ['ke eh'],\n",
       "             'bagitahu': ['pape roger'],\n",
       "             'apa semua': ['pe sume'],\n",
       "             'mari siapa': ['meh sapa'],\n",
       "             'bagaimana': ['dah guane']})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_rules_compound_normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0de880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kedah_compound_normalizer = defaultdict(list)\n",
    "kedah_normalizer = defaultdict(list)\n",
    "for k, v in kedah.dictionary.items():\n",
    "    for v_ in v:\n",
    "        if len(v_.split()) > 1:\n",
    "            kedah_compound_normalizer[v_].append(k)\n",
    "        else:\n",
    "            kedah_normalizer[v_].append(k)\n",
    "            \n",
    "kedah_compound_normalizer_regex = (\n",
    "    '(?:' + '|'.join(list(kedah_compound_normalizer.keys())) + ')'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce988382",
   "metadata": {},
   "outputs": [],
   "source": [
    "kelantan_compound_normalizer = defaultdict(list)\n",
    "kelantan_normalizer = defaultdict(list)\n",
    "for k, v in kelantan.dictionary.items():\n",
    "    for v_ in v:\n",
    "        if len(v_.split()) > 1:\n",
    "            kelantan_compound_normalizer[v_].append(k)\n",
    "        else:\n",
    "            kelantan_normalizer[v_].append(k)\n",
    "            \n",
    "for k, v in kelantan_v2.dictionary.items():\n",
    "    for v_ in v:\n",
    "        if len(v_.split()) > 1:\n",
    "            kelantan_compound_normalizer[v_].append(k)\n",
    "        else:\n",
    "            kelantan_normalizer[v_].append(k)\n",
    "            \n",
    "kelantan_compound_normalizer_regex = (\n",
    "    '(?:' + '|'.join(list(kelantan_compound_normalizer.keys())) + ')'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba1156a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "melaka_compound_normalizer = defaultdict(list)\n",
    "melaka_normalizer = defaultdict(list)\n",
    "for k, v in melaka.dictionary.items():\n",
    "    for v_ in v:\n",
    "        if len(v_.split()) > 1:\n",
    "            melaka_compound_normalizer[v_].append(k)\n",
    "        else:\n",
    "            melaka_normalizer[v_].append(k)\n",
    "            \n",
    "melaka_compound_normalizer_regex = (\n",
    "    '(?:' + '|'.join(list(melaka_compound_normalizer.keys())) + ')'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fa63d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "negeri_sembilan_compound_normalizer = defaultdict(list)\n",
    "negeri_sembilan_normalizer = defaultdict(list)\n",
    "for k, v in negeri_sembilan.dictionary.items():\n",
    "    for v_ in v:\n",
    "        if len(v_.split()) > 1:\n",
    "            negeri_sembilan_compound_normalizer[v_].append(k)\n",
    "        else:\n",
    "            negeri_sembilan_normalizer[v_].append(k)\n",
    "            \n",
    "negeri_sembilan_compound_normalizer_regex = (\n",
    "    '(?:' + '|'.join(list(negeri_sembilan_compound_normalizer.keys())) + ')'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acf7515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pahang_compound_normalizer = defaultdict(list)\n",
    "pahang_normalizer = defaultdict(list)\n",
    "for k, v in pahang.dictionary.items():\n",
    "    for v_ in v:\n",
    "        if len(v_.split()) > 1:\n",
    "            pahang_compound_normalizer[v_].append(k)\n",
    "        else:\n",
    "            pahang_normalizer[v_].append(k)\n",
    "            \n",
    "pahang_compound_normalizer_regex = (\n",
    "    '(?:' + '|'.join(list(pahang_compound_normalizer.keys())) + ')'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93cff624",
   "metadata": {},
   "outputs": [],
   "source": [
    "perak_compound_normalizer = defaultdict(list)\n",
    "perak_normalizer = defaultdict(list)\n",
    "for k, v in perak.dictionary.items():\n",
    "    for v_ in v:\n",
    "        if len(v_.split()) > 1:\n",
    "            perak_compound_normalizer[v_].append(k)\n",
    "        else:\n",
    "            perak_normalizer[v_].append(k)\n",
    "            \n",
    "perak_compound_normalizer_regex = (\n",
    "    '(?:' + '|'.join(list(perak_compound_normalizer.keys())) + ')'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76a76051",
   "metadata": {},
   "outputs": [],
   "source": [
    "sabah_compound_normalizer = defaultdict(list)\n",
    "sabah_normalizer = defaultdict(list)\n",
    "for k, v in sabah.dictionary.items():\n",
    "    for v_ in v:\n",
    "        if len(v_.split()) > 1:\n",
    "            sabah_compound_normalizer[v_].append(k)\n",
    "        else:\n",
    "            sabah_normalizer[v_].append(k)\n",
    "            \n",
    "sabah_compound_normalizer_regex = (\n",
    "    '(?:' + '|'.join(list(sabah_compound_normalizer.keys())) + ')'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e53e749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialects = [\n",
    "    (kedah_compound_normalizer, kedah_normalizer, kedah_compound_normalizer_regex),\n",
    "    (kelantan_compound_normalizer, kelantan_normalizer, kelantan_compound_normalizer_regex),\n",
    "    (melaka_compound_normalizer, melaka_normalizer, melaka_compound_normalizer_regex),\n",
    "    (negeri_sembilan_compound_normalizer, negeri_sembilan_normalizer, negeri_sembilan_compound_normalizer_regex),\n",
    "    (pahang_compound_normalizer, pahang_normalizer, pahang_compound_normalizer_regex),\n",
    "    (perak_compound_normalizer, perak_normalizer, perak_compound_normalizer_regex),\n",
    "    (sabah_compound_normalizer, sabah_normalizer, sabah_compound_normalizer_regex),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bad7638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', ',', 'counters')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PUNCTUATION = '!\"#$%&\\'()*+,./:;<=>?@[\\]^_`{|}~'\n",
    "\n",
    "def case_of(text):\n",
    "    return (\n",
    "        str.upper\n",
    "        if text.isupper()\n",
    "        else str.lower\n",
    "        if text.islower()\n",
    "        else str.title\n",
    "        if text.istitle()\n",
    "        else str\n",
    "    )\n",
    "\n",
    "def strip_punct(word):\n",
    "    left = []\n",
    "    right = []\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        if word[i] in PUNCTUATION:\n",
    "            left.append(word[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            break\n",
    "    i = len(word) - 1\n",
    "    while i > 0:\n",
    "        if word[i] in PUNCTUATION:\n",
    "            right.append(word[i])\n",
    "            i -= 1\n",
    "        else:\n",
    "            break\n",
    "    left = ''.join(left)\n",
    "    right = ''.join(right[::-1])\n",
    "    if len(right):\n",
    "        word_ = word[:-len(right)]\n",
    "    else:\n",
    "        word_ = word\n",
    "    word_ = word_[len(left):]\n",
    "    return left, right, word_\n",
    "\n",
    "\n",
    "def replace_shortword(word, rules = rev_rules_normalizer):\n",
    "    left, right, word_ = strip_punct(word)\n",
    "    word_ = word_[len(left):]\n",
    "    lower_word_ = word_.lower()\n",
    "    if lower_word_ in rules:\n",
    "        word_ = case_of(word_)(random.choice(rules[lower_word_]))\n",
    "        word_ = f'{left}{word_}{right}'\n",
    "        return word_\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "strip_punct('counters,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "504d5d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _replace_compound(string, \n",
    "#                       rules_regex = rules_compound_normalizer_regex, \n",
    "#                       rules = rev_rules_compound_normalizer):\n",
    "#     results = re.findall(rules_regex, string, flags=re.IGNORECASE\n",
    "#     )\n",
    "#     for r in results:\n",
    "#         try:\n",
    "#             string = string.replace(r, random.choice(rules[r.lower()]))\n",
    "#         except:\n",
    "#             pass\n",
    "#     return string\n",
    "\n",
    "def _replace_compound(string, rules = rev_rules_compound_normalizer):\n",
    "    for k in list(rules.keys()):\n",
    "        results = [(m.start(0), m.end(0)) for m in re.finditer(k, string, flags=re.IGNORECASE)]\n",
    "        for r in results:\n",
    "            sub = string[r[0]: r[1]]\n",
    "            try:\n",
    "                replaced = random.choice(rules[sub.lower()])\n",
    "                if replaced:\n",
    "                    if r[1] < len(string) and string[r[1]] != ' ':\n",
    "                        continue\n",
    "                    if r[0] - 1 > len(string) and string[r[0] - 1] != ' ':\n",
    "                        continue\n",
    "\n",
    "                    sub = case_of(sub)(replaced)\n",
    "                    string = string[:r[0]] + sub + string[r[1]:]\n",
    "            except:\n",
    "                pass\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebb538b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'main kejar': ['aci ligan'],\n",
       "             'air lumpur': ['air acaq'],\n",
       "             'air longkang': ['air acaq'],\n",
       "             'susu basi': ['bakiaq'],\n",
       "             'tali air': ['ban'],\n",
       "             'bangun tidur': ['bangkit tidoq'],\n",
       "             'basah lencun': ['basah lokoih'],\n",
       "             'kusut masai': ['belemoih'],\n",
       "             'muka calar': ['berderemen'],\n",
       "             'calar balar': ['berderemen'],\n",
       "             'besar sangat': ['besaq gabai'],\n",
       "             'cakap besar': ['borak'],\n",
       "             'lembik berair': ['benyai'],\n",
       "             'isi durian': ['benyai'],\n",
       "             'perangai tak tentu hala': ['cempera', 'cempegha'],\n",
       "             'tak senonoh': ['cempera', 'cempegha'],\n",
       "             'kurus kering': ['cekeding'],\n",
       "             'tak cekap': ['cemerkap'],\n",
       "             'perasaan sayang': ['chewi'],\n",
       "             'anak perempuan': ['combi'],\n",
       "             'baju senget': ['cunggit'],\n",
       "             'daun kari': ['daun karapole'],\n",
       "             'kira panjat': ['ghabat'],\n",
       "             'buat perangai': ['ghaplah'],\n",
       "             'baju koyak': ['gherit'],\n",
       "             'undur belakang': ['gostan'],\n",
       "             'pengasah pensil': ['gurmit'],\n",
       "             'bagi banyak': ['hamboq'],\n",
       "             'tak guna': ['haprak'],\n",
       "             'tak boleh pakai': ['haprak'],\n",
       "             'laungan semangat': ['haria'],\n",
       "             'perangai tak elok': ['hawaq'],\n",
       "             'menunjuk hebat': ['jora'],\n",
       "             'tunjuk hebat': ['jora'],\n",
       "             'mulut celupar': ['julut hebiaq'],\n",
       "             'kelam kabut': ['kalut'],\n",
       "             'alang rumah': ['kambee'],\n",
       "             'budak higusan': ['kanyiaq'],\n",
       "             'pencuri kampung': ['kawaq'],\n",
       "             'kedekut sangat': ['kedekut pait', 'kedekut pahit'],\n",
       "             'paru-paru lembu': ['kelempung'],\n",
       "             'keras gila': ['keras kedekiang'],\n",
       "             'ke sudah': ['kesut'],\n",
       "             'sangat lapar': ['ketedarah'],\n",
       "             'degil sangat': ['ketegaq'],\n",
       "             'tempat basuh tangan': ['ketoq'],\n",
       "             'semut kepala': ['kongkiaq'],\n",
       "             'semut besar': ['kongkiaq'],\n",
       "             'kupas buah': ['kopek'],\n",
       "             'pulas minyak': ['koyak minyak'],\n",
       "             'banyak berkira': ['kore'],\n",
       "             'macam itu': ['lagu tu'],\n",
       "             'macam tu': ['lagu tu'],\n",
       "             'minum gelojoh': ['langgah'],\n",
       "             'basah sangat': ['lencun', 'basah lencun'],\n",
       "             'maki hamun': ['maarop sintok'],\n",
       "             'makan pelahap': ['makan poloq'],\n",
       "             'tak adil': ['mana aci'],\n",
       "             'tidak adil': ['mana aci'],\n",
       "             'tak aktif': ['mandom'],\n",
       "             'manis sangat': ['manih melecaih'],\n",
       "             'isi durian yang keras': ['mankaq'],\n",
       "             'isi yang keras': ['mankaq'],\n",
       "             'pengantin lelaki': ['mapley'],\n",
       "             'barang kemas': ['mastoura'],\n",
       "             'makan gelojoh': ['melahaq'],\n",
       "             'perut tak selesa': ['melugai'],\n",
       "             'rasa nak muntah': ['melugai'],\n",
       "             'mengulang kaji': ['meneghas'],\n",
       "             'mengulang baca': ['meneghas'],\n",
       "             'ulang kaji': ['meneghas'],\n",
       "             'lalu lalang': ['meneghu-meneghang'],\n",
       "             'sarapan pagi': ['menyorok'],\n",
       "             'gatal miang': ['merenyam'],\n",
       "             'makan lewat malam': ['morey'],\n",
       "             'anak hantu': ['musibat'],\n",
       "             'mak aaih': ['nalla'],\n",
       "             'mak aih': ['nalla'],\n",
       "             'mak aaiih': ['nalla'],\n",
       "             'budak hingusan': ['nangoi'],\n",
       "             'saliran air': ['parit ban'],\n",
       "             'kacang dal': ['parpu'],\n",
       "             'rojak mamak': ['pasemboq'],\n",
       "             'di perkenakan': ['pedajai'],\n",
       "             'buat terok': ['pelaq'],\n",
       "             'makan sampai licin': ['perabih buang'],\n",
       "             'makan angin': ['ronda'],\n",
       "             'muka masam': ['roshom'],\n",
       "             'sekejap lagi': ['satgi'],\n",
       "             'muka sedih': ['sebek'],\n",
       "             'luku kepala': ['sekeh'],\n",
       "             'bercapuk, bertampung': ['selepong'],\n",
       "             'solat zuhur': ['solat lohoq'],\n",
       "             'buah pelir': ['tahantuak'],\n",
       "             'tak muat': ['tak celuih'],\n",
       "             'tak sempat': ['takdan'],\n",
       "             'tak mau': ['tak kelapaq', 'tarak mau'],\n",
       "             'tak mahu': ['tak kelapaq', 'tarak mau'],\n",
       "             'tak hingin': ['tak kelapaq'],\n",
       "             'dendam simpan': ['taqham'],\n",
       "             'hentam saja': ['taram ja'],\n",
       "             'bedal ja': ['taram ja'],\n",
       "             'tiada rasa': ['tawaq'],\n",
       "             'ubi kentang': ['ubi mengala'],\n",
       "             'trouble maker': [\"ya'apur seyre\"]})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kedah_compound_normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b9dd907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kemudahan, main kejar, taram ja'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_replace_compound('kemudahan, main kejar, bedal ja', rules = kedah_compound_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1553ebb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'bingits', 'cuaaantikk\"']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'saya sangat cantik\"'\n",
    "[replace_shortword(word) for word in string.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e390d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Pembangkang',\n",
       " 'hnya',\n",
       " 'brani',\n",
       " 'cakap',\n",
       " 'big,',\n",
       " 'seolah-olah',\n",
       " 'indah',\n",
       " 'rupa',\n",
       " 'dripda',\n",
       " 'brita,\"',\n",
       " 'ktnya.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[replace_shortword(word) for word in left[1].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c6cfd63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Pembangkang',\n",
       " 'hanya',\n",
       " 'berani',\n",
       " 'cakap',\n",
       " 'besar,',\n",
       " 'seolah-olah',\n",
       " 'indah',\n",
       " 'rupa',\n",
       " 'daripada',\n",
       " 'berita,\"',\n",
       " 'katanya.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[replace_shortword(word, rules = perak_compound_normalizer) for word in left[1].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbb25de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words_punct(left_word, right_word):\n",
    "    left_left, left_right, left_word = strip_punct(left_word)\n",
    "    right_left, right_right, right_word = strip_punct(right_word)\n",
    "    return f'{left_left}{right_word}{left_right}'\n",
    "\n",
    "def random_replace_alignment(left, right, alignment, min_replace = 2, max_replace = 7):\n",
    "    splitted_left = left.split()\n",
    "    splitted_right = right.split()\n",
    "    \n",
    "    selected_alignment = []\n",
    "    for s in alignment:\n",
    "        l = s[0]\n",
    "        r = s[1]\n",
    "        if _is_number_regex(splitted_left[l].replace(',', '').replace('.', '')) or _is_number_regex(splitted_right[r].replace(',', '').replace('.', '')):\n",
    "            continue\n",
    "        elif splitted_left[l].isupper() or splitted_right[r].isupper():\n",
    "            continue\n",
    "        elif splitted_left[l] == splitted_right[r]:\n",
    "            continue\n",
    "        elif splitted_right[r].lower() in ['the', 'a', 'an', 'it', 'is', 'are']:\n",
    "            continue\n",
    "        else:\n",
    "            selected_alignment.append((l, r))\n",
    "    \n",
    "    count_replace = random.randint(min_replace, min(max_replace, len(selected_alignment)))\n",
    "    \n",
    "    selected = random.sample(selected_alignment, count_replace)\n",
    "    for s in selected:\n",
    "        splitted_left[s[0]] = replace_words_punct(splitted_left[s[0]], splitted_right[s[1]])\n",
    "    \n",
    "    return ' '.join(splitted_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af94bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random replace alignment\n",
    "# random replace compound\n",
    "# random replace word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8806c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5055752996165427"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "376f52ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sayak', 'saye']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malaya.augmentation.socialmedia_form('Saya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8bd1b357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sya'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malaya.augmentation.vowel_alternate('saya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66a630f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.check(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0a6b162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['eng', 'eng', 'malay']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_text = malaya.language_detection.fasttext()\n",
    "fast_text.predict([left[7106], right[7106], 'saya suka'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d9b02d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a737928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "consonants = 'bcdfghjklmnpqrstvwxyz'\n",
    "\n",
    "def augment(left, p_replace_shortword = 0.7, p_socialmedia = 0.85, p_vowel_alternate = 0.8, p_replace_en_shortword = 0.3):\n",
    "    \n",
    "    compound_rules_, rules_, rules_regex_ = random.choice(dialects)\n",
    "    left = _replace_compound(left, rules = copy.deepcopy(rev_rules_compound_normalizer))\n",
    "    left = _replace_compound(left, rules = copy.deepcopy(compound_rules_))\n",
    "    #left = _replace_compound(left, rules_regex = rules_regex_, rules = compound_rules_)\n",
    "    left = [replace_shortword(word, rules = en_lexicon) if random.random() > p_replace_en_shortword else word for word in left.split()]\n",
    "    left = [replace_shortword(word, rules = rules_) for word in left]\n",
    "    left = [(replace_shortword(word), False) if random.random() > p_replace_shortword else (word, True) for word in left]\n",
    "    left_ = []\n",
    "    for l in left:\n",
    "        if _is_number_regex(l[0].replace(',', '').replace('.', '')):\n",
    "            left_.append(l[0])\n",
    "            continue\n",
    "        if l[0].isupper():\n",
    "            left_.append(l[0])\n",
    "            continue\n",
    "        if l[0].istitle():\n",
    "            left_.append(l[0])\n",
    "            continue\n",
    "        if d.check(l[0]):\n",
    "            left_.append(l[0])\n",
    "            continue\n",
    "        \n",
    "        if l[1]:\n",
    "            \n",
    "            if random.random() > p_socialmedia:\n",
    "                try:\n",
    "                    r = malaya.augmentation.socialmedia_form(l[0])\n",
    "                except:\n",
    "                    r = [l[0]]\n",
    "                if len(r):\n",
    "                    s = random.choice(r)\n",
    "                else:\n",
    "                    s = l[0]\n",
    "            else:\n",
    "                s = l[0]\n",
    "                \n",
    "            if random.random() > p_vowel_alternate:\n",
    "                try:\n",
    "                    s = malaya.augmentation.vowel_alternate(s)\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    if random.random() and s[-1] == 'a' and s[-2] in consonants:\n",
    "                        s = s[:-1] + 'e'\n",
    "                except:\n",
    "                    pass\n",
    "        else:\n",
    "            s = l[0]\n",
    "        \n",
    "        left_.append(case_of(l[0])(s))\n",
    "    return ' '.join(left_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "98b72268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Terminal 1 KKIA dilengkapi kemudahan 64 kaunter daftar masuk, 12 aero bridge selain mampu menampung 3,200 penumpang dalam satu masa.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9d7739c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Terminal 1 KKIA dilngkapi kemudahan 64 kaunter dfto masuk, 12 aero brdg selain mampu menampung 3,200 penumpang dalam satuk mse cakap kasar'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augment(left[0] + ', cakap kasar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "54556d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Terminal 1 KKIA dilengkapi kemudahan 64 kaunter daftar masuk, 12 aero brdg selain mampk menampung 3,200 penumpang dalam satu masa.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augment(left[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1f0cfd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import traceback\n",
    "\n",
    "def loop(rows):\n",
    "    rows, _ = rows\n",
    "    new_left, new_right, original = [], [], []\n",
    "    for i in tqdm(range(len(rows))):\n",
    "        left, right = rows[i][0], rows[i][1]\n",
    "        if len(left.split()) > 100 or len(right.split()) > 100:\n",
    "            continue\n",
    "        langs = fast_text.predict([left, right])\n",
    "        if langs[0] == 'eng':\n",
    "            continue\n",
    "        if langs[1] == 'malay':\n",
    "            continue\n",
    "        try:\n",
    "            new_left_ = augment(left)\n",
    "            if new_left_ != left:\n",
    "                new_left.append(new_left_)\n",
    "                new_right.append(right)\n",
    "                original.append(left)\n",
    "        except Exception as e:\n",
    "            print(traceback.format_exc())\n",
    "    return [[new_left, new_right]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6576a61d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jenazah Allahyarham yang siap dimandik dan dikapan tiba dari Hospital Pulau Pinang ke eh rmahye di Taman Bersatu, Kulim kira-kira 11.30 pgi dan disambut oleh beratus-ratus orng jiran dan sahabat handai serta rakan-rakan pnlis dri Kedah dan Pulau Pinang.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augment(left[1800000 + 3160])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "60ea1dfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 1100/1100 [00:02<00:00, 471.59it/s]\n"
     ]
    }
   ],
   "source": [
    "r = loop((list(zip(left[1800000:1801100], right[1800000:1801100])),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3468fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a3537f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1700100"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3000100 - 1300000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "195c8109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 212512/212512 [13:31<00:00, 261.76it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 333.91it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 212512/212512 [13:40<00:00, 259.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 212512/212512 [13:41<00:00, 258.78it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 212512/212512 [13:42<00:00, 258.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 212512/212512 [13:43<00:00, 258.12it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 212512/212512 [13:43<00:00, 257.93it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 212512/212512 [13:44<00:00, 257.78it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 212512/212512 [13:45<00:00, 257.54it/s]\n"
     ]
    }
   ],
   "source": [
    "r = mp.multiprocessing(list(zip(left[1300000:3000100], right[1300000:3000100])), loop, cores = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b81c279a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 182306\n",
      "1 182208\n",
      "2 182280\n",
      "3 182093\n",
      "4 182232\n",
      "5 182493\n",
      "6 182590\n",
      "7 182097\n",
      "8 4\n"
     ]
    }
   ],
   "source": [
    "en, ms = [], []\n",
    "for i in range(len(r)):\n",
    "    print(i, len(r[i][0]))\n",
    "    ms.extend(r[i][0])\n",
    "    en.extend(r[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7ba25ecd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1458303, 1458303)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ms), len(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cb3ddc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Kita ada garis panduan yang lengkap, cheq rasa prturn enakmen iburn ini tidak menghalang depa bersni',\n",
       " 'Suami pelakon yanckh kini sedang hangat by filem Dukun nii iaitu Datuk Seri Khairuddin Abu Hassan bakal bertanding untuk merebut kersi Parlimen Jasin, Melaka.',\n",
       " 'Beliau kni bermain for pasukan yang bermain deep Liga Super Malaysia,Pahang FA.',\n",
       " 'Kempek Kempek merupakan sebuah desak yang terletk dalam (\"daerah\") kecamatan Gempol, Kabupaten Cirebon, Provinsi Jawa Barat, Indonesia.',\n",
       " '\"taw Inggris, untuk memutuskan korupsi butuh waktu 150 thn.',\n",
       " 'Sayang, maaf aku harus tinggal di lab. Ado bbrp maslah dalam video game tu.',\n",
       " 'Lini lingerie Savage X Fenty, misalnyak menawarkan pakaian dlm untuk bergam warna kulit dn ukrn',\n",
       " 'PRK Rantau: Gw tidak brpa optimis, tap8 harap kitak menang - Tun M',\n",
       " 'Luther sendiri tampak bersedih hati ats keputusannya untuk p.',\n",
       " '\"Saya rase selain lima isu yang dibangkitkan, kita sudah kenal psti masa depan pembangunan di Tanjung Piai spt jambatan baharu yang akan merentasi Tangair Pulai akan mendj penghubung baharu dri Johor Bahru ke eh Tanjung Piai dan amik time 20 minit.']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "138bd69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('augmented-ms-en-v4.json', 'w') as fopen:\n",
    "    json.dump({'en': en, 'ms': ms}, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9125f6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('augmented-ms-en-v4.json') as fopen:\n",
    "    data = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a827c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
