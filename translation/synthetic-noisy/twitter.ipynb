{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cf88f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/datasets/mesolitica/chatgpt-noisy-translation-twitter/resolve/main/processed-twitter.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee8842a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16327ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (5.2.0)/charset_normalizer (2.0.7) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'mesolitica/translation-t5-small-standard-bahasa-cased-v2',\n",
    "    use_fast=False\n",
    ")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\n",
    "    'mesolitica/translation-t5-small-standard-bahasa-cased-v2'\n",
    ")\n",
    "all_special_ids = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3a6f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dea2e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean(string):\n",
    "    string = re.sub(\n",
    "        'http\\\\S+|www.\\\\S+',\n",
    "        '',\n",
    "        ' '.join(\n",
    "            [\n",
    "                word\n",
    "                for word in string.split()\n",
    "                if word.find('#') < 0 and word.find('@') < 0\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    string = re.sub('[^A-Za-z ]+', ' ', string.lower())\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c84d1885",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "691013it [00:03, 209301.44it/s]\n"
     ]
    }
   ],
   "source": [
    "ls = []\n",
    "with open('processed-twitter.jsonl') as fopen:\n",
    "    for l in tqdm(fopen):\n",
    "        l = json.loads(l)\n",
    "        l['cleaned'] = clean(l['left'])\n",
    "        ls.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "103c6109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "691013"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bb7b451",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'left': '@mazwinnikanis Dalam hujan lebat some more',\n",
       " 'en': 'In heavy rain some more',\n",
       " 'ms': 'Dalam hujan lebat lagi',\n",
       " 'cleaned': 'Dalam hujan lebat some more'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03ecc2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# s = 'Dalam hujan lebat some more'\n",
    "# input_ids = tokenizer.encode(f'terjemah ke pasar Melayu: {s}', return_tensors = 'pt')\n",
    "# outputs = model.generate(input_ids.cuda(), max_length = 100, do_sample=True,\n",
    "#     top_k=50,\n",
    "#     top_p=0.95,\n",
    "#     num_return_sequences=5, temperature = 1.0, output_scores = True, return_dict_in_generate = True)\n",
    "# seqs = []\n",
    "# for o in outputs.sequences:\n",
    "#     o = tokenizer.decode([i for i in o if i not in all_special_ids], \n",
    "#                          spaces_between_special_tokens = False)\n",
    "#     seqs.append(o)\n",
    "# seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e02b64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘twitter-predict’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# !rm -rf twitter-predict\n",
    "!mkdir twitter-predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1330066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = {\n",
    "    'en': 'Manglish',\n",
    "    'ms': 'pasar Melayu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a86898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|██████████████████████████████████████▉                                              | 316587/691013 [19:46:02<65:45:49,  1.58it/s]"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(ls))):\n",
    "    filename = os.path.join('twitter-predict', f'{i}.json')\n",
    "    if os.path.exists(filename):\n",
    "        continue\n",
    "        \n",
    "    results = {'original': ls[i]}\n",
    "        \n",
    "    for lang, prefix in pairs.items():\n",
    "    \n",
    "        if ls[i][lang] and len(ls[i][lang]) > 5:\n",
    "            s = ls[i][lang]\n",
    "            input_ids = tokenizer.encode(f'terjemah ke {prefix}: {s}', return_tensors = 'pt')\n",
    "            outputs = model.generate(input_ids.cuda(), max_length = 100, do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95,\n",
    "                num_return_sequences=5, temperature = 0.7, output_scores = True, return_dict_in_generate = True)\n",
    "            logits = torch.stack(outputs.scores, dim=1)\n",
    "            score = logits.max(dim = -1).values.mean(dim = -1).detach().cpu().numpy().tolist()\n",
    "            seqs = []\n",
    "            for o in outputs.sequences:\n",
    "                o = tokenizer.decode([i for i in o if i not in all_special_ids], \n",
    "                                     spaces_between_special_tokens = False)\n",
    "                seqs.append(o)\n",
    "            \n",
    "            results[lang] = {\n",
    "                'score': score,\n",
    "                'sequences': seqs\n",
    "            }\n",
    "        \n",
    "    with open(filename, 'w') as fopen:\n",
    "        json.dump(results, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c3dc0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "870b7f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316607"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob('twitter-predict/*.json')\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db6dee40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 316607/316607 [00:27<00:00, 11537.94it/s]\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for f in tqdm(files):\n",
    "    with open(f) as fopen:\n",
    "        l = json.load(fopen)\n",
    "    predicted.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bb93520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': {'left': 'Aku esok shooting kat Sabak Bernam tak tahulah macam mana cerita dia sebab bila baca SOP baru macam migrain sikit. Hahahaha.',\n",
       "  'en': 'I have a shoot in Sabak Bernam tomorrow, not sure what the story is about because reading the SOP gives me a bit of a headache. Hahaha.',\n",
       "  'ms': 'Saya ada tembakan di Sabak Bernam esok, tidak pasti apa ceritanya kerana membaca SOP membuat saya sedikit migrain. Hahaha.',\n",
       "  'cleaned': 'Aku esok shooting kat Sabak Bernam tak tahulah macam mana cerita dia sebab bila baca SOP baru macam migrain sikit. Hahahaha.'},\n",
       " 'en': {'score': [30.524616241455078,\n",
       "   30.882753372192383,\n",
       "   30.931371688842773,\n",
       "   30.892059326171875,\n",
       "   30.45795249938965],\n",
       "  'sequences': ['sabaq bernam shoot esok taktau lah cerita apa ni sbb baca sop sakit kepala sikit hahahahaha',\n",
       "   'shoot sabak bernam esok tak sure cerita apa sebab baca sop sakit kepala sikit hahaha',\n",
       "   'sabak bernam shoot esok tak tahu lah cerita apa sebab baca sop sakit kepala sikit hahahaha',\n",
       "   'shoot sabak bernam esok tak tau apa cerita tu sebab baca sop sakit kepala sikit hahaha',\n",
       "   'i shoot sabak bernam esok x tau lah mcm mana ceritanya sbb baca sop tu sakit kepala sikit hahaha']},\n",
       " 'ms': {'score': [30.378564834594727,\n",
       "   28.000797271728516,\n",
       "   31.38227081298828,\n",
       "   28.574857711791992,\n",
       "   29.74186897277832],\n",
       "  'sequences': ['sabak bernam aku ade tembak esok dah, tak tau cerita apa sebab baca sop tu migrain sikit. hahaha',\n",
       "   'ade sajok sajbak bernam camne esok x tau citer sbb baca SOP tu migrain sikit hahaha',\n",
       "   'aku ada tembakan kat sabak bernam esok, tak tau citer apa sebab baca sop migrain sikit. hahaha.',\n",
       "   'saki bernam esok dah ada shooting  x tau citer ape sbb baca sop migrain sikit huhuhu',\n",
       "   'dah bg tembakan kat sabak bernam esok xtau citer ape sbb baca sop tu migrain sket. hahaha']}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "772e5322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████| 316607/316607 [00:02<00:00, 112953.31it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('noisy-augmentation-twitter.jsonl', 'w') as fopen:\n",
    "    for p in tqdm(predicted):\n",
    "        fopen.write(f'{json.dumps(p)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db077888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (5.2.0)/charset_normalizer (2.0.7) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe1c2283dbe48c1978b39cdcca208fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "noisy-augmentation-twitter.jsonl:   0%|          | 0.00/482M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/mesolitica/noisy-augmentation/commit/afe9f82a5b85b8bfc258d5348d9821a67ab7d45e', commit_message='Upload noisy-augmentation-twitter.jsonl with huggingface_hub', commit_description='', oid='afe9f82a5b85b8bfc258d5348d9821a67ab7d45e', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj='noisy-augmentation-twitter.jsonl',\n",
    "    path_in_repo='noisy-augmentation-twitter.jsonl',\n",
    "    repo_id='mesolitica/noisy-augmentation',\n",
    "    repo_type='dataset',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8baf7773",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranged = range(len(ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b39e661",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'left': '@mazwinnikanis Dalam hujan lebat some more',\n",
       " 'en': 'In heavy rain some more',\n",
       " 'ms': 'Dalam hujan lebat lagi',\n",
       " 'cleaned': 'Dalam hujan lebat some more'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f826296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def overlap(string1, string2):\n",
    "    l = set([w for w in clean(string1).split() if len(w) > 2])\n",
    "    r = set([w for w in clean(string2).split() if len(w) > 2])\n",
    "    return len(l & r) / len(l)\n",
    "\n",
    "overlap(ls[0]['left'], ls[1]['left'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92fa2f94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anyone nak belanja??  https://t.co/z5ColdjELX',\n",
       " 'Gaiisssss member ni single gaiiss @KhaiiJohn , pape DM, kalau dia tak reply maybe dia tak tertarik kowt dkt org tu, https://t.co/rfLLwqThK1',\n",
       " '@TuneTalk Dah seminggu line slow giler2 kat kampung paya siput, lanchang pahang.. sila ambil tindakan',\n",
       " 'Selamat Pagi Lombok Tengah,\\nRabu sebenarnya hari yang beruntung. Kenapa? Sebab, ada di tengah pekan. Tetap aman wal https://t.co/X031iLDmFm',\n",
       " 'dok pusing ke klang and shah alam, sesak mcm biasa je. mana roadblock?']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sampled = random.sample(ranged, 100)\n",
    "negs = []\n",
    "for s in sampled:\n",
    "    overlapped = overlap(ls[0]['ms'], ls[s]['ms'])\n",
    "    if overlapped < 0.05:\n",
    "        negs.append(ls[s]['left'])\n",
    "    if len(negs) >= 5:\n",
    "        break\n",
    "        \n",
    "negs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b78a104e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'left': '@mazwinnikanis Dalam hujan lebat some more',\n",
       " 'en': 'In heavy rain some more',\n",
       " 'ms': 'Dalam hujan lebat lagi',\n",
       " 'cleaned': 'Dalam hujan lebat some more'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c094cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how to make asbf sounds interesting leh',\n",
       " 'how to make asbf sounds interesting lor',\n",
       " 'How to make asbf. Sounds interesting']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_augmentation = []\n",
    "for no, score in enumerate(predicted[1]['en']['score']):\n",
    "    if score > 30:\n",
    "        en_augmentation.append(predicted[1]['en']['sequences'][no])\n",
    "        \n",
    "en_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "52ce2b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': {'left': 'Geng 12 hb ni ade tak yang nak balik terengganu ade satu ticket dah beli tapi tak jadi pergi nak bagi harga murah j https://t.co/15KiWG9vfh',\n",
       "  'en': \"Anyone going back to Terengganu on 12th September? I have a ticket but can't go. Willing to sell at a cheaper price. DM me.\",\n",
       "  'ms': 'Ada sesiapa nak balik Terengganu pada 12 September? Saya ada tiket tapi tak dapat pergi. Sedia untuk jual pada harga yang lebih murah. DM saya.',\n",
       "  'cleaned': 'Geng 12 hb ni ade tak yang nak balik terengganu ade satu ticket dah beli tapi tak jadi pergi nak bagi harga murah j '},\n",
       " 'en': {'score': [32.828792572021484,\n",
       "   33.19839859008789,\n",
       "   32.189056396484375,\n",
       "   31.707237243652344,\n",
       "   32.984588623046875],\n",
       "  'sequences': ['sesiapa balik terengganu 12sept takde tiket tak boleh pi nak jual harga lagi murah dm me',\n",
       "   'ada yang balik terengganu 12 sept ni ada ticket tapi takleh nak pi bakalan jual murah dm sikit',\n",
       "   'Anyone going back to Terengganu on 12 September? I got a ticket but cannot go. Willing to sell at a cheaper price. DM me.',\n",
       "   'Anyone going back to Terengganu on 12th September? Got ticket but cant go. Willing to sell at a cheaper price. DM me',\n",
       "   \"Anyone going back to Terengganu on 12th September? I have a ticket but can't go. Willing to sell at a cheaper price. DM me.\"]},\n",
       " 'ms': {'score': [32.65056228637695,\n",
       "   30.710777282714844,\n",
       "   32.17496109008789,\n",
       "   33.49098205566406,\n",
       "   31.255817413330078],\n",
       "  'sequences': ['Ada sapa nak balik terengganu 12 sept? Ada tiket tapi tak boleh pergi. Siap jual lagi murah. DM aku',\n",
       "   'Ada sesiapa nak balik Terengganu 12 Sep. Aku ada tiket tak dapat. Ready to sell harga murah. DM aku',\n",
       "   'Ada sapa2 nak balik terengganu 12 Sep ni? I ada tiket tapi tak boleh pergi. Siap nak jual murah lagi. DM me https://t.co/LhxYzFk7xY',\n",
       "   'Ada yang nak balik terengganu 12 september? Aku ada tiket tapi tak dapat pergi. Siap nak jual harga murah. DM aku',\n",
       "   'Ada siapa nak balik Terengganu 12 September? Ada tiket tapi tak boleh pergi. Ready nak jual murah. DM saya']}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28e08029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': [30.87648582458496,\n",
       "  30.03171157836914,\n",
       "  29.950002670288086,\n",
       "  32.05991744995117,\n",
       "  29.388994216918945],\n",
       " 'sequences': ['how to make asbf sounds interesting leh',\n",
       "  'how to make asbf sounds interesting lor',\n",
       "  'how to make asbf sound interesting lah',\n",
       "  'How to make asbf. Sounds interesting',\n",
       "  'how to make asbf sounds interesting']}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[1]['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cbfcbfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf mining-twitter\n",
    "!mkdir mining-twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f674534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def loop(rows):\n",
    "    rows, index = rows\n",
    "    for i in tqdm(range(len(rows))):\n",
    "        filename = os.path.join('mining-twitter', f'{i}-{index}.json')\n",
    "        if os.path.exists(filename):\n",
    "            continue\n",
    "        \n",
    "        sampled = random.sample(ranged, 100)\n",
    "        negs = []\n",
    "        for s in sampled:\n",
    "            try:\n",
    "                overlapped = overlap(rows[i]['original']['ms'], ls[s]['ms'])\n",
    "            except:\n",
    "                continue\n",
    "            if overlapped < 0.05:\n",
    "                negs.append(ls[s]['left'])\n",
    "            if len(negs) >= 5:\n",
    "                break\n",
    "                \n",
    "        en = rows[i]['original']['en']\n",
    "        ms = rows[i]['original']['ms']\n",
    "        \n",
    "        en_augmentation = []\n",
    "        try:\n",
    "            for no, score in enumerate(rows[i]['en']['score']):\n",
    "                if score > 30:\n",
    "                    en_augmentation.append(rows[i]['en']['sequences'][no])\n",
    "            en_augmentation = list(set(en_augmentation))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        ms_augmentation = []\n",
    "        try:\n",
    "            for no, score in enumerate(rows[i]['ms']['score']):\n",
    "                if score > 30:\n",
    "                    ms_augmentation.append(rows[i]['ms']['sequences'][no])\n",
    "            ms_augmentation = list(set(ms_augmentation))\n",
    "        except:\n",
    "            pass\n",
    "                \n",
    "        d = {\n",
    "            'negs': negs,\n",
    "            'pos': list(set([en, ms] + en_augmentation + ms_augmentation)),\n",
    "            'query': rows[i]['original']['left'],\n",
    "        }\n",
    "        \n",
    "        with open(filename, 'w') as fopen:\n",
    "            json.dump(d, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc68b733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 73035.87it/s]\n"
     ]
    }
   ],
   "source": [
    "loop((predicted[:1000],0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "57d6e84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://gist.githubusercontent.com/huseinzol05/98974ae8c6c7a65d4bc0af9f5003786a/raw/2e06e71ef7349a57bc58cc9913ae6bae1f9f8447/mp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "41535d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ac6d6989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 31660/31660 [00:05<00:00, 5372.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 31660/31660 [00:05<00:00, 5310.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 31660/31660 [00:05<00:00, 5282.61it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 31660/31660 [00:06<00:00, 5224.26it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 31660/31660 [00:05<00:00, 5295.88it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 31660/31660 [00:06<00:00, 5248.44it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 31660/31660 [00:05<00:00, 5283.67it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 3417.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 31660/31660 [00:05<00:00, 5311.24it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 31660/31660 [00:05<00:00, 5357.44it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 31660/31660 [00:05<00:00, 5457.08it/s]\n"
     ]
    }
   ],
   "source": [
    "mp.multiprocessing(predicted, loop, cores = 10, returned = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "52edc4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316607"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob('mining-twitter/*.json')\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "330f9626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 316607/316607 [00:12<00:00, 24393.06it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('mining-twitter.jsonl', 'w') as fopen_l:\n",
    "    for f in tqdm(files):\n",
    "        try:\n",
    "            with open(f) as fopen:\n",
    "                data = json.load(fopen)\n",
    "            fopen_l.write(f'{json.dumps(data)}\\n')\n",
    "            fopen_l.flush()\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c69aff60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24028adaf7004e0f8adbed72365c7a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mining-twitter.jsonl:   0%|          | 0.00/394M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/mesolitica/title-context-pair/commit/93f020b3dc6def5325ec2cbee6de903e38ad74de', commit_message='Upload mining-twitter.jsonl with huggingface_hub', commit_description='', oid='93f020b3dc6def5325ec2cbee6de903e38ad74de', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj='mining-twitter.jsonl',\n",
    "    path_in_repo='mining-twitter.jsonl',\n",
    "    repo_id='mesolitica/title-context-pair',\n",
    "    repo_type='dataset',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b0dd91f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negs': ['Rabu / 18 Mei 2022 / 17 Syawal 1443H\\n5:51pg - Masuk waktu solat fardhu #Subuh bagi Pulau Pinang &amp; kwsn yg sama wakt https://t.co/zfdy4mpC1x',\n",
       "  '@nilamsaniiiiii @idek_hm ngak g gya ... matik bak isik borang sen agy',\n",
       "  'Labuan Bajo adalah salah satu destinasi wisata super prioritas dan premium, namun kami ingin agar wisatawan nusatar https://t.co/hVzQPVLFLS',\n",
       "  '@BangRiz91376468 Mau, dong....',\n",
       "  'gak kasian sama aku ya? oke anak kita nambah https://t.co/WG6a1DyT0g'],\n",
       " 'pos': ['For your information, last week during Eid al-Fitr, Ms Maharani was known to have had a political meeting with the Chairman of https://t.co/E1ITii7Pcl.',\n",
       "  'Sebagai makluman, minggu lalu semasa raya al-fitri, Puan Maharani diketahui telah mengadakan pertemuan politik dengan Pengerusi https://t.co/E1ITii7Pcl',\n",
       "  'Untuk pengetahuan, pekan lalu saat lebaran, Puan Maharani diketahui telah mengadakan pertemuan politik dengan Kepala Maj https://t.co/E1ITii7Pcl',\n",
       "  'Sebagai informasi, pekan lalu selama lebaran, Puan Maharani diketahui telah mengadakan pertemuan politik dengan Pengerusi https://t.co/E1ITii7Pcl.',\n",
       "  'Untuk makluman, minggu lalu semasa Eid al-Fitr, Puan Maharani diketahui telah mengadakan pertemuan politik dengan Pengerusi https://t.co/E1ITii7Pcl',\n",
       "  'Untuk pengetahuan, minggu lalu selama lebaran, Puan Maharani diketahui telah melakukan pertemuan politik dengan Pengerusi https://t.co/E1ITii7Pcl',\n",
       "  'Sebagai makluman, minggu lalu semasa Eid al-Fitr, Puan Maharani diketahui telah mengadakan pertemuan politik dengan Pengerusi https://t.co/E1ITii7Pcl.',\n",
       "  'For your information, last week during Eid al-Fitr, Puan Maharani was known to have had a political meeting with the Chairman of https://t.co/E1ITii7Pcl.'],\n",
       " 'query': 'Sebagai informasi, pekan lalu selama lebaran, Puan Maharani diketahui melalukan pertemuan politik dengan Ketua Umum https://t.co/E1ITii7Pcl'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38bd05a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
