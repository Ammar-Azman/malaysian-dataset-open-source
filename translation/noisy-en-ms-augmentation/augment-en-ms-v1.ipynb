{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b8fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/train-en-ms.tar.gz\n",
    "# !tar -zxf train-en-ms.tar.gz\n",
    "# !rm train-en-ms.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4229a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pyenchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56b79e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import enchant\n",
    "# d = enchant.Dict(\"en_US\")\n",
    "# d.check(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "115c2ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/huseinzol05/malay-dataset/master/normalization/en-lexicon/en-social-media-lexicon.json\n",
    "# !wget https://raw.githubusercontent.com/huseinzol05/malay-dataset/master/normalization/en-lexicon/en-social-media-lexicon-v2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51eb22f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe86564f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.6G\r\n",
      "drwxr-xr-x  2 ubuntu ubuntu 4.0K Jun  28  2020 .\r\n",
      "drwxr-xr-x 43 ubuntu ubuntu  12K Jul  12 22:03 ..\r\n",
      "-rw-r--r--  1 ubuntu ubuntu 772M Ogos  2  2020 left.txt\r\n",
      "-rw-r--r--  1 ubuntu ubuntu 860M Ogos  2  2020 right.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lha train-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0741e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train-en/left.txt') as fopen:\n",
    "    left = fopen.read().split('\\n')\n",
    "    \n",
    "with open('train-en/right.txt') as fopen:\n",
    "    right = fopen.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "042d9bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The goal is to make sure the people who receive the land are the ones who really deserve it.',\n",
       " 'Tujuannya bagi memastikan pihak yang menerima tanah itu adalah mereka yang benar-benar layak.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left[7106], right[7106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "762dce3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3807616, 3807616)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(left), len(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9272db3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.5.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow_addons/utils/resource_loader.py:72: UserWarning: You are currently using TensorFlow 2.6.0 and trying to load a custom op (custom_ops/seq2seq/_beam_search_ops.so).\n",
      "TensorFlow Addons has compiled its custom ops against TensorFlow 2.4.0, and there are no compatibility guarantees between the two versions. \n",
      "This means that you might get segfaults when loading the custom op, or other kind of low-level errors.\n",
      " If you do, do not file an issue on Github. This is a known limitation.\n",
      "\n",
      "It might help you to fallback to pure Python ops with TF_ADDONS_PY_OPS . To do that, see https://github.com/tensorflow/addons#gpucpu-custom-ops \n",
      "\n",
      "You can also change the TensorFlow version installed on your system. You would need a TensorFlow version equal to or above 2.4.0 and strictly below 2.5.0.\n",
      " Note that nightly versions of TensorFlow, as well as non-pip TensorFlow like `conda install tensorflow` or compiled from source are not supported.\n",
      "\n",
      "The last solution is to find the TensorFlow Addons version that has custom ops compatible with the TensorFlow installed on your system. To do that, refer to the readme: https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/malaya_boilerplate/frozen_graph.py:35: UserWarning: Cannot import beam_search_ops from Tensorflow Addons, ['malaya.jawi_rumi.deep_model', 'malaya.phoneme.deep_model', 'malaya.rumi_jawi.deep_model', 'malaya.stem.deep_model'] will not available to use, make sure Tensorflow Addons version >= 0.12.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import malaya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aa318d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from malaya.text.rules import rules_normalizer, rules_compound_normalizer\n",
    "from malaya.text.normalization import _is_number_regex\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ff9548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en-social-media-lexicon.json') as fopen:\n",
    "    en_lexicon = json.load(fopen)\n",
    "    \n",
    "with open('en-social-media-lexicon-v2.json') as fopen:\n",
    "    en_lexicon_v2 = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d284466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_normalizer = defaultdict(list)\n",
    "normalizer = defaultdict(list)\n",
    "\n",
    "for k, v in en_lexicon.items():\n",
    "    if not len(v):\n",
    "        continue\n",
    "    if len(k.split()) > 1:\n",
    "        compound_normalizer[k].extend(v)\n",
    "    else:\n",
    "        normalizer[k].extend(v)\n",
    "\n",
    "for k, v in en_lexicon_v2.items():\n",
    "    if not len(v):\n",
    "        continue\n",
    "    if len(k.split()) > 1:\n",
    "        compound_normalizer[k].extend(v)\n",
    "    else:\n",
    "        normalizer[k].extend(v)\n",
    "        \n",
    "compound_normalizer_regex = (\n",
    "    '(?:' + '|'.join(list(compound_normalizer.keys())) + ')'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5bad7638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', ',', 'counters')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PUNCTUATION = '!\"#$%&\\'()*+,./:;<=>?@[\\]^_`{|}~'\n",
    "\n",
    "def case_of(text):\n",
    "    return (\n",
    "        str.upper\n",
    "        if text.isupper()\n",
    "        else str.lower\n",
    "        if text.islower()\n",
    "        else str.title\n",
    "        if text.istitle()\n",
    "        else str\n",
    "    )\n",
    "\n",
    "def strip_punct(word):\n",
    "    left = []\n",
    "    right = []\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        if word[i] in PUNCTUATION:\n",
    "            left.append(word[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            break\n",
    "    i = len(word) - 1\n",
    "    while i > 0:\n",
    "        if word[i] in PUNCTUATION:\n",
    "            right.append(word[i])\n",
    "            i -= 1\n",
    "        else:\n",
    "            break\n",
    "    left = ''.join(left)\n",
    "    right = ''.join(right[::-1])\n",
    "    if len(right):\n",
    "        word_ = word[:-len(right)]\n",
    "    else:\n",
    "        word_ = word\n",
    "    word_ = word_[len(left):]\n",
    "    return left, right, word_\n",
    "\n",
    "\n",
    "def replace_shortword(word, rules = normalizer):\n",
    "    left, right, word_ = strip_punct(word)\n",
    "    word_ = word_[len(left):]\n",
    "    lower_word_ = word_.lower()\n",
    "    if lower_word_ in rules:\n",
    "        word_ = case_of(word_)(random.choice(rules[lower_word_]))\n",
    "        word_ = f'{left}{word_}{right}'\n",
    "        return word_\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "strip_punct('counters,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "504d5d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _replace_compound(string, \n",
    "#                       rules_regex = rules_compound_normalizer_regex, \n",
    "#                       rules = rev_rules_compound_normalizer):\n",
    "#     results = re.findall(rules_regex, string, flags=re.IGNORECASE\n",
    "#     )\n",
    "#     for r in results:\n",
    "#         try:\n",
    "#             string = string.replace(r, random.choice(rules[r.lower()]))\n",
    "#         except:\n",
    "#             pass\n",
    "#     return string\n",
    "\n",
    "def _replace_compound(string, rules = compound_normalizer):\n",
    "    for k in list(rules.keys()):\n",
    "        results = [(m.start(0), m.end(0)) for m in re.finditer(k, string, flags=re.IGNORECASE)]\n",
    "        for r in results:\n",
    "            sub = string[r[0]: r[1]]\n",
    "            try:\n",
    "                replaced = random.choice(rules[sub.lower()])\n",
    "                if replaced:\n",
    "                    if r[1] < len(string) and string[r[1]] != ' ':\n",
    "                        continue\n",
    "                    if r[0] - 1 > len(string) and string[r[0] - 1] != ' ':\n",
    "                        continue\n",
    "\n",
    "                    sub = case_of(sub)(replaced)\n",
    "                    string = string[:r[0]] + sub + string[r[1]:]\n",
    "            except:\n",
    "                pass\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b9dd907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i like, ttc'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_replace_compound('i like, text to cell', rules = compound_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1553ebb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iiii', 'amt', 'soooooo', 'bueityful\"']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'i am so beautiful\"'\n",
    "[replace_shortword(word) for word in string.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e390d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pitt',\n",
       " 'haves',\n",
       " 'snc',\n",
       " 'addopted*2',\n",
       " 'buth',\n",
       " 'clraner.',\n",
       " 'Iin',\n",
       " '2006,',\n",
       " 'thge',\n",
       " 'cupple',\n",
       " 'head',\n",
       " \"its'\",\n",
       " 'fast',\n",
       " 'biological',\n",
       " 'dauter,',\n",
       " 'Shiloh',\n",
       " 'Nouvel,',\n",
       " 'hole',\n",
       " 'prefers',\n",
       " 'tew',\n",
       " 'by*3',\n",
       " 'cald',\n",
       " 'Johhn,',\n",
       " 'acording*2',\n",
       " '2',\n",
       " 'They*6',\n",
       " 'Telagraph.',\n",
       " 'Thge',\n",
       " 'cupple',\n",
       " 'addopted*2',\n",
       " 'Pax',\n",
       " 'Thien,',\n",
       " 'hor',\n",
       " '12,',\n",
       " 'pefor',\n",
       " 'Jolie',\n",
       " 'cave*6',\n",
       " 'berth',\n",
       " 'tp',\n",
       " 'twins',\n",
       " 'Knox',\n",
       " 'Leon',\n",
       " 'anbd',\n",
       " 'Vivienne',\n",
       " 'Marcheline,',\n",
       " 'noe',\n",
       " 'seven.',\n",
       " 'I',\n",
       " 'Januari,',\n",
       " 'Jolie',\n",
       " 'maked',\n",
       " 'headlines',\n",
       " 'of*3',\n",
       " 'bringin',\n",
       " 'hier',\n",
       " 'kidz',\n",
       " 'ko',\n",
       " 'thaa',\n",
       " 'premeire',\n",
       " 'on*2',\n",
       " '\"Kung',\n",
       " 'Fu',\n",
       " 'Panda',\n",
       " '3\".',\n",
       " 'Jolie',\n",
       " 'andd',\n",
       " 'Pitt',\n",
       " 'alls',\n",
       " 'reportedly',\n",
       " 'bareng',\n",
       " 'aling',\n",
       " 'and',\n",
       " 'teem',\n",
       " 'with',\n",
       " 'nannies',\n",
       " 'than',\n",
       " 'tichers',\n",
       " 'wereever',\n",
       " 'their',\n",
       " 'travill.',\n",
       " 'Acording*2',\n",
       " 'ko',\n",
       " 'Jolie,',\n",
       " 'millionnaire',\n",
       " 'moms',\n",
       " 'shouldnt',\n",
       " 'complane',\n",
       " 'to',\n",
       " 'juggling',\n",
       " 'workk',\n",
       " 'i',\n",
       " 'ffam',\n",
       " 'becau',\n",
       " 'thay',\n",
       " 'haven',\n",
       " 'moar',\n",
       " 'acess,',\n",
       " 'supp',\n",
       " 'n',\n",
       " 'leaway',\n",
       " 'then',\n",
       " 'tehe',\n",
       " 'avg',\n",
       " 'womon:.',\n",
       " '\"I\\'m',\n",
       " 'is',\n",
       " 'aa',\n",
       " 'rarr',\n",
       " 'posistion',\n",
       " 'ther',\n",
       " 'IIII',\n",
       " \"do'nt\",\n",
       " 'fo',\n",
       " 'tow*2',\n",
       " 'boo',\n",
       " 'gob',\n",
       " 'nfre',\n",
       " 'job,\"',\n",
       " 'shhh',\n",
       " 'toled',\n",
       " 'T',\n",
       " 'Daly*2',\n",
       " 'Nieuws',\n",
       " 'i',\n",
       " 'Nay',\n",
       " '2014.',\n",
       " '\"I',\n",
       " 'cn',\n",
       " 'tack*2',\n",
       " 'tim',\n",
       " 'whne',\n",
       " 'ma',\n",
       " 'famly',\n",
       " 'nds',\n",
       " 'ittt\".',\n",
       " 'Tish',\n",
       " 'artical*2',\n",
       " 'organly',\n",
       " 'appeard',\n",
       " 'onnnn',\n",
       " 'GoBankingRates.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[replace_shortword(word) for word in left[1].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bbb25de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words_punct(left_word, right_word):\n",
    "    left_left, left_right, left_word = strip_punct(left_word)\n",
    "    right_left, right_right, right_word = strip_punct(right_word)\n",
    "    return f'{left_left}{right_word}{left_right}'\n",
    "\n",
    "def random_replace_alignment(left, right, alignment, min_replace = 2, max_replace = 7):\n",
    "    splitted_left = left.split()\n",
    "    splitted_right = right.split()\n",
    "    \n",
    "    selected_alignment = []\n",
    "    for s in alignment:\n",
    "        l = s[0]\n",
    "        r = s[1]\n",
    "        if _is_number_regex(splitted_left[l].replace(',', '').replace('.', '')) or _is_number_regex(splitted_right[r].replace(',', '').replace('.', '')):\n",
    "            continue\n",
    "        elif splitted_left[l].isupper() or splitted_right[r].isupper():\n",
    "            continue\n",
    "        elif splitted_left[l] == splitted_right[r]:\n",
    "            continue\n",
    "        elif splitted_right[r].lower() in ['the', 'a', 'an', 'it', 'is', 'are']:\n",
    "            continue\n",
    "        else:\n",
    "            selected_alignment.append((l, r))\n",
    "    \n",
    "    count_replace = random.randint(min_replace, min(max_replace, len(selected_alignment)))\n",
    "    \n",
    "    selected = random.sample(selected_alignment, count_replace)\n",
    "    for s in selected:\n",
    "        splitted_left[s[0]] = replace_words_punct(splitted_left[s[0]], splitted_right[s[1]])\n",
    "    \n",
    "    return ' '.join(splitted_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af94bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random replace alignment\n",
    "# random replace compound\n",
    "# random replace word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8806c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5055752996165427"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "376f52ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sayak', 'saye']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malaya.augmentation.socialmedia_form('Saya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8bd1b357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sya'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malaya.augmentation.vowel_alternate('saya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66a630f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.check(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0a6b162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['eng', 'malay', 'malay']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_text = malaya.language_detection.fasttext()\n",
    "fast_text.predict([left[7106], right[7106], 'saya suka'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d9b02d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a737928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "consonants = 'bcdfghjklmnpqrstvwxyz'\n",
    "\n",
    "def augment(left, p_replace_shortword = 0.7, p_vowel_alternate = 0.8):\n",
    "    \n",
    "    left = _replace_compound(left, rules = copy.deepcopy(compound_normalizer))\n",
    "    left = [(replace_shortword(word, rules = normalizer), False) if random.random() > p_replace_shortword else (word, True) for word in left.split()]\n",
    "    left_ = []\n",
    "    for l in left:\n",
    "        if _is_number_regex(l[0].replace(',', '').replace('.', '')):\n",
    "            left_.append(l[0])\n",
    "            continue\n",
    "        if l[0].isupper():\n",
    "            left_.append(l[0])\n",
    "            continue\n",
    "        if l[0].istitle():\n",
    "            left_.append(l[0])\n",
    "            continue\n",
    "        \n",
    "        if l[1]:\n",
    "            s = l[0]\n",
    "                \n",
    "            if random.random() > p_vowel_alternate:\n",
    "                try:\n",
    "                    s = malaya.augmentation.vowel_alternate(s)\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    if random.random() and s[-1] == 'a' and s[-2] in consonants:\n",
    "                        s = s[:-1] + 'e'\n",
    "                except:\n",
    "                    pass\n",
    "        else:\n",
    "            s = l[0]\n",
    "        \n",
    "        left_.append(case_of(l[0])(s))\n",
    "    return ' '.join(left_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6ec51d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lke'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malaya.augmentation.vowel_alternate('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "98b72268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Most of them are teenagers or in their 20s.'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d7739c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Most ofr them are teenagers or in their 20s., fml'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augment(left[0] + ', fuck my life')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "54556d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 305 µs, sys: 122 µs, total: 427 µs\n",
      "Wall time: 431 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Most of them are teenagers or in their 20s.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "augment(left[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1f0cfd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import traceback\n",
    "\n",
    "def loop(rows):\n",
    "    rows, _ = rows\n",
    "    new_left, new_right, original = [], [], []\n",
    "    for i in tqdm(range(len(rows))):\n",
    "        left, right = rows[i][0], rows[i][1]\n",
    "        if len(left.split()) > 100 or len(right.split()) > 100:\n",
    "            continue\n",
    "        langs = fast_text.predict([left, right])\n",
    "        if langs[0] == 'malay':\n",
    "            continue\n",
    "        if langs[1] == 'eng':\n",
    "            continue\n",
    "        try:\n",
    "            new_left_ = augment(left)\n",
    "            if new_left_ != left:\n",
    "                new_left.append(new_left_)\n",
    "                new_right.append(right)\n",
    "                original.append(left)\n",
    "        except Exception as e:\n",
    "            print(traceback.format_exc())\n",
    "    return [[new_left, new_right]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6576a61d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yupp that's yoy you choud quit now\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augment(left[1800000 + 3160])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "60ea1dfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1198.34it/s]\n"
     ]
    }
   ],
   "source": [
    "r = loop((list(zip(left[1800000:1800010], right[1800000:1800010])),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d569d3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['Dhaa government has given us alots on*2 help but we need 2 get outte our minds fourth free assistence.',\n",
       "   \"Allah Almighty sez in the Qur'an Surat An-Nisa Verse 86: If you r honred wth honor then respond whit a better respect, or respond with duh sam.\",\n",
       "   'Apart fro tht said Baharudin Djafar, an Sulbar Police si also prepairing concrete measures to prvde security tp the public frm vairous possible acks of crime and terrorism over the years',\n",
       "   'we ve graphed our parabolas.',\n",
       "   'Mooe than 170 millon copys of his book have been sold with 40 languages worldwide, meny*2 of themmmmm into firms.',\n",
       "   'SOE Effords tew Hellp Rescue Donggala and Rescue Earthquake and Tsunami Page all',\n",
       "   'Of parents e chiden yoou bo not knw hwo are more usfull*2 to youuuuu.'],\n",
       "  ['Kerajaan sudah memberikan banyak bantuan tetapi kita kena keluar daripada pemikiran untuk mendapat bantuan percuma.',\n",
       "   \"Allah SWT berfirman didalam Al-Qur'an Surat An-Nisa Ayat 86: Apabila kamu dihormati dengan suatu penghormatan maka balaslah dengan penghormatan yang lebih baik, atau balaslah dengan yang serupa.\",\n",
       "   'Selain itu, kata Baharudin Djafar, Polda Sulbar juga mempersiapkan langkah-langkah kongkrik untuk memberikan jaminan keamanan kepada masyarakat dari berbagai kemungkinan aksi kejahatan maupun teror jelang penggantian tahun.',\n",
       "   'KIta sudah pun merangka parabola kita.',\n",
       "   'Melebihi 170 juta salinan bukunya telah dijual dalam 40 bahasa diseluruh dunia, dengan banyaknya dijadikan filem.',\n",
       "   'Upaya BUMN Bantu Pulihkan Donggala dan Palu Usai Gempa dan Tsunami Halaman all',\n",
       "   'Kalau pula si mati itu mempunyai beberapa orang saudara (adik-beradik), maka bahagian ibunya ialah satu perenam.']]]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3468fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3a9aea50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3807616"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "195c8109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 150000/150000 [02:20<00:00, 1065.29it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 150000/150000 [02:21<00:00, 1059.07it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 150000/150000 [02:22<00:00, 1056.27it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 150000/150000 [02:21<00:00, 1058.60it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 150000/150000 [02:21<00:00, 1057.91it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 150000/150000 [02:22<00:00, 1052.77it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 150000/150000 [02:23<00:00, 1048.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 150000/150000 [02:23<00:00, 1043.73it/s]\n"
     ]
    }
   ],
   "source": [
    "r = mp.multiprocessing(list(zip(left[:1200000], right[:1200000])), loop, cores = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b81c279a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 123137\n",
      "1 123110\n",
      "2 123105\n",
      "3 123253\n",
      "4 123105\n",
      "5 123171\n",
      "6 123066\n",
      "7 122843\n"
     ]
    }
   ],
   "source": [
    "en, ms = [], []\n",
    "for i in range(len(r)):\n",
    "    print(i, len(r[i][0]))\n",
    "    en.extend(r[i][0])\n",
    "    ms.extend(r[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7ba25ecd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(984790, 984790)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ms), len(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "78e784d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"They promise to dowr sch outlying quarters ass Edinburgh's Westen and Granton harbours back the the heart of the existing sied.\",\n",
       " 'Not alot buts overdue is*2 when I do',\n",
       " \"Ir 1967, Sultan Haji Omar 'Ali Saifuddien Sa'adul Khairi Waddien stepped down frm tehe throne and appointed his eldest son, Sultan Haji Hassanal Bolkiah Mu'izzaddin Waddaulah, tooo bacome the 29th Sultan of Brunei.\",\n",
       " 'It is understud that the investigation into the corruption of the government agentcy involvd nearly RM3 millon.',\n",
       " \"Anwar is already a Memeber of Parliament and der country iz generally ready to accept him as Mahathir's replasments when tie time comes.\",\n",
       " \"The name given to Lakshamilavan's daughtr was Mom Chao Varnbimol Voravarn.\",\n",
       " 'The chemotherapy treatment for tuberculosis was only started in 1946 wiv teh dscovery of streptomycin und wos*5 followed by the intro of isniazid as the first effective antitubi drg in 1952.',\n",
       " 'Dee Zip code used att First it 46192.',\n",
       " 'You con compare thee*2 options through',\n",
       " \"Ther fnancial problem for the clob*3 were tjhe main cause from Pupo's dparture\"]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cb3ddc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Mereka berjanji untuk menarik tempat terpencil seperti Edinburgh's Western dan Granton kembali ke tengah-tengah kota yang ada.\",\n",
       " 'Tidak banyak tetapi terlambat ketika saya melakukan',\n",
       " \"Pada tahun 1967, Sultan Haji Omar 'Ali Saifuddien Sa'adul Khairi Waddien telah turun dari takhta dan melantik anakanda sulung baginda, Sultan Haji Hassanal Bolkiah Mu'izzaddin Waddaulah, menjadi Sultan Brunei ke-29.\",\n",
       " 'Difahamkan, siasatan kes rasuah agensi kerajaan tersebut melibatkan nilai hampir RM3 juta.',\n",
       " 'Anwar pun sudah menjadi ahli Parlimen dan umumnya negara sentiasa bersedia menerimanya sebagai pengganti Mahathir apabila tiba masanya.',\n",
       " 'Nama yang diberikan puteri Lakshamilavan adalah Mom Chao Varnbimol Voravarn.',\n",
       " 'Rawatan kemoterapi bagi merawat tibi hanya mula ditemui pada 1946 dengan penemuan streptomisin dan diikuti pengenalan isoniazid sebagai ubat antitibi barisan pertama yang efektif pada 1952.',\n",
       " 'Poskod yang digunakan di Kawitan adalah 46192.',\n",
       " 'Anda boleh membandingkan tiga pilihan melalui .',\n",
       " 'Masalah kewangan kelab itu menjadi punca utama Pupo keluar dari kelab itu.']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "138bd69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('augmented-en-ms-v1.json', 'w') as fopen:\n",
    "    json.dump({'en': en, 'ms': ms}, fopen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
