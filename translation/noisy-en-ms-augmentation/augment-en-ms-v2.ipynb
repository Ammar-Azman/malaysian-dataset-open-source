{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b8fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/train-en-ms.tar.gz\n",
    "# !tar -zxf train-en-ms.tar.gz\n",
    "# !rm train-en-ms.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4229a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pyenchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b79e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import enchant\n",
    "# d = enchant.Dict(\"en_US\")\n",
    "# d.check(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "115c2ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/huseinzol05/malay-dataset/master/normalization/en-lexicon/en-social-media-lexicon.json\n",
    "# !wget https://raw.githubusercontent.com/huseinzol05/malay-dataset/master/normalization/en-lexicon/en-social-media-lexicon-v2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51eb22f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe86564f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.6G\r\n",
      "drwxr-xr-x  2 ubuntu ubuntu 4.0K Jun  28  2020 .\r\n",
      "drwxr-xr-x 45 ubuntu ubuntu  12K Jul  13 18:58 ..\r\n",
      "-rw-r--r--  1 ubuntu ubuntu 772M Ogos  2  2020 left.txt\r\n",
      "-rw-r--r--  1 ubuntu ubuntu 860M Ogos  2  2020 right.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lha train-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0741e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train-en/left.txt') as fopen:\n",
    "    left = fopen.read().split('\\n')\n",
    "    \n",
    "with open('train-en/right.txt') as fopen:\n",
    "    right = fopen.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "042d9bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The goal is to make sure the people who receive the land are the ones who really deserve it.',\n",
       " 'Tujuannya bagi memastikan pihak yang menerima tanah itu adalah mereka yang benar-benar layak.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left[7106], right[7106]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "762dce3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3807616, 3807616)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(left), len(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9272db3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.3.0 and strictly below 2.5.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/tensorflow_addons/utils/resource_loader.py:72: UserWarning: You are currently using TensorFlow 2.6.0 and trying to load a custom op (custom_ops/seq2seq/_beam_search_ops.so).\n",
      "TensorFlow Addons has compiled its custom ops against TensorFlow 2.4.0, and there are no compatibility guarantees between the two versions. \n",
      "This means that you might get segfaults when loading the custom op, or other kind of low-level errors.\n",
      " If you do, do not file an issue on Github. This is a known limitation.\n",
      "\n",
      "It might help you to fallback to pure Python ops with TF_ADDONS_PY_OPS . To do that, see https://github.com/tensorflow/addons#gpucpu-custom-ops \n",
      "\n",
      "You can also change the TensorFlow version installed on your system. You would need a TensorFlow version equal to or above 2.4.0 and strictly below 2.5.0.\n",
      " Note that nightly versions of TensorFlow, as well as non-pip TensorFlow like `conda install tensorflow` or compiled from source are not supported.\n",
      "\n",
      "The last solution is to find the TensorFlow Addons version that has custom ops compatible with the TensorFlow installed on your system. To do that, refer to the readme: https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/malaya_boilerplate/frozen_graph.py:35: UserWarning: Cannot import beam_search_ops from Tensorflow Addons, ['malaya.jawi_rumi.deep_model', 'malaya.phoneme.deep_model', 'malaya.rumi_jawi.deep_model', 'malaya.stem.deep_model'] will not available to use, make sure Tensorflow Addons version >= 0.12.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import malaya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1aa318d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from malaya.text.rules import rules_normalizer, rules_compound_normalizer\n",
    "from malaya.text.normalization import _is_number_regex\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ff9548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en-social-media-lexicon.json') as fopen:\n",
    "    en_lexicon = json.load(fopen)\n",
    "    \n",
    "with open('en-social-media-lexicon-v2.json') as fopen:\n",
    "    en_lexicon_v2 = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d284466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_normalizer = defaultdict(list)\n",
    "normalizer = defaultdict(list)\n",
    "\n",
    "for k, v in en_lexicon.items():\n",
    "    if not len(v):\n",
    "        continue\n",
    "    if len(k.split()) > 1:\n",
    "        compound_normalizer[k].extend(v)\n",
    "    else:\n",
    "        normalizer[k].extend(v)\n",
    "\n",
    "for k, v in en_lexicon_v2.items():\n",
    "    if not len(v):\n",
    "        continue\n",
    "    if len(k.split()) > 1:\n",
    "        compound_normalizer[k].extend(v)\n",
    "    else:\n",
    "        normalizer[k].extend(v)\n",
    "        \n",
    "compound_normalizer_regex = (\n",
    "    '(?:' + '|'.join(list(compound_normalizer.keys())) + ')'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bad7638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', ',', 'counters')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PUNCTUATION = '!\"#$%&\\'()*+,./:;<=>?@[\\]^_`{|}~'\n",
    "\n",
    "def case_of(text):\n",
    "    return (\n",
    "        str.upper\n",
    "        if text.isupper()\n",
    "        else str.lower\n",
    "        if text.islower()\n",
    "        else str.title\n",
    "        if text.istitle()\n",
    "        else str\n",
    "    )\n",
    "\n",
    "def strip_punct(word):\n",
    "    left = []\n",
    "    right = []\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        if word[i] in PUNCTUATION:\n",
    "            left.append(word[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            break\n",
    "    i = len(word) - 1\n",
    "    while i > 0:\n",
    "        if word[i] in PUNCTUATION:\n",
    "            right.append(word[i])\n",
    "            i -= 1\n",
    "        else:\n",
    "            break\n",
    "    left = ''.join(left)\n",
    "    right = ''.join(right[::-1])\n",
    "    if len(right):\n",
    "        word_ = word[:-len(right)]\n",
    "    else:\n",
    "        word_ = word\n",
    "    word_ = word_[len(left):]\n",
    "    return left, right, word_\n",
    "\n",
    "\n",
    "def replace_shortword(word, rules = normalizer):\n",
    "    left, right, word_ = strip_punct(word)\n",
    "    word_ = word_[len(left):]\n",
    "    lower_word_ = word_.lower()\n",
    "    if lower_word_ in rules:\n",
    "        word_ = case_of(word_)(random.choice(rules[lower_word_]))\n",
    "        word_ = f'{left}{word_}{right}'\n",
    "        return word_\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "strip_punct('counters,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "504d5d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _replace_compound(string, \n",
    "#                       rules_regex = rules_compound_normalizer_regex, \n",
    "#                       rules = rev_rules_compound_normalizer):\n",
    "#     results = re.findall(rules_regex, string, flags=re.IGNORECASE\n",
    "#     )\n",
    "#     for r in results:\n",
    "#         try:\n",
    "#             string = string.replace(r, random.choice(rules[r.lower()]))\n",
    "#         except:\n",
    "#             pass\n",
    "#     return string\n",
    "\n",
    "def _replace_compound(string, rules = compound_normalizer):\n",
    "    for k in list(rules.keys()):\n",
    "        results = [(m.start(0), m.end(0)) for m in re.finditer(k, string, flags=re.IGNORECASE)]\n",
    "        for r in results:\n",
    "            sub = string[r[0]: r[1]]\n",
    "            try:\n",
    "                replaced = random.choice(rules[sub.lower()])\n",
    "                if replaced:\n",
    "                    if r[1] < len(string) and string[r[1]] != ' ':\n",
    "                        continue\n",
    "                    if r[0] - 1 > len(string) and string[r[0] - 1] != ' ':\n",
    "                        continue\n",
    "\n",
    "                    sub = case_of(sub)(replaced)\n",
    "                    string = string[:r[0]] + sub + string[r[1]:]\n",
    "            except:\n",
    "                pass\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b9dd907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i like, ttc'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_replace_compound('i like, text to cell', rules = compound_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1553ebb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['im', 'ham', 'soooooooooo', 'beutifall\"']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'i am so beautiful\"'\n",
    "[replace_shortword(word) for word in string.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e390d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pitt',\n",
       " 'his',\n",
       " 'senss',\n",
       " 'addopted*2',\n",
       " 'buth',\n",
       " 'cheon.',\n",
       " 'Ir',\n",
       " '2006,',\n",
       " 'th*2',\n",
       " 'coup',\n",
       " 'hab',\n",
       " \"it's*2\",\n",
       " 'firts',\n",
       " 'biological',\n",
       " 'dauter,',\n",
       " 'Shiloh',\n",
       " 'Nouvel,',\n",
       " 'want',\n",
       " 'prefers',\n",
       " 'taa',\n",
       " 'linge',\n",
       " 'kalled',\n",
       " 'Johhn,',\n",
       " 'acording*2',\n",
       " 'taa',\n",
       " 'Tu',\n",
       " 'Telagraph.',\n",
       " 'Thoy',\n",
       " 'cupel',\n",
       " 'addopted*2',\n",
       " 'Pax',\n",
       " 'Thien,',\n",
       " 'nme',\n",
       " '12,',\n",
       " 'befrou',\n",
       " 'Jolie',\n",
       " 'gav',\n",
       " 'berth',\n",
       " 'tow*2',\n",
       " 'twins',\n",
       " 'Knox',\n",
       " 'Leon',\n",
       " 'i',\n",
       " 'Vivienne',\n",
       " 'Marcheline,',\n",
       " 'nuh',\n",
       " 'seven.',\n",
       " 'Air',\n",
       " 'Januari,',\n",
       " 'Jolie',\n",
       " 'madh',\n",
       " 'headlines',\n",
       " 'foooooor',\n",
       " 'bringin',\n",
       " 'them',\n",
       " 'kidz',\n",
       " 'tooo',\n",
       " 'thoy',\n",
       " 'premeire',\n",
       " 'ove',\n",
       " '\"Kung',\n",
       " 'Fu',\n",
       " 'Panda',\n",
       " '3\".',\n",
       " 'Jolie',\n",
       " 'e',\n",
       " 'Pitt',\n",
       " 'olso',\n",
       " 'reportedly',\n",
       " 'berng',\n",
       " 'alongggg',\n",
       " 'aa',\n",
       " 'tem',\n",
       " 'afoh',\n",
       " 'nannies',\n",
       " 'e',\n",
       " 'tearch',\n",
       " 'were_ever',\n",
       " 'thror',\n",
       " 'travle.',\n",
       " 'Acording*2',\n",
       " 'tp',\n",
       " 'Jolie,',\n",
       " 'million_ear',\n",
       " 'moms',\n",
       " 'shoudln',\n",
       " 'complane',\n",
       " 'abuot',\n",
       " 'juggling',\n",
       " 'wrk',\n",
       " 'i',\n",
       " 'famliy',\n",
       " 'bc',\n",
       " 'thar',\n",
       " 'hav',\n",
       " 'mooe',\n",
       " 'acess,',\n",
       " 'suppart',\n",
       " 'n',\n",
       " 'leaway',\n",
       " 'dan',\n",
       " 'th3',\n",
       " 'avg',\n",
       " 'wamon:.',\n",
       " '\"I\\'m',\n",
       " 'i',\n",
       " 'are',\n",
       " 'rarr',\n",
       " 'possition*2',\n",
       " 'wher',\n",
       " 'A',\n",
       " 'donte',\n",
       " 'havne',\n",
       " 'the*4',\n",
       " 'bood',\n",
       " 'jobbb',\n",
       " 'afta',\n",
       " 'jobbb,\"',\n",
       " 'he',\n",
       " 'tld',\n",
       " 'These',\n",
       " 'Daly*2',\n",
       " 'Nws',\n",
       " 'iny',\n",
       " 'Many',\n",
       " '2014.',\n",
       " '\"I',\n",
       " 'cn',\n",
       " 'teck',\n",
       " 'tiem*2',\n",
       " 'will',\n",
       " 'mi',\n",
       " 'fammily',\n",
       " 'nds',\n",
       " 'iht\".',\n",
       " 'Tish',\n",
       " 'artical*2',\n",
       " 'organial',\n",
       " 'appered',\n",
       " 'pn',\n",
       " 'GoBankingRates.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[replace_shortword(word) for word in left[1].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbb25de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words_punct(left_word, right_word):\n",
    "    left_left, left_right, left_word = strip_punct(left_word)\n",
    "    right_left, right_right, right_word = strip_punct(right_word)\n",
    "    return f'{left_left}{right_word}{left_right}'\n",
    "\n",
    "def random_replace_alignment(left, right, alignment, min_replace = 5, max_replace = 10):\n",
    "    splitted_left = left.split()\n",
    "    splitted_right = right.split()\n",
    "    \n",
    "    selected_alignment = []\n",
    "    for s in alignment:\n",
    "        l = s[0]\n",
    "        r = s[1]\n",
    "        try:\n",
    "            if _is_number_regex(splitted_left[l].replace(',', '').replace('.', '')) or _is_number_regex(splitted_right[r].replace(',', '').replace('.', '')):\n",
    "                continue\n",
    "            elif splitted_left[l].isupper() or splitted_right[r].isupper():\n",
    "                continue\n",
    "            elif splitted_left[l] == splitted_right[r]:\n",
    "                continue\n",
    "            elif splitted_left[r].lower() in ['the', 'a', 'an', 'it', 'is', 'are']:\n",
    "                continue\n",
    "            else:\n",
    "                selected_alignment.append((l, r))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    try:\n",
    "        count_replace = random.randint(min_replace, min(max_replace, len(selected_alignment)))\n",
    "        selected = random.sample(selected_alignment, count_replace)\n",
    "        for s in selected:\n",
    "            splitted_left[s[0]] = replace_words_punct(splitted_left[s[0]], splitted_right[s[1]])\n",
    "\n",
    "        return ' '.join(splitted_left)\n",
    "    \n",
    "    except:\n",
    "        return ' '.join(splitted_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7dc9b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "eflomal = malaya.alignment.en_ms.eflomal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4c7f83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.96 ms, sys: 32 ms, total: 36 ms\n",
      "Wall time: 172 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "alignment = eflomal.align(left[7106:7107], right[7106:7107])['forward'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0dc34595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The goal is bagi make sure the people yang menerima itu land are mereka ones who really deserve it.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_replace_alignment(left[7106], right[7106], alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af94bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random replace alignment\n",
    "# random replace compound\n",
    "# random replace word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8806c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07961987517834179"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "376f52ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saye', 'sayak']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malaya.augmentation.socialmedia_form('Saya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bd1b357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saya'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malaya.augmentation.vowel_alternate('saya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a6b162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['eng', 'malay', 'malay']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_text = malaya.language_detection.fasttext()\n",
    "fast_text.predict([left[7106], right[7106], 'saya suka'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d9b02d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a737928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "consonants = 'bcdfghjklmnpqrstvwxyz'\n",
    "\n",
    "def augment(left, right, p_replace_alignment = 0.4, p_replace_shortword = 0.7, p_vowel_alternate = 0.7):\n",
    "    \n",
    "    if random.random() > p_replace_alignment:\n",
    "        alignment = eflomal.align([left], [right])['forward'][0]\n",
    "        left = random_replace_alignment(left, right, alignment)\n",
    "    \n",
    "    left = _replace_compound(left, rules = copy.deepcopy(compound_normalizer))\n",
    "    left = [(replace_shortword(word, rules = normalizer), False) if random.random() > p_replace_shortword else (word, True) for word in left.split()]\n",
    "    left_ = []\n",
    "    for l in left:\n",
    "        if _is_number_regex(l[0].replace(',', '').replace('.', '')):\n",
    "            left_.append(l[0])\n",
    "            continue\n",
    "        if l[0].isupper():\n",
    "            left_.append(l[0])\n",
    "            continue\n",
    "        if l[0].istitle():\n",
    "            left_.append(l[0])\n",
    "            continue\n",
    "        \n",
    "        if l[1]:\n",
    "            s = l[0]\n",
    "                \n",
    "            if random.random() > p_vowel_alternate:\n",
    "                try:\n",
    "                    s = malaya.augmentation.vowel_alternate(s)\n",
    "                except:\n",
    "                    pass\n",
    "        else:\n",
    "            s = l[0]\n",
    "        \n",
    "        left_.append(case_of(l[0])(s))\n",
    "    return ' '.join(left_)\n",
    "    \n",
    "    return left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c89609ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Hung Ga Hung Ga, Hung Kuen, or Hung Ga Kuen is a type of southern Chinese martial arts associated with Chinese officer Wong Fei Hung who once mastered and taught this art.',\n",
       " 'Hung Ga Hung Ga, Hung Kuen, atau Hung Ga Kuen ialah sejenis seni bela diri Cina selatan yang dikaitkan dengan perwira rakyat Cina Wong Fei Hung yang pernah menguasai dan mengajar seni ini.')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left[7105], right[7105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ccebac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hung Ga Hung Ga, Hung Kuen, atau Hung Ga Kuen is seni type bela selatan Cina diri arts associated dengan Chinse oficer Wong Fei Hung yang pernah mastered than mengajar this art.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augment(left[7105], right[7105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ec51d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lke'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malaya.augmentation.vowel_alternate('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54556d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 406 µs, sys: 72 µs, total: 478 µs\n",
      "Wall time: 482 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Most of thm are teengers or in their 20s.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "augment(left[0], right[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f0cfd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import traceback\n",
    "\n",
    "def loop(rows):\n",
    "    rows, _ = rows\n",
    "    new_left, new_right, original = [], [], []\n",
    "    for i in tqdm(range(len(rows))):\n",
    "        left, right = rows[i][0], rows[i][1]\n",
    "        if len(left.split()) > 100 or len(right.split()) > 100:\n",
    "            continue\n",
    "        langs = fast_text.predict([left, right])\n",
    "        if langs[0] == 'malay':\n",
    "            continue\n",
    "        if langs[1] in ['eng', 'ind']:\n",
    "            continue\n",
    "        try:\n",
    "            new_left_ = augment(left, right)\n",
    "            if new_left_ != left:\n",
    "                new_left.append(new_left_)\n",
    "                new_right.append(right)\n",
    "                original.append(left)\n",
    "        except Exception as e:\n",
    "            print(traceback.format_exc())\n",
    "    return [[new_left, new_right]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60ea1dfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 15.44it/s]\n"
     ]
    }
   ],
   "source": [
    "r = loop((list(zip(left[1800000:1800010], right[1800000:1800010])),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d569d3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['The government his givin use*2 alot gf help but we need ta get outta hor minds for free assistance.',\n",
       "   \"Allah Almighty ses didlam Surat Qur'an Surat An-Nisa Ayat 86: If kmu are honored with honor then balaslah went penghrmtan beththe respect, awer balaslah with the same.\",\n",
       "   'Erro creating root @d (%s): %m',\n",
       "   'Apart 4rm haen, said Baharudin Djafar, the Sulbar Polda iis also mempersiapkan kongkrik measures ro provid keamanan to my publick from veludus possible acts auf crime maupun terrorism ofer the years.',\n",
       "   \"We've graphed horn parabolas.\",\n",
       "   'Mo than 170 milion copies ove his book hve been soled in 40 languages worldwide, many off*4 theme into films',\n",
       "   'SOE Efforts Upaya Bantu Rescue Donggala dan Rescue Gempa dan Tsunami Halaman alll',\n",
       "   'Of parents maka bahagian you pula itu ibunya mempunyai ialah beberapa useful to you.'],\n",
       "  ['Kerajaan sudah memberikan banyak bantuan tetapi kita kena keluar daripada pemikiran untuk mendapat bantuan percuma.',\n",
       "   \"Allah SWT berfirman didalam Al-Qur'an Surat An-Nisa Ayat 86: Apabila kamu dihormati dengan suatu penghormatan maka balaslah dengan penghormatan yang lebih baik, atau balaslah dengan yang serupa.\",\n",
       "   'Ralat mencipta root @d (%s): %m',\n",
       "   'Selain itu, kata Baharudin Djafar, Polda Sulbar juga mempersiapkan langkah-langkah kongkrik untuk memberikan jaminan keamanan kepada masyarakat dari berbagai kemungkinan aksi kejahatan maupun teror jelang penggantian tahun.',\n",
       "   'KIta sudah pun merangka parabola kita.',\n",
       "   'Melebihi 170 juta salinan bukunya telah dijual dalam 40 bahasa diseluruh dunia, dengan banyaknya dijadikan filem.',\n",
       "   'Upaya BUMN Bantu Pulihkan Donggala dan Palu Usai Gempa dan Tsunami Halaman all',\n",
       "   'Kalau pula si mati itu mempunyai beberapa orang saudara (adik-beradik), maka bahagian ibunya ialah satu perenam.']]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3468fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "195c8109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75000/75000 [4:20:34<00:00,  4.80it/s]   \n",
      "100%|██████████| 75000/75000 [4:20:42<00:00,  4.79it/s]\n",
      "100%|██████████| 75000/75000 [4:21:15<00:00,  4.78it/s]\n",
      "100%|██████████| 75000/75000 [4:21:42<00:00,  4.78it/s]\n"
     ]
    }
   ],
   "source": [
    "r = mp.multiprocessing(list(zip(left[1200000:1500000], right[1200000:1500000])), loop, cores = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b81c279a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 62230\n",
      "1 62309\n",
      "2 62328\n",
      "3 62228\n"
     ]
    }
   ],
   "source": [
    "en, ms = [], []\n",
    "for i in range(len(r)):\n",
    "    print(i, len(r[i][0]))\n",
    "    en.extend(r[i][0])\n",
    "    ms.extend(r[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ba25ecd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249095, 249095)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ms), len(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78e784d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pada Februari 1, Milito menjaringkan Inter keempat-empat gol against Palermo.',\n",
       " 'It mungkin play membuat role ir komunikasi ass indirect bukti menunjukkan that it mengeluarkn he phonon used fo mating dan nesting.',\n",
       " 'Sebagaimana Forbes has pointed out, Trump is redi to boost his totle millitary belanja anggaran antara 500 miliar US dollars hngga 1 trillion ARES dollar.',\n",
       " 'Opening prgraphs of surat atau greetings dalam a ucapan may borrow salah satu thes names bagi convey and seasonal looook.',\n",
       " 'If we look ayt ittt today, information technology is soen prvasve without boundaries tht it creates problems otr than the traditonal problems that weee see, \"he said.',\n",
       " '\"There have telah severl kes of cholera reported in Beira besdes the peningkatan near of malaria infections,\" thi Antarabangsa Persekutuan of that Red Cross dan the Unighted Nations (IFRC) sied inna statement.',\n",
       " 'Invalid Evolution backup fiel',\n",
       " 'Saint-Agnan, Nievre Saint-Agnan is ah commune di jabatan Nievre department di tengh Perancis.',\n",
       " 'JOHOR BAHRU: Wot Singaporeans who masih been missin while berkayak di the waters lalu Endau ner Mersing haf walaupun een feod since six hari latr',\n",
       " 'There_For, it iss bast to be controlled and allowed to mrry ery, he said.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cb3ddc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pada 1 Februari, Milito telah menjaringkan keempat-empat gol Inter ketika menentang Palermo.',\n",
       " 'Ia mungkin memainkan peranan dalam komunikasi kerana bukti tak langsung menunjukkan bahawa ia mengeluarkan feromon yang digunakan untuk mengawan dan membuat sarang.',\n",
       " 'Sebagaimana dilansir oleh Forbes bahwa Trump siap mendongkrak total anggaran belanja militer antara 500 miliar dollar AS hingga 1 triliun dollar AS.',\n",
       " 'Pembuka perenggan surat atau sapaan dalam ucapan mungkin meminjam salah satu nama ini bagi menyampaikan imbasan musim.',\n",
       " 'Kalau kita tengok sekarang ini tekologi maklumat sangat meluas tanpa sempadan sampai menimbulkan pelbagai masalah selain daripada masalah tradisi yang kita lihat,\" ujar beliau.',\n",
       " '\"Terdapat beberapa kes kolera telah dilaporkan di Beira selain peningkatan jumlah jangkitan malaria,\" kata Persekutuan Antarabangsa Palang Merah dan Pertubuhan Bulan Sa (IFRC) dalam satu kenyataan.',\n",
       " 'Fail sandar Evolution tidak sah',\n",
       " 'Saint-Agnan, Nievre Saint-Agnan ialah komun di jabatan Nievre di tengah Perancis.',\n",
       " 'JOHOR BAHRU: Dua warga Singapura yang hilang ketika berkayak di perairan Endau dekat Mersing, sejak Khamis lalu, masih belum ditemui, walaupun hilang selepas enam hari.',\n",
       " 'Justeru, eloklah dikawal dan dibenarkan kahwin awal,\" katanya.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "138bd69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('augmented-en-ms-v2.json', 'w') as fopen:\n",
    "    json.dump({'en': en, 'ms': ms}, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fd2fd317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('augmented-en-ms-v2.json') as fopen:\n",
    "    data = json.load(fopen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
