{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b8fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/train-en-ms.tar.gz\n",
    "# !tar -zxf train-en-ms.tar.gz\n",
    "# !rm train-en-ms.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4229a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pyenchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b79e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import enchant\n",
    "# d = enchant.Dict(\"en_US\")\n",
    "# d.check(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "115c2ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/huseinzol05/malay-dataset/master/normalization/en-lexicon/en-social-media-lexicon.json\n",
    "# !wget https://raw.githubusercontent.com/huseinzol05/malay-dataset/master/normalization/en-lexicon/en-social-media-lexicon-v2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51eb22f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe86564f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'train-en': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lha train-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0741e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/husein/translation/zsm_Latn.dev') as fopen:\n",
    "    right = fopen.read().split('\\n')\n",
    "    \n",
    "with open('/home/husein/translation/eng_Latn.dev') as fopen:\n",
    "    left = fopen.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9272db3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.8/site-packages/malaya/tokenizer.py:202: FutureWarning: Possible nested set at position 3361\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n",
      "/home/husein/.local/lib/python3.8/site-packages/malaya/tokenizer.py:202: FutureWarning: Possible nested set at position 3879\n",
      "  self.tok = re.compile(r'({})'.format('|'.join(pipeline)))\n"
     ]
    }
   ],
   "source": [
    "import malaya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aa318d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from malaya.text.rules import rules_normalizer, rules_compound_normalizer\n",
    "from malaya.text.normalization import _is_number_regex\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ff9548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('en-social-media-lexicon.json') as fopen:\n",
    "    en_lexicon = json.load(fopen)\n",
    "    \n",
    "with open('en-social-media-lexicon-v2.json') as fopen:\n",
    "    en_lexicon_v2 = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d284466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_normalizer = defaultdict(list)\n",
    "normalizer = defaultdict(list)\n",
    "\n",
    "for k, v in en_lexicon.items():\n",
    "    if not len(v):\n",
    "        continue\n",
    "    if len(k.split()) > 1:\n",
    "        compound_normalizer[k].extend(v)\n",
    "    else:\n",
    "        normalizer[k].extend(v)\n",
    "\n",
    "for k, v in en_lexicon_v2.items():\n",
    "    if not len(v):\n",
    "        continue\n",
    "    if len(k.split()) > 1:\n",
    "        compound_normalizer[k].extend(v)\n",
    "    else:\n",
    "        normalizer[k].extend(v)\n",
    "        \n",
    "compound_normalizer_regex = (\n",
    "    '(?:' + '|'.join(list(compound_normalizer.keys())) + ')'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bad7638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', ',', 'counters')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PUNCTUATION = '!\"#$%&\\'()*+,./:;<=>?@[\\]^_`{|}~'\n",
    "\n",
    "def case_of(text):\n",
    "    return (\n",
    "        str.upper\n",
    "        if text.isupper()\n",
    "        else str.lower\n",
    "        if text.islower()\n",
    "        else str.title\n",
    "        if text.istitle()\n",
    "        else str\n",
    "    )\n",
    "\n",
    "def strip_punct(word):\n",
    "    left = []\n",
    "    right = []\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        if word[i] in PUNCTUATION:\n",
    "            left.append(word[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            break\n",
    "    i = len(word) - 1\n",
    "    while i > 0:\n",
    "        if word[i] in PUNCTUATION:\n",
    "            right.append(word[i])\n",
    "            i -= 1\n",
    "        else:\n",
    "            break\n",
    "    left = ''.join(left)\n",
    "    right = ''.join(right[::-1])\n",
    "    if len(right):\n",
    "        word_ = word[:-len(right)]\n",
    "    else:\n",
    "        word_ = word\n",
    "    word_ = word_[len(left):]\n",
    "    return left, right, word_\n",
    "\n",
    "\n",
    "def replace_shortword(word, rules = normalizer):\n",
    "    left, right, word_ = strip_punct(word)\n",
    "    word_ = word_[len(left):]\n",
    "    lower_word_ = word_.lower()\n",
    "    if lower_word_ in rules:\n",
    "        word_ = case_of(word_)(random.choice(rules[lower_word_]))\n",
    "        word_ = f'{left}{word_}{right}'\n",
    "        return word_\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "strip_punct('counters,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "504d5d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _replace_compound(string, \n",
    "#                       rules_regex = rules_compound_normalizer_regex, \n",
    "#                       rules = rev_rules_compound_normalizer):\n",
    "#     results = re.findall(rules_regex, string, flags=re.IGNORECASE\n",
    "#     )\n",
    "#     for r in results:\n",
    "#         try:\n",
    "#             string = string.replace(r, random.choice(rules[r.lower()]))\n",
    "#         except:\n",
    "#             pass\n",
    "#     return string\n",
    "\n",
    "def _replace_compound(string, rules = compound_normalizer):\n",
    "    for k in list(rules.keys()):\n",
    "        results = [(m.start(0), m.end(0)) for m in re.finditer(k, string, flags=re.IGNORECASE)]\n",
    "        for r in results:\n",
    "            sub = string[r[0]: r[1]]\n",
    "            try:\n",
    "                replaced = random.choice(rules[sub.lower()])\n",
    "                if replaced:\n",
    "                    if r[1] < len(string) and string[r[1]] != ' ':\n",
    "                        continue\n",
    "                    if r[0] - 1 > len(string) and string[r[0] - 1] != ' ':\n",
    "                        continue\n",
    "\n",
    "                    sub = case_of(sub)(replaced)\n",
    "                    string = string[:r[0]] + sub + string[r[1]:]\n",
    "            except:\n",
    "                pass\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b9dd907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i like, ttc'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_replace_compound('i like, text to cell', rules = compound_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1553ebb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['im', 'amt', 'som', 'buitfull\"']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'i am so beautiful\"'\n",
    "[replace_shortword(word) for word in string.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e390d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Leed',\n",
       " 'rea',\n",
       " 'sayy',\n",
       " 'thia',\n",
       " 'many',\n",
       " 'bing',\n",
       " 'erly',\n",
       " 'detection',\n",
       " 'af',\n",
       " 'cancer,',\n",
       " 'tuberculosis,',\n",
       " 'HIV',\n",
       " 'aand',\n",
       " 'malaria',\n",
       " 'qu',\n",
       " 'patiants',\n",
       " 'i',\n",
       " 'low-income',\n",
       " 'contries,',\n",
       " 'whereeee',\n",
       " 'dah',\n",
       " 'survival',\n",
       " 'rates',\n",
       " 'fur',\n",
       " 'illnesses',\n",
       " 'sach',\n",
       " 'saw',\n",
       " 'brest',\n",
       " 'cancer',\n",
       " 'came*2',\n",
       " 'de',\n",
       " 'haff',\n",
       " 'thsoe',\n",
       " 'afoh',\n",
       " 'recher',\n",
       " 'contries.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[replace_shortword(word) for word in left[1].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbb25de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words_punct(left_word, right_word):\n",
    "    left_left, left_right, left_word = strip_punct(left_word)\n",
    "    right_left, right_right, right_word = strip_punct(right_word)\n",
    "    return f'{left_left}{right_word}{left_right}'\n",
    "\n",
    "def random_replace_alignment(left, right, alignment, min_replace = 5, max_replace = 10):\n",
    "    splitted_left = left.split()\n",
    "    splitted_right = right.split()\n",
    "    \n",
    "    selected_alignment = []\n",
    "    for s in alignment:\n",
    "        l = s[0]\n",
    "        r = s[1]\n",
    "        try:\n",
    "            if _is_number_regex(splitted_left[l].replace(',', '').replace('.', '')) or _is_number_regex(splitted_right[r].replace(',', '').replace('.', '')):\n",
    "                continue\n",
    "            elif splitted_left[l].isupper() or splitted_right[r].isupper():\n",
    "                continue\n",
    "            elif splitted_left[l] == splitted_right[r]:\n",
    "                continue\n",
    "            elif splitted_left[r].lower() in ['the', 'a', 'an', 'it', 'is', 'are']:\n",
    "                continue\n",
    "            else:\n",
    "                selected_alignment.append((l, r))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    try:\n",
    "        count_replace = random.randint(min_replace, min(max_replace, len(selected_alignment)))\n",
    "        selected = random.sample(selected_alignment, count_replace)\n",
    "        for s in selected:\n",
    "            splitted_left[s[0]] = replace_words_punct(splitted_left[s[0]], splitted_right[s[1]])\n",
    "\n",
    "        return ' '.join(splitted_left)\n",
    "    \n",
    "    except:\n",
    "        return ' '.join(splitted_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dc9b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "eflomal = malaya.alignment.en_ms.eflomal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af94bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random replace alignment\n",
    "# random replace compound\n",
    "# random replace word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8806c09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05309057329529965"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "376f52ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Saye', 'Sayak']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malaya.augmentation.socialmedia_form('Saya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bd1b357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saya'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malaya.augmentation.vowel_alternate('saya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a6b162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "fast_text = malaya.language_detection.fasttext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d9b02d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a737928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "consonants = 'bcdfghjklmnpqrstvwxyz'\n",
    "\n",
    "def augment(left, right, p_replace_alignment = 0.4, p_replace_shortword = 0.7, p_vowel_alternate = 0.7):\n",
    "    \n",
    "    if random.random() > p_replace_alignment:\n",
    "        alignment = eflomal.align([left], [right])['forward'][0]\n",
    "        left = random_replace_alignment(left, right, alignment)\n",
    "    \n",
    "    left = _replace_compound(left, rules = copy.deepcopy(compound_normalizer))\n",
    "    left = [(replace_shortword(word, rules = normalizer), False) if random.random() > p_replace_shortword else (word, True) for word in left.split()]\n",
    "    left_ = []\n",
    "    for l in left:\n",
    "        if _is_number_regex(l[0].replace(',', '').replace('.', '')):\n",
    "            left_.append(l[0])\n",
    "            continue\n",
    "        if l[0].isupper():\n",
    "            left_.append(l[0])\n",
    "            continue\n",
    "        if l[0].istitle():\n",
    "            left_.append(l[0])\n",
    "            continue\n",
    "        \n",
    "        if l[1]:\n",
    "            s = l[0]\n",
    "                \n",
    "            if random.random() > p_vowel_alternate:\n",
    "                try:\n",
    "                    s = malaya.augmentation.vowel_alternate(s)\n",
    "                except:\n",
    "                    pass\n",
    "        else:\n",
    "            s = l[0]\n",
    "        \n",
    "        left_.append(case_of(l[0])(s))\n",
    "    return ' '.join(left_)\n",
    "    \n",
    "    return left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ec51d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'like'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malaya.augmentation.vowel_alternate('like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54556d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.12 ms, sys: 203 µs, total: 1.32 ms\n",
      "Wall time: 1.32 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'On Mon, scc fraom thee Stanford Universit Shcoole of Medacine announced the invention of a new diagnostic tool that kan sort cells by type: a tiney printable chip that ca be manufactured using standard inkjet printers foow possibly about one U.S. cnt each.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "augment(left[0], right[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 998/998 [01:46<00:00,  9.40it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "new_left, new_right = [], []\n",
    "for i in tqdm(range(len(left))):\n",
    "    if len(left[i].split()) > 100 or len(right[i].split()) > 100:\n",
    "        continue\n",
    "    langs = fast_text.predict([left[i], right[i]])\n",
    "    if langs[0] == 'malay':\n",
    "        continue\n",
    "    if langs[1] in ['eng', 'ind']:\n",
    "        continue\n",
    "    try:\n",
    "        new_left_ = augment(left[i], right[i])\n",
    "        if new_left_ != left[i]:\n",
    "            new_left.append(new_left_)\n",
    "            new_right.append(right[i])\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(949, 949)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_left), len(new_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "138bd69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('augmented-eng_Latn-zsm_Latn-noisy-en-ms-dev-set-v2.json', 'w') as fopen:\n",
    "    json.dump({'ms': new_right, 'en': new_left}, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
