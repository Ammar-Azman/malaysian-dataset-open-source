{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "q_E4fFtFoOpc"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "G1cV01asptpt"
      },
      "outputs": [],
      "source": [
        "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36',\n",
        "           'Referer':'https://aliffchannel.com/'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "dameb1XOpzVZ"
      },
      "outputs": [],
      "source": [
        "#under topic malaysia, ada dua page je haha\n",
        "url = ['https://tamil.goodreturns.in/topic/malaysia', 'https://tamil.goodreturns.in/topic/malaysia/?page-no=2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3PbS7i5wPbJ",
        "outputId": "eba54631-66fe-4529-cd0f-8cfdcb9019fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:01<00:00,  1.82it/s]\n"
          ]
        }
      ],
      "source": [
        "#get links\n",
        "links = []\n",
        "for a in tqdm(url):\n",
        "\n",
        "  r = requests.get(a, headers=headers)\n",
        "  soup = BeautifulSoup(r.text, \"html\")\n",
        "\n",
        "  #ada dua class yang simpan links article\n",
        "  links_1 = soup.find_all('div', attrs={\"class\":\"tagsLeftDesc\"})\n",
        "  links_2 = soup.find_all('div', attrs={\"class\":\"tagsDescMain\"})\n",
        "\n",
        "  for x in range(len(links_1)):\n",
        "    links.append(links_1[x].a['href'])\n",
        "\n",
        "  for x in range(len(links_2)):\n",
        "    links.append(links_2[x].a['href'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBY2Q337xyN3",
        "outputId": "a142fd03-6f36-43c1-e7f6-4decd1cf014b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no of links: 24\n"
          ]
        }
      ],
      "source": [
        "print('no of links:', len(links))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "TtsDS3G16WH0"
      },
      "outputs": [],
      "source": [
        "#every content ada unwanted string kat last index\n",
        "def scrape_content(soup):\n",
        "  texts = []\n",
        "  body = soup.find_all('p')\n",
        "  for x in body:\n",
        "    texts.append(x.get_text())\n",
        "  del texts[-1]\n",
        "  combine_string = \"\".join(texts)\n",
        "\n",
        "  return combine_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7eIIyZu7cxa",
        "outputId": "432c933e-050d-46b7-a266-08c703ebfb1c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24/24 [00:12<00:00,  1.89it/s]\n"
          ]
        }
      ],
      "source": [
        "with open('tamilgoodreturns.jsonl', 'a') as f:\n",
        "    for a in tqdm(links):\n",
        "        #try:\n",
        "            url = 'https://tamil.goodreturns.in' + a\n",
        "            r = requests.get(url, headers=headers)\n",
        "            soup = BeautifulSoup(r.text, \"html\")\n",
        "            body = scrape_content(soup)\n",
        "            data = {\n",
        "                'url': url,\n",
        "                'title': soup.find('h1').get_text(),\n",
        "                'body': body,\n",
        "            }\n",
        "            json.dump(data, f)\n",
        "            f.write('\\n')\n",
        "        #except:\n",
        "            pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
