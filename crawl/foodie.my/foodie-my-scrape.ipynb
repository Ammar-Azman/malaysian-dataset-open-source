{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping pages: 100%|██████████| 4/4 [00:30<00:00,  7.55s/it]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "visited_urls = set()\n",
    "\n",
    "\n",
    "def extract_text(element):\n",
    "    if element.name == \"p\" or (\n",
    "        element.name == \"h2\" and \"wp-block-heading\" in element.get(\"class\", [])\n",
    "    ):\n",
    "        return element.text.replace(\"\\n\", \" \").strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "total_pages = 4\n",
    "\n",
    "for x in tqdm(range(1, total_pages + 1), desc=\"Scraping pages\"):\n",
    "    response = requests.get(f\"https://foodie.my/food/page/{x}\")\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    entries = soup.find_all(\"h3\", attrs={\"class\": \"post-title\"})\n",
    "\n",
    "    for entry in tqdm(entries, desc=f\"Scraping entries on page {x}\", leave=False):\n",
    "        blog_url = entry.find(\"a\").get(\"href\")\n",
    "\n",
    "        # Check if the URL has already been processed\n",
    "        if blog_url not in visited_urls:\n",
    "            blog_title = entry.text.strip()\n",
    "\n",
    "            page = requests.get(blog_url)\n",
    "            blog_soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "            body = blog_soup.find(\n",
    "                \"div\", attrs={\"class\": \"entry-content entry clearfix\"}\n",
    "            )\n",
    "\n",
    "            texts = [extract_text(tag) for tag in body.find_all([\"p\", \"h2\"])]\n",
    "            content_text = \"\\n\".join(filter(None, texts))\n",
    "\n",
    "            data.append({\"url\": blog_url, \"title\": blog_title, \"body\": content_text})\n",
    "\n",
    "            # Add the URL to the set to mark it as processed\n",
    "            visited_urls.add(blog_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"foodie-my-food.json\", \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # open foodie-my-food.json as csv\n",
    "# df = pd.read_json(\"foodie-my-food.json\")\n",
    "# df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
