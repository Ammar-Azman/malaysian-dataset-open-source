{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests_html import HTMLSession, AsyncHTMLSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected = [\n",
    "    'wa.link',\n",
    "    'mailto:',\n",
    "    't.me',\n",
    "    'tally.so',\n",
    "    'discord.gg',\n",
    "    'arxiv.org',\n",
    "    'javascript:'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_html(url):\n",
    "    session = AsyncHTMLSession(verify = False)\n",
    "    result = None\n",
    "    try:\n",
    "        \n",
    "        r = await session.get(url, verify = False, timeout = 10)\n",
    "        await r.html.arender(timeout = 10)\n",
    "        \n",
    "        result = r.html.html\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    await session.close()\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_links(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        links.append(link.get('href'))\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "visited_pages = set(['https://www.mohr.gov.my/index.php/ms/'])\n",
    "\n",
    "async def crawl(base_url):\n",
    "    if base_url.endswith('.pdf'):\n",
    "        return\n",
    "    \n",
    "    html = await get_html(base_url)\n",
    "    \n",
    "    if html is None:\n",
    "        return\n",
    "\n",
    "    visited_pages.add(base_url)\n",
    "    links = get_links(html)\n",
    "    for link in links:\n",
    "        try:\n",
    "            if any([r in link for r in rejected]):\n",
    "                continue\n",
    "\n",
    "            if not link.startswith('http'):\n",
    "                link = urljoin(base_url, link)\n",
    "\n",
    "            link = link.split('#')[0]\n",
    "            link = link.split('?')[0]\n",
    "\n",
    "            if link not in visited_pages:\n",
    "                visited_pages.add(link)\n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:02<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:59<00:00,  5.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████▍                       | 115/271 [13:55<18:27,  7.10s/it]Future exception was never retrieved\n",
      "future: <Future finished exception=NetworkError('Protocol error Target.sendMessageToTarget: Target closed.')>\n",
      "pyppeteer.errors.NetworkError: Protocol error Target.sendMessageToTarget: Target closed.\n",
      " 66%|███████████████████████████▏             | 180/271 [22:11<08:03,  5.32s/it]Future exception was never retrieved\n",
      "future: <Future finished exception=NetworkError('Protocol error (Target.sendMessageToTarget): No session with given id')>\n",
      "pyppeteer.errors.NetworkError: Protocol error (Target.sendMessageToTarget): No session with given id\n",
      " 90%|████████████████████████████████████▉    | 244/271 [32:16<04:09,  9.24s/it]Future exception was never retrieved\n",
      "future: <Future finished exception=NetworkError('Protocol error Target.detachFromTarget: Target closed.')>\n",
      "pyppeteer.errors.NetworkError: Protocol error Target.detachFromTarget: Target closed.\n",
      "Future exception was never retrieved\n",
      "future: <Future finished exception=NetworkError('Protocol error (Target.sendMessageToTarget): No session with given id')>\n",
      "pyppeteer.errors.NetworkError: Protocol error (Target.sendMessageToTarget): No session with given id\n",
      "100%|█████████████████████████████████████████| 271/271 [37:15<00:00,  8.25s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "for no in range(3):\n",
    "    l = list(visited_pages)\n",
    "    print(no, len(l))\n",
    "    for i in tqdm(range(0, len(l), batch_size)):\n",
    "        b = l[i: i + batch_size]\n",
    "        await asyncio.gather(*[crawl(url) for url in b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('url-mohr.gov.my.json', 'w') as fopen:\n",
    "    json.dump(list(visited_pages), fopen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
