{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6597867"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../language-detection/dumping-twitter-6-july-2019.json') as fopen:\n",
    "    twitter = json.load(fopen)\n",
    "    \n",
    "len(twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../language-detection/2020-02-22-twitter-dump-in.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1617795/1617795 [00:01<00:00, 970771.39it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../language-detection/2020-03-08-twitter-dump-in.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1711555/1711555 [00:01<00:00, 967862.72it/s]\n"
     ]
    }
   ],
   "source": [
    "files = ['2020-02-22-twitter-dump-in.json', '2020-03-08-twitter-dump-in.json']\n",
    "\n",
    "for file in files:\n",
    "    file = f'../language-detection/{file}'\n",
    "    print(file)\n",
    "\n",
    "    with open(file) as fopen:\n",
    "        temp = json.load(fopen)\n",
    "\n",
    "    for i in tqdm(range(len(temp))):\n",
    "        retweet = temp[i]['retweet_text_full']\n",
    "        t = temp[i]['data_text']\n",
    "        if retweet != 'NULL' and len(retweet) > len(t):\n",
    "            t = retweet\n",
    "        twitter.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9927217"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessing(string):\n",
    "    string = re.sub(\n",
    "        'http\\S+|www.\\S+',\n",
    "        '',\n",
    "        ' '.join(\n",
    "            [i for i in string.split() if i.find('#') < 0 and i.find('@') < 0]\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    chars = ',.()!:\\'\"/;=-'\n",
    "    for c in chars:\n",
    "        string = string.replace(c, f' {c} ')\n",
    "        \n",
    "    string = re.sub(\n",
    "        u'[0-9!@#$%^&*()_\\-+{}|\\~`\\'\";:?/.>,<]',\n",
    "        ' ',\n",
    "        string,\n",
    "        flags = re.UNICODE,\n",
    "    )\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    \n",
    "    return string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9927217/9927217 [03:27<00:00, 47956.30it/s]\n"
     ]
    }
   ],
   "source": [
    "cleaned = []\n",
    "\n",
    "for i in tqdm(range(len(twitter))):\n",
    "    c = preprocessing(twitter[i])\n",
    "    cleaned.append((twitter[i], c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "negate_words = {\n",
    "    'tak',\n",
    "    'jangan',\n",
    "    'tidak',\n",
    "    'enggak',\n",
    "    'tiada',\n",
    "    'bukan',\n",
    "    'usah',\n",
    "    'tidaklah',\n",
    "    'jgn',\n",
    "    'tk',\n",
    "    'bkn',\n",
    "    \"shouldnt\",\n",
    "    \"dont\",\n",
    "    \"doesnt\",\n",
    "}\n",
    "\n",
    "import itertools\n",
    "\n",
    "def _pad_sequence(\n",
    "    sequence,\n",
    "    n,\n",
    "    pad_left = False,\n",
    "    pad_right = False,\n",
    "    left_pad_symbol = None,\n",
    "    right_pad_symbol = None,\n",
    "):\n",
    "    sequence = iter(sequence)\n",
    "    if pad_left:\n",
    "        sequence = itertools.chain((left_pad_symbol,) * (n - 1), sequence)\n",
    "    if pad_right:\n",
    "        sequence = itertools.chain(sequence, (right_pad_symbol,) * (n - 1))\n",
    "    return sequence\n",
    "\n",
    "def ngrams(\n",
    "    sequence,\n",
    "    n: int,\n",
    "    pad_left = False,\n",
    "    pad_right = False,\n",
    "    left_pad_symbol = None,\n",
    "    right_pad_symbol = None,\n",
    "):\n",
    "    sequence = _pad_sequence(\n",
    "        sequence, n, pad_left, pad_right, left_pad_symbol, right_pad_symbol\n",
    "    )\n",
    "\n",
    "    history = []\n",
    "    while n > 1:\n",
    "        try:\n",
    "            next_item = next(sequence)\n",
    "        except StopIteration:\n",
    "            return\n",
    "        history.append(next_item)\n",
    "        n -= 1\n",
    "    for item in sequence:\n",
    "        history.append(item)\n",
    "        yield tuple(history)\n",
    "        del history[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('populate-results.json') as fopen:\n",
    "    l = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2260, 2922)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, p = [], []\n",
    "\n",
    "for k, v in l.items():\n",
    "    if v == 'negative':\n",
    "        n.append(k)\n",
    "    else:\n",
    "        p.append(k)\n",
    "        \n",
    "len(n), len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives, shouldnot = n[:], []\n",
    "for w in negate_words:\n",
    "    for w_ in p:\n",
    "        if w_ in ['sabar']:\n",
    "            continue\n",
    "        negatives.extend([f'{w}{w_}', f'{w} {w_}'])\n",
    "    for w_ in n:\n",
    "        shouldnot.extend([f'{w}{w_}', f'{w} {w_}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives_words = set(negatives)\n",
    "shouldnot_words = set(shouldnot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "laughing = {\n",
    "    'huhu',\n",
    "    'haha',\n",
    "    'gagaga',\n",
    "    'hihi',\n",
    "    'wkawka',\n",
    "    'wkwk',\n",
    "    'kiki',\n",
    "    'keke',\n",
    "    'huehue',\n",
    "    'hshs',\n",
    "    'zzz',\n",
    "    'hoho',\n",
    "    'hew',\n",
    "    'uwu',\n",
    "    'ooo',\n",
    "    'sksk',\n",
    "    'ksks',\n",
    "    'gitu',\n",
    "    'meow',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "reject = ['sukanya', 'tolong', 'bulan', 'bau', 'pukul', 'handai', 'abis', 'visi',\n",
    "'hiburkan', 'kek', 'depa', 'senyum', 'ulama', 'habis', 'bener', 'dipilih',\n",
    "'suami', 'penyertaan', 'teknikal', 'jam', 'mencapai', 'kekuatan', 'hepi']\n",
    "\n",
    "negatives_words = {w for w in negatives_words if w not in reject}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9927217/9927217 [05:20<00:00, 31006.07it/s]\n"
     ]
    }
   ],
   "source": [
    "ns, found = [], []\n",
    "for s in tqdm(cleaned):\n",
    "    splitted = s[1].split()\n",
    "    ngs = splitted[:]\n",
    "    ngs.extend([' '.join(n) for n in ngrams(splitted, 2)])\n",
    "    r = set(ngs) & negatives_words\n",
    "    sn = set(ngs) & shouldnot_words\n",
    "    string = [\n",
    "        word\n",
    "        for word in splitted\n",
    "        if any([laugh in word for laugh in laughing])\n",
    "    ]\n",
    "    if len(r) and not len(sn) and not len(string):\n",
    "        found.extend(list(r))\n",
    "        ns.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3103071, 0.3125821667845077)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ns), len(ns) / len(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ns.json', 'w') as fopen:\n",
    "    json.dump(ns, fopen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
