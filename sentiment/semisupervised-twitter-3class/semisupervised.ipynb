{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepro_utils import preprocess_text, encode_ids, encode_pieces\n",
    "\n",
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.Load('sp10m.cased.bert.model')\n",
    "\n",
    "with open('sp10m.cased.bert.vocab') as fopen:\n",
    "    v = fopen.read().split('\\n')[:-1]\n",
    "v = [i.split('\\t') for i in v]\n",
    "v = {i[0]: i[1] for i in v}\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, v):\n",
    "        self.vocab = v\n",
    "        pass\n",
    "    \n",
    "    def tokenize(self, string):\n",
    "        return encode_pieces(sp_model, string, return_unicode=False, sample=False)\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [sp_model.PieceToId(piece) for piece in tokens]\n",
    "    \n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return [sp_model.IdToPiece(i) for i in ids]\n",
    "    \n",
    "tokenizer = Tokenizer(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.Load('sp10m.cased.v9.model')\n",
    "\n",
    "def tokenize_fn(text):\n",
    "    text = preprocess_text(text, lower= False)\n",
    "    return encode_ids(sp_model, text)\n",
    "\n",
    "SEG_ID_A   = 0\n",
    "SEG_ID_B   = 1\n",
    "SEG_ID_CLS = 2\n",
    "SEG_ID_SEP = 3\n",
    "SEG_ID_PAD = 4\n",
    "\n",
    "special_symbols = {\n",
    "    \"<unk>\"  : 0,\n",
    "    \"<s>\"    : 1,\n",
    "    \"</s>\"   : 2,\n",
    "    \"<cls>\"  : 3,\n",
    "    \"<sep>\"  : 4,\n",
    "    \"<pad>\"  : 5,\n",
    "    \"<mask>\" : 6,\n",
    "    \"<eod>\"  : 7,\n",
    "    \"<eop>\"  : 8,\n",
    "}\n",
    "\n",
    "VOCAB_SIZE = 32000\n",
    "UNK_ID = special_symbols[\"<unk>\"]\n",
    "CLS_ID = special_symbols[\"<cls>\"]\n",
    "SEP_ID = special_symbols[\"<sep>\"]\n",
    "MASK_ID = special_symbols[\"<mask>\"]\n",
    "EOD_ID = special_symbols[\"<eod>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from malaya.text.rules import normalized_chars\n",
    "from unidecode import unidecode\n",
    "import random\n",
    "\n",
    "laughing = {\n",
    "    'huhu',\n",
    "    'haha',\n",
    "    'gagaga',\n",
    "    'hihi',\n",
    "    'wkawka',\n",
    "    'wkwk',\n",
    "    'kiki',\n",
    "    'keke',\n",
    "    'huehue',\n",
    "    'hshs',\n",
    "    'hoho',\n",
    "    'hewhew',\n",
    "    'uwu',\n",
    "    'sksk',\n",
    "    'ksks',\n",
    "    'gituu',\n",
    "    'gitu',\n",
    "    'mmeeooww',\n",
    "    'meow',\n",
    "    'alhamdulillah',\n",
    "    'muah',\n",
    "    'mmuahh',\n",
    "    'hehe',\n",
    "    'salamramadhan',\n",
    "    'happywomensday',\n",
    "    'jahagaha',\n",
    "    'ahakss',\n",
    "    'ahksk'\n",
    "}\n",
    "\n",
    "def make_cleaning(s, c_dict):\n",
    "    s = s.translate(c_dict)\n",
    "    return s\n",
    "\n",
    "def cleaning(string):\n",
    "    \"\"\"\n",
    "    use by any transformer model before tokenization\n",
    "    \"\"\"\n",
    "    string = unidecode(string)\n",
    "    \n",
    "    string = ' '.join(\n",
    "        [make_cleaning(w, normalized_chars) for w in string.split()]\n",
    "    )\n",
    "    string = re.sub('\\(dot\\)', '.', string)\n",
    "    string = (\n",
    "        re.sub(re.findall(r'\\<a(.*?)\\>', string)[0], '', string)\n",
    "        if (len(re.findall(r'\\<a (.*?)\\>', string)) > 0)\n",
    "        and ('href' in re.findall(r'\\<a (.*?)\\>', string)[0])\n",
    "        else string\n",
    "    )\n",
    "    string = re.sub(\n",
    "        r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', ' ', string\n",
    "    )\n",
    "    \n",
    "    chars = '.,/'\n",
    "    for c in chars:\n",
    "        string = string.replace(c, f' {c} ')\n",
    "        \n",
    "    string = re.sub(r'[ ]+', ' ', string).strip().split()\n",
    "    string = [w for w in string if w[0] != '@']\n",
    "    x = []\n",
    "    for word in string:\n",
    "        if any([laugh in word for laugh in laughing]):\n",
    "            if random.random() >= 0.5:\n",
    "                x.append(word)\n",
    "        else:\n",
    "            x.append(word)\n",
    "    string = [w.title() if w[0].isupper() else w for w in x]\n",
    "    return ' '.join(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hallo'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning('hallo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = load_graph('bert-base-sentiment/frozen_model.pb')\n",
    "x = g.get_tensor_by_name('import/Placeholder:0')\n",
    "mask = g.get_tensor_by_name('import/Placeholder_1:0')\n",
    "logits = tf.nn.softmax(g.get_tensor_by_name('import/logits:0'))\n",
    "test_sess = tf.InteractiveSession(graph = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_xlnet = load_graph('xlnet-base-sentiment/frozen_model.pb')\n",
    "x_xlnet = g_xlnet.get_tensor_by_name('import/Placeholder:0')\n",
    "seg_xlnet = g_xlnet.get_tensor_by_name('import/Placeholder_1:0')\n",
    "m_xlnet = g_xlnet.get_tensor_by_name('import/Placeholder_2:0')\n",
    "logits_xlnet = tf.nn.softmax(g_xlnet.get_tensor_by_name('import/logits:0'))\n",
    "test_sess_xlnet = tf.InteractiveSession(graph = g_xlnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_ids(text):\n",
    "    tokens_a = tokenizer.tokenize(text)\n",
    "    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "    input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_id)\n",
    "    return input_id, input_mask\n",
    "\n",
    "input_id, input_mask = str_to_ids('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XY(left_train):\n",
    "    X, segments, masks = [], [], []\n",
    "    for i in range(len(left_train)):\n",
    "        tokens_a = tokenize_fn(left_train[i])\n",
    "        segment_id = [SEG_ID_A] * len(tokens_a)\n",
    "        tokens_a.append(SEP_ID)\n",
    "        tokens_a.append(CLS_ID)\n",
    "        segment_id.append(SEG_ID_A)\n",
    "        segment_id.append(SEG_ID_CLS)\n",
    "        input_mask = [0] * len(tokens_a)\n",
    "        X.append(tokens_a)\n",
    "        segments.append(segment_id)\n",
    "        masks.append(input_mask)\n",
    "    return X, segments, masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Negative', 'Neutral', 'Positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18384206, 0.81245416, 0.0037037 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = test_sess.run(logits, feed_dict = {x: [input_id], mask: [input_mask]})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id, input_segment, input_mask = XY(['hello'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.1995971e-02, 9.7793204e-01, 7.2078059e-05]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = test_sess_xlnet.run(logits_xlnet, feed_dict = {x_xlnet: input_id, m_xlnet: input_mask,\n",
    "                                                       seg_xlnet: input_segment})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "twitter = []\n",
    "for file in glob('*twitter-filtered.txt'):\n",
    "    with open(file) as fopen:\n",
    "        twitter.extend(fopen.read().split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400014"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30707"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('politics.txt') as fopen:\n",
    "    politics = fopen.read().split('\\n')\n",
    "    \n",
    "len(politics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "batch_size = 10\n",
    "pad_sequences = tf.keras.preprocessing.sequence.pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40002/40002 [38:01<00:00, 17.54it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = []\n",
    "for i in tqdm(range(0, len(twitter), batch_size)):\n",
    "    train_ids, input_masks, accepted, cleaned = [], [], [], []\n",
    "    for t in twitter[i: i + batch_size]:\n",
    "        t_ = cleaning(t)\n",
    "        if len(t_):\n",
    "            input_id, input_mask = str_to_ids(t_)\n",
    "            train_ids.append(input_id)\n",
    "            input_masks.append(input_mask)\n",
    "            accepted.append(t)\n",
    "            cleaned.append(t_)\n",
    "    train_ids = pad_sequences(train_ids, padding='post')\n",
    "    input_masks = pad_sequences(input_masks, padding='post')\n",
    "    result = test_sess.run(logits, feed_dict = {x: train_ids, mask: input_masks})\n",
    "    \n",
    "    input_id, input_segment, input_mask = XY(cleaned)\n",
    "    input_id = pad_sequences(input_id, padding='post')\n",
    "    input_mask = pad_sequences(input_mask, padding='post', value = 1)\n",
    "    input_segment = pad_sequences(input_segment, padding='post', value = 4)\n",
    "    result_xlnet = test_sess_xlnet.run(logits_xlnet, feed_dict = {x_xlnet: input_id, m_xlnet: input_mask,\n",
    "                                                       seg_xlnet: input_segment})\n",
    "    result = (result + result_xlnet) / 2\n",
    "    for no, row in enumerate(result):\n",
    "        if len(row[row > 0.85]):\n",
    "            a = np.argmax(row)\n",
    "            results.append({'text': accepted[no], 'label': labels[a], 'prob': row[a]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185787"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': '@JakeSimRawr kapit jake shhsgsddhasdgahga',\n",
       "  'label': 'Neutral',\n",
       "  'prob': 0.999179},\n",
       " {'text': 'Pelanjutan Perintah Kawalan Pergerakan Diperketatkan (PKPD) Kawasan Terlibat: -Taman Pelangi, Semporna -Kampung T https://t.co/AmLVEYGFIA',\n",
       "  'label': 'Neutral',\n",
       "  'prob': 0.99893045},\n",
       " {'text': 'PERHATIAN: STATUS ARAS AIR SEMASA di SARAWAK , Miri, 18/07/2021 01:15, Marudi, Aras air sungai adalah 3.39m iait https://t.co/EPsvW4rZAw',\n",
       "  'label': 'Neutral',\n",
       "  'prob': 0.9993696},\n",
       " {'text': 'End cap flasing. Menghalang air hujan drip masuk kedalam bumbung. Kesannnya kalau endcap tak ada, kayu manis jadi r https://t.co/u32eJAH7UW',\n",
       "  'label': 'Negative',\n",
       "  'prob': 0.99535036},\n",
       " {'text': 'So lepas ni nak cari pakwe kena cari yang ada rupa. Takde rupa ni pon sama keparat je ada betina lain belakang kita https://t.co/SXvyFb7Ooq',\n",
       "  'label': 'Negative',\n",
       "  'prob': 0.9984772},\n",
       " {'text': 'Tahukah anda? Varian baharu #DELTA merupakan Variant Of Concern (VOC) paling dominan di seluruh dunia dan paling bahaya. Info On Wheels 18/7/2021 Parlimen : Tanah Merah Dun : Bukit Panau Hebahan Proklamasi Darurat/PPN Fasa 2/PICK Pemantauan #bergheh https://t.co/N25c4EDAuo',\n",
       "  'label': 'Neutral',\n",
       "  'prob': 0.9059062},\n",
       " {'text': '@IsmaFaishalAmir Jahak oddo second match sakni. Kecit perut tgk',\n",
       "  'label': 'Neutral',\n",
       "  'prob': 0.98140424},\n",
       " {'text': 'Uniqlo x Kaws Size XL Pit 22/28 RM50 @ Bukit Jalil National Stadium https://t.co/x2ponWec2e',\n",
       "  'label': 'Neutral',\n",
       "  'prob': 0.99630785},\n",
       " {'text': 'Ramai je rentas daerah date dalam kereta yek aku perati seorang2 .',\n",
       "  'label': 'Neutral',\n",
       "  'prob': 0.99031216},\n",
       " {'text': '@LUZLvms @BTS_twt 21. De Jungkook # # #JK #JUNGKOOK # @BTS_twt https://t.co/ALnsq3o3Ea',\n",
       "  'label': 'Neutral',\n",
       "  'prob': 0.99926716}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185787"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('semisupervised-bert-xlnet.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3071/3071 [03:11<00:00, 16.02it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = []\n",
    "for i in tqdm(range(0, len(politics), batch_size)):\n",
    "    train_ids, input_masks, accepted, cleaned = [], [], [], []\n",
    "    for t in politics[i: i + batch_size]:\n",
    "        t_ = cleaning(t)\n",
    "        if len(t_):\n",
    "            input_id, input_mask = str_to_ids(t_)\n",
    "            train_ids.append(input_id)\n",
    "            input_masks.append(input_mask)\n",
    "            accepted.append(t)\n",
    "            cleaned.append(t_)\n",
    "    train_ids = pad_sequences(train_ids, padding='post')\n",
    "    input_masks = pad_sequences(input_masks, padding='post')\n",
    "    result = test_sess.run(logits, feed_dict = {x: train_ids, mask: input_masks})\n",
    "    \n",
    "    input_id, input_segment, input_mask = XY(cleaned)\n",
    "    input_id = pad_sequences(input_id, padding='post')\n",
    "    input_mask = pad_sequences(input_mask, padding='post', value = 1)\n",
    "    input_segment = pad_sequences(input_segment, padding='post', value = 4)\n",
    "    result_xlnet = test_sess_xlnet.run(logits_xlnet, feed_dict = {x_xlnet: input_id, m_xlnet: input_mask,\n",
    "                                                       seg_xlnet: input_segment})\n",
    "    result = (result + result_xlnet) / 2\n",
    "    for no, row in enumerate(result):\n",
    "        if len(row[row > 0.85]):\n",
    "            a = np.argmax(row)\n",
    "            results.append({'text': accepted[no], 'label': labels[a], 'prob': row[a]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23029"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Menggelabah masing-masing nak beraya kan. Lepastu heboh kerajaan gagal. Idiot',\n",
       "  'label': 'Negative',\n",
       "  'prob': 0.9993708},\n",
       " {'text': 'Bendera putih tu bukannya nak harap bantuan kerajaan pon pada asalnya, hanya untuk rakyat jaga sesama rakyat. Ahli https://t.co/P8lqGaRpMj',\n",
       "  'label': 'Negative',\n",
       "  'prob': 0.9994317},\n",
       " {'text': 'Nape nak kena tampal gambar Dia pulak...Guna duit sendiri ke atau Guna duit kerajaan ...',\n",
       "  'label': 'Negative',\n",
       "  'prob': 0.9994585},\n",
       " {'text': 'Sudah sudah la kak mas woi. Kamu tahu tak semua ahli umno bahagian kuale tak sokong kamu masa pru14. Kami bukan banggang mcm kamu. https://t.co/uTrGsm62gh',\n",
       "  'label': 'Negative',\n",
       "  'prob': 0.9993581},\n",
       " {'text': 'Kerajaan kita bukan serba boleh, tapi serba bodoh bahalol.',\n",
       "  'label': 'Negative',\n",
       "  'prob': 0.99945045},\n",
       " {'text': '@magmalaya Maybe azmin rase selangor n kl dh keluar malaysia.kalo gini,do we get all those ahli politik..tak kan..',\n",
       "  'label': 'Negative',\n",
       "  'prob': 0.9994402},\n",
       " {'text': 'Penat lah tengok rakyat malaysia yg toxic nih. Aku nih takdelah sokong kerajaan pun ade je tak suke tapi takde lah https://t.co/PjjRFiF34b',\n",
       "  'label': 'Negative',\n",
       "  'prob': 0.999312},\n",
       " {'text': 'Bodoh dgn kerajaan asyik sambung pkp pkp jelah sampai kiamat mental gak camni',\n",
       "  'label': 'Negative',\n",
       "  'prob': 0.9994802},\n",
       " {'text': 'Dah mula dah... Masuk keluar mahkamah bgi ahli2 politik yg tak sebulu dgn kerajaan... tu biasa... cuba tnya DSAi...',\n",
       "  'label': 'Negative',\n",
       "  'prob': 0.9993253},\n",
       " {'text': 'Dua Hari Nyaris Hatrick, Murai Batu Ceriwis Siap Meraikan Even Bekasi Bersatu! https://t.co/2jFS9Myheg',\n",
       "  'label': 'Positive',\n",
       "  'prob': 0.9996058}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('semisupervised-politics-bert-xlnet.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
